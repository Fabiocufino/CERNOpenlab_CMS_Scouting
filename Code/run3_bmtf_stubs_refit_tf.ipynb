{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMTF input stubs fit with a Deep Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 14:00:07.649843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 14:00:07.649886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 14:00:07.651216: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 14:00:07.658665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mplhep as hep\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hep.style.use(\"CMS\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# mpl.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training device: {'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.getenv(\"USER\")\n",
    "\n",
    "# FILE_PATH = \"/eos/cms/store/cmst3/group/daql1scout/ml_data/run3/bmtf_stubs_refit/\"\n",
    "FILE_PATH = \"/mnt/ml_data/run3/bmtf_stubs_refit/\"\n",
    "\n",
    "FILE_NAME = \"rereco\"\n",
    "\n",
    "OUT_PATH = \"Plots/\"\n",
    "LOSS_FNAME = \"losses.csv\"\n",
    "\n",
    "# FIGSIZE = (12, 9)\n",
    "\n",
    "petroff_10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler('color', petroff_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model for regression+classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNN(tf.keras.Model):\n",
    "    def __init__(self, architecture, reg_strength=0.01):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "\n",
    "        # Check if the architecture list has at least 2 values (input size and one hidden layer)\n",
    "        if len(architecture) < 2:\n",
    "            raise ValueError(\"Architecture must contain at least input size and one hidden layer.\")\n",
    "\n",
    "        self.layers_list = []\n",
    "\n",
    "        # Iterate over the architecture list to dynamically create dense layers followed by batch normalization\n",
    "        for i in range(1, len(architecture)):\n",
    "            self.layers_list.append(layers.Dense(architecture[i], kernel_regularizer=regularizers.l2(reg_strength)))\n",
    "            self.layers_list.append(layers.Activation('elu'))\n",
    "\n",
    "            # TODO: It looks like there might be a typo in the comment.\n",
    "            #  The code does not currently include a batch normalization layer after each dense layer.\n",
    "            #  It only has dense layers followed by activation layers.\n",
    "\n",
    "        # Separate heads for regression and classification tasks\n",
    "        self.regression_head = layers.Dense(3, kernel_regularizer=regularizers.l2(reg_strength))  # for pt, eta, phi\n",
    "        self.classification_head = layers.Dense(1, kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "        # self.classification_head = layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "\n",
    "    #forward pass of the neural network.\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        # Where all the multiplications are done\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "\n",
    "        reg_output = self.regression_head(x)\n",
    "        class_output = self.classification_head(x)\n",
    "        return reg_output, class_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback is designed to adjust the learning rate of the optimizer during training based on the validation loss.\\\n",
    "It adjusts the learning rate based on the validation loss, reducing it if the loss does not improve sufficiently over a number of epochs (patience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate scheduler callback\n",
    "class CustomLRScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        optimizer, \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_improvement=0.01, \n",
    "        verbose=True\n",
    "    ):\n",
    "        super(CustomLRScheduler, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_improvement = min_improvement\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            self.optimizer.lr.assign(self.min_lr)\n",
    "        if epoch < self.decrease_epoch:\n",
    "            self.increase_flag = True\n",
    "        else:\n",
    "            self.increase_flag = False\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if loss:\n",
    "            relative_improvement = (self.best_loss - loss) / self.best_loss\n",
    "\n",
    "            if relative_improvement < self.min_improvement:\n",
    "                self.patience_counter += 1\n",
    "            else:\n",
    "                self.patience_counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "            if self.patience_counter >= self.patience:\n",
    "                self._decrease_lr()\n",
    "\n",
    "    def _decrease_lr(self):\n",
    "        old_lr = self.optimizer.lr.numpy()\n",
    "        new_lr = old_lr * self.factor\n",
    "        self.optimizer.lr.assign(new_lr)\n",
    "        if self.verbose:\n",
    "            print(f\"Decreasing learning rate to {new_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stubs_norm         = 2**2\n",
    "station_norm         = 2**2\n",
    "sector_norm          = 2**3\n",
    "wheel_norm           = 2**1\n",
    "eta_norm             = 2**8\n",
    "qeta_norm            = 2**1\n",
    "tag_norm             = 2**0\n",
    "phi_norm             = 2**11\n",
    "phib_norm            = 2**9\n",
    "quality_norm         = 2**3\n",
    "reco_pt_norm         = 2**0\n",
    "reco_pt_inverse_norm = 2**0\n",
    "reco_eta_norm        = 2**2\n",
    "reco_phi_norm        = 2**2\n",
    "reco_charge_norm     = 2**0\n",
    "\n",
    "normalizations = {\n",
    "    \"n_stubs\": n_stubs_norm,\n",
    "    \"s1_stNum\": station_norm,\n",
    "    \"s1_scNum\": sector_norm,\n",
    "    \"s1_whNum\": wheel_norm,\n",
    "    \"s1_eta_1\": eta_norm,\n",
    "    \"s1_qeta_1\": qeta_norm,\n",
    "    \"s1_eta_2\": eta_norm,\n",
    "    \"s1_qeta_2\": qeta_norm,\n",
    "    \"s1_tag\": tag_norm,\n",
    "    \"s1_phi\": phi_norm,\n",
    "    \"s1_phiB\": phib_norm,\n",
    "    \"s1_quality\": quality_norm,\n",
    "    \"s2_stNum\": station_norm,\n",
    "    \"s2_scNum\": sector_norm,\n",
    "    \"s2_whNum\": wheel_norm,\n",
    "    \"s2_eta_1\": eta_norm,\n",
    "    \"s2_qeta_1\": qeta_norm,\n",
    "    \"s2_eta_2\": eta_norm,\n",
    "    \"s2_qeta_2\": qeta_norm,\n",
    "    \"s2_tag\": tag_norm,\n",
    "    \"s2_phi\": phi_norm,\n",
    "    \"s2_phiB\": phib_norm,\n",
    "    \"s2_quality\": quality_norm,\n",
    "    \"s3_stNum\": station_norm,\n",
    "    \"s3_scNum\": sector_norm,\n",
    "    \"s3_whNum\": wheel_norm,\n",
    "    \"s3_eta_1\": eta_norm,\n",
    "    \"s3_qeta_1\": qeta_norm,\n",
    "    \"s3_eta_2\": eta_norm,\n",
    "    \"s3_qeta_2\": qeta_norm,\n",
    "    \"s3_tag\": tag_norm,\n",
    "    \"s3_phi\": phi_norm,\n",
    "    \"s3_phiB\": phib_norm,\n",
    "    \"s3_quality\": quality_norm,\n",
    "    \"s4_stNum\": station_norm,\n",
    "    \"s4_scNum\": sector_norm,\n",
    "    \"s4_whNum\": wheel_norm,\n",
    "    \"s4_eta_1\": eta_norm,\n",
    "    \"s4_qeta_1\": qeta_norm,\n",
    "    \"s4_eta_2\": eta_norm,\n",
    "    \"s4_qeta_2\": qeta_norm,\n",
    "    \"s4_tag\": tag_norm,\n",
    "    \"s4_phi\": phi_norm,\n",
    "    \"s4_phiB\": phib_norm,\n",
    "    \"s4_quality\": quality_norm,\n",
    "    # \"ptReco\": reco_pt_norm,\n",
    "    \"ptRecoInverse\": reco_pt_inverse_norm,\n",
    "    \"etaExtRecoSt2\": reco_eta_norm,\n",
    "    \"phiExtRecoSt2\": reco_phi_norm,\n",
    "    \"chargeReco\": reco_charge_norm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features \n",
    "\n",
    "| Feature                  | Description                                                                                  |\n",
    "|--------------------------|----------------------------------------------------------------------------------------------|\n",
    "| **n_stubs**              | The total number of stubs detected for a muon.                                               |\n",
    "| **s1_stNum, s2_stNum, s3_stNum, s4_stNum** | The specific muon station where the stub was detected.                            |\n",
    "| **s1_scNum, s2_scNum, s3_scNum, s4_scNum** | The sector within the muon station where the stub was detected.                       |\n",
    "| **s1_whNum, s2_whNum, s3_whNum, s4_whNum** | The wheel of the muon detector where the stub was detected.                          |\n",
    "| **s1_eta_1, s2_eta_1, s3_eta_1, s4_eta_1, s1_eta_2, s2_eta_2, s3_eta_2, s4_eta_2** | The pseudorapidity of the stub.                                                       |\n",
    "| **s1_qeta_1, s2_qeta_1, s3_qeta_1, s4_qeta_1, s1_qeta_2, s2_qeta_2, s3_qeta_2, s4_qeta_2** |----- ----                                            |\n",
    "| **s1_tag, s2_tag, s3_tag, s4_tag** | A tag indicating specific characteristics of the stub.                                  |\n",
    "| **s1_phi, s2_phi, s3_phi, s4_phi** | The azimuthal angle of the stub.                                                          |\n",
    "| **s1_phiB, s2_phiB, s3_phiB, s4_phiB** | The bending angle of the stub in the phi plane.                                           |\n",
    "| **s1_quality, s2_quality, s3_quality, s4_quality** | A quality metric for the stub.                                                           |\n",
    "| **ptReco, ptRecoInverse** | The transverse momentum of the muon, reconstructed from the stubs.                              |\n",
    "| **etaExtRecoSt2** | The reconstructed pseudorapidity of the muon from the stubs.                                   |\n",
    "| **phiExtRecoSt2** | The reconstructed azimuthal angle of the muon from the stubs.                                      |\n",
    "| **chargeReco** | The reconstructed charge of the muon from the stubs.                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_features = [\n",
    "    'n_stubs', \n",
    "    's1_stNum', 's1_scNum', 's1_whNum', 's1_eta_1', 's1_qeta_1', 's1_eta_2', 's1_qeta_2', 's1_tag', 's1_phi', 's1_phiB', 's1_quality', \n",
    "    's2_stNum', 's2_scNum', 's2_whNum', 's2_eta_1', 's2_qeta_1', 's2_eta_2', 's2_qeta_2', 's2_tag', 's2_phi', 's2_phiB', 's2_quality', \n",
    "    's3_stNum', 's3_scNum', 's3_whNum', 's3_eta_1', 's3_qeta_1', 's3_eta_2', 's3_qeta_2', 's3_tag', 's3_phi', 's3_phiB', 's3_quality', \n",
    "    's4_stNum', 's4_scNum', 's4_whNum', 's4_eta_1', 's4_qeta_1', 's4_eta_2', 's4_qeta_2', 's4_tag', 's4_phi', 's4_phiB', 's4_quality'\n",
    "]\n",
    "\n",
    "target_features = [\n",
    "    'ptRecoInverse', 'etaExtRecoSt2', 'phiExtRecoSt2', 'chargeReco',\n",
    "]\n",
    "\n",
    "l1_features = [\n",
    "    'ptL1', 'etaL1', 'phiL1', 'hwSignL1',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_ = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(FILE_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_data_ = pd.concat([full_data_, pd.read_csv(FILE_PATH + file)], ignore_index=True)\n",
    "        \n",
    "full_data_ = full_data_.iloc[:, :-1]\n",
    "\n",
    "# drop rows with ptL1 == 4.5\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] != 4.5]\n",
    "\n",
    "# drop rows with reco eta > 1 and < -1\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] < 1]\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] > -1]\n",
    "\n",
    "# drop rows with ptL1 > 50\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] < 256]\n",
    "\n",
    "# drop rows with ptReco > 50\n",
    "full_data_ = full_data_[full_data_[\"ptReco\"] < 256]\n",
    "\n",
    "\n",
    "# mask_1 = (full_data_.etaL1 == 0) & (np.abs(full_data_.etaExtRecoSt2) == 0)\n",
    "# mask_2 = (full_data_.etaL1 != 0)\n",
    "\n",
    "# mask = mask_1 | mask_2\n",
    "\n",
    "# full_data_ = full_data_[mask]\n",
    "\n",
    "full_data_[\"ptRecoInverse\"] = 1 / full_data_[\"ptReco\"]\n",
    "\n",
    "full_data = full_data_[stub_features + target_features + l1_features]\n",
    "\n",
    "full_data[\"chargeReco\"] = full_data[\"chargeReco\"].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHACAYAAABeV0mSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+5UlEQVR4nO3de1xWZb7///fNWUQUTTwiqKl5yAMZiu4ZyXTQ/DrimNOUpWaataXBqNzpjJrWzg4itstRy0nUJptskm1pTUqR5hmT1DJD85QCmspROQjr94c/7+0tB7kRWLB8PR+P+/Forfta6/pc92qGd2tday2bYRiGAAAALMLF7AIAAACqEuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYyi0dbjZv3qzhw4erZcuWstlsio+Pd3ofhmFo/vz56tixozw9PdWqVSv993//d9UXCwAAKsTN7ALMlJubqx49emjChAn6wx/+UKl9REVF6YsvvtD8+fN155136vz58zp//nwVVwoAACrKxoszr7DZbFq7dq0iIiLs6/Lz8/WXv/xFq1evVkZGhrp166ZXX31VYWFhkqSDBw+qe/fuOnDggDp16mRO4QAAwMEtfVnqRiIjI7V9+3Z98MEH2rdvn0aPHq0hQ4YoJSVFkvTJJ5+oXbt2+vTTT9W2bVsFBQVp4sSJnLkBAMBEhJsynDhxQsuXL9eaNWv0m9/8Ru3bt9ezzz6r//iP/9Dy5cslST///LOOHz+uNWvWaOXKlYqLi9OePXt0//33m1w9AAC3rlt6zk159u/fr6KiInXs2NFhfX5+vpo0aSJJKi4uVn5+vlauXGlv9/e//1133XWXDh06xKUqAABMQLgpQ05OjlxdXbVnzx65uro6fOfj4yNJatGihdzc3BwCUOfOnSVdOfNDuAEAoOYRbsrQq1cvFRUV6cyZM/rNb35Tapv+/fvr8uXLOnLkiNq3by9J+umnnyRJgYGBNVYrAAD4P7f03VI5OTk6fPiwpCthZsGCBbrnnnvUuHFjtWnTRg8//LC2bt2qmJgY9erVS2fPnlVCQoK6d++uYcOGqbi4WHfffbd8fHy0cOFCFRcXa8qUKfL19dUXX3xh8ugAALg13dLhJjExUffcc0+J9ePGjVNcXJwKCwv10ksvaeXKlTp16pRuu+029e3bV3PmzNGdd94pSTp9+rSeeuopffHFF6pfv76GDh2qmJgYNW7cuKaHAwAAdIuHGwAAYD3cCg4AACyFcAMAACzllrtbqri4WKdPn1aDBg1ks9nMLgcAAFSAYRjKzs5Wy5Yt5eJS/rmZWy7cnD59WgEBAWaXAQAAKuHkyZNq3bp1uW1uuXDToEEDSVd+HF9fX5OrAQAAFZGVlaWAgAD73/Hy3HLh5uqlKF9fX8INAAB1TEWmlDChGAAAWArhBgAAWArhBgAAWMotN+cGAICKKioqUmFhodll3DI8PDxueJt3RRBuAAC4jmEYSktLU0ZGhtml3FJcXFzUtm1beXh43NR+CDcAAFznarDx9/eXt7c3D32tAVcfspuamqo2bdrc1G9OuAEA4BpFRUX2YNOkSROzy7mlNG3aVKdPn9bly5fl7u5e6f0woRgAgGtcnWPj7e1tciW3nquXo4qKim5qP4QbAABKwaWomldVvznhBgAAWApzbgAAqKD0rEJl5t3cJRNnNPRyVTPfys89qUphYWHq2bOnFi5caHYpN0S4AQCgAtKzCjVu1XHlXTZqrE8vN5tWPBJY4YBz9uxZzZo1S+vXr1d6err8/PzUo0cPzZo1S/3795d05dLP2rVrFRERUY2VX5GamqpnnnlGSUlJOnz4sP785z/XSDgi3AAAUAGZeUXKu2zoL+HNFNj45p7DUhHHzxfov/+drsy8ogqHm1GjRqmgoEArVqxQu3btlJ6eroSEBJ07d66aqy1dfn6+mjZtqr/+9a+KjY2tsX4JNwAAOCGwsYc6+nuZXUYJGRkZ2rJlixITEzVgwABJUmBgoEJCQuxtgoKCJEkjR460f3/s2DGNHz9eGRkZio+Pt7edOnWqkpOTlZiYaF93+fJlRUZGatWqVXJ3d9eTTz6puXPnljkROCgoSG+88YYk6d13363C0ZaPcANcL/ekVGDOf+WgCng0keoHmF0FUON8fHzk4+Oj+Ph49e3bV56eniXa7N69W/7+/lq+fLmGDBkiV1dXp/pYsWKFHnvsMe3atUtJSUl6/PHH1aZNG02aNKmqhlElCDfAtXJPSp+HSEUXza4EleXqLQ3ZRcDBLcfNzU1xcXGaNGmSlixZouDgYA0YMEB/+tOf1L17d0lXHpInSY0aNVLz5s2d7iMgIECxsbGy2Wzq1KmT9u/fr9jYWMINUKsVnLsSbPq8I/l2NLsaOCvrJ2nnpCvHkXCDW9CoUaM0bNgwbdmyRTt27NBnn32m1157TcuWLdP48eNvev99+/Z1uAQVGhqqmJgYFRUVOX0WqDoRboDS+HaU/HqaXQUAOM3Ly0uDBw/W4MGDNXPmTE2cOFGzZ88uN9y4uLjIMBzvAqvLb0PnIX4AAFhYly5dlJuba192d3cv8XqDpk2bKjU11WFdcnJyiX3t3LnTYXnHjh3q0KFDrTprI3HmBoDF5F0oVuHBg5JvsdmloJLc/W6TV6s2ZpdR55w7d06jR4/WhAkT1L17dzVo0EBJSUl67bXXNGLECHu7oKAgJSQkqH///vL09JSfn58GDhyo119/XStXrlRoaKjee+89HThwQL169XLo48SJE4qOjtbkyZP17bff6s0331RMTEy5dV0NSTk5OTp79qySk5Pl4eGhLl26VPlvcBXhBoBl5KWmasf8XBUXjjG7FNwEl3re6rvph1obcI6fL6iV/fj4+KhPnz6KjY3VkSNHVFhYqICAAE2aNEkzZsywt4uJiVF0dLTeeecdtWrVSseOHVN4eLhmzpypadOmKS8vTxMmTNDYsWO1f/9+hz7Gjh2rS5cuKSQkRK6uroqKitLjjz9ebl3XBqQ9e/bo/ffft9+CXl1sxvUX2SwuKytLDRs2VGZmpnx9fc0uB7XNhWRp4wBp8NfMuamDsret1u6HxqjLf7+k+j2GmF0OKiH38EH9MPUR3f1pkhp0Czalhry8PB09elRt27aVl9f/Pc+mLjyhuK4r67eXnPv7zZkbAJZTv21b0/4wwrqa+bprxSOBt+y7peoSwg0AABXUzNedsFEHcLcUAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFJ5zAwBAReWelArO1Vx/Hk2k+gE11185wsLC1LNnTy1cuNDsUm6IcAMAQEXknpQ+D5GKLtZcn67e0pBdFQ44Z8+e1axZs7R+/Xqlp6fLz89PPXr00KxZs9S/f39Jks1m09q1axUREVGNhV/x8ccfa/HixUpOTlZ+fr66du2qF154QeHh4dXaL+EGAICKKDh3Jdj0eUfy7Vj9/WX9JO2cdKXfCoabUaNGqaCgQCtWrFC7du2Unp6uhIQEnTtXg2ebrrF582YNHjxYL7/8sho1aqTly5dr+PDh2rlzZ4k3jlclwg0AAM7w7VgrX6ybkZGhLVu2KDExUQMGDJAkBQYGKiQkxN4mKChIkjRy5Ej798eOHdP48eOVkZGh+Ph4e9upU6cqOTlZiYmJ9nWXL19WZGSkVq1aJXd3dz355JOaO3eubDZbqTVdfwnr5Zdf1v/+7//qk08+qdZww4RiAAAswMfHRz4+PoqPj1d+fn6pbXbv3i1JWr58uVJTU+3LFbVixQq5ublp165deuONN7RgwQItW7aswtsXFxcrOztbjRs3dqpfZxFuAACwADc3N8XFxWnFihVq1KiR+vfvrxkzZmjfvn32Nk2bNpUkNWrUSM2bN7cvV1RAQIBiY2PVqVMnjRkzRk899ZRiY2MrvP38+fOVk5OjP/7xj0716yzCDQAAFjFq1CidPn1a69at05AhQ5SYmKjg4GDFxcVVyf779u3rcAkqNDRUKSkpKioquuG277//vubMmaMPP/xQ/v7+VVJPWQg3AABYiJeXlwYPHqyZM2dq27ZtGj9+vGbPnl3uNi4uLjIMw2FdYWFhldX0wQcfaOLEifrwww81aNCgKttvWQg3AABYWJcuXZSbm2tfdnd3L3GmpWnTpkpNTXVYl5ycXGJfO3fudFjesWOHOnToIFdX1zL7X716tR599FGtXr1aw4YNq8QInEe4AQDAAs6dO6eBAwfqvffe0759+3T06FGtWbNGr732mkaMGGFvFxQUpISEBKWlpenChQuSpIEDByopKUkrV65USkqKZs+erQMHDpTo48SJE4qOjtahQ4e0evVqvfnmm4qKiiqzpvfff19jx45VTEyM+vTpo7S0NKWlpSkzM7Pqf4BrcCs4AADOyPqpVvbj4+OjPn36KDY2VkeOHFFhYaECAgI0adIkzZgxw94uJiZG0dHReuedd9SqVSsdO3ZM4eHhmjlzpqZNm6a8vDxNmDBBY8eO1f79+x36GDt2rC5duqSQkBC5uroqKipKjz/+eJk1vf3227p8+bKmTJmiKVOm2NePGzeuyuYBlYZwAwBARXg0ufLE4J2Taq5PV+8r/VaAp6en5s2bp3nz5pXbbvjw4Ro+fHiJ9XPmzNGcOXPK3O7a590sXry4QjVdu01NItwAAFAR9QOuvArhFn23VF1CuAEAoKLqBxA26gAmFAMAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsxNdzMmzdPd999txo0aCB/f39FRETo0KFDN9xuzZo1uuOOO+Tl5aU777xTGzZsqIFqAQBAXWBquPn66681ZcoU7dixQxs3blRhYaF+97vfObwD43rbtm3Tgw8+qMcee0x79+5VRESEIiIiSn1MNAAAuPWY+pybzz//3GE5Li5O/v7+2rNnj37729+Wus0bb7yhIUOG6LnnnpMkvfjii9q4caPeeustLVmypNprBgDcuvJOnVDhhV9rrD93v9vk1apNjfVXnrCwMPXs2VMLFy40u5QbqlUP8bv6Iq3GjRuX2Wb79u2Kjo52WBceHq74+PjqLK3C0rMKlZlXdOOGqJU8swsUaHYRAGqlvFMntGNQFxVfulhjfbrU81bfTT9UOOCcPXtWs2bN0vr165Weni4/Pz/16NFDs2bNUv/+/SVJNptNa9euVURERDVWfsU333yj//qv/9KPP/6oixcvKjAwUJMnT9bTTz9drf3WmnBTXFysqVOnqn///urWrVuZ7dLS0tSsWTOHdc2aNVNaWlqp7fPz85Wfn29fzsrKqpqCS5GeVahxq44r77JRbX2genVwS9c7TaVzOZfVxM/sagDUJoUXflXxpYvqsnCV6t/eudr7yz18UD9MfUSFF36tcLgZNWqUCgoKtGLFCrVr107p6elKSEjQuXM1+MqIa9SvX1+RkZHq3r276tevr2+++UaTJ09W/fr1y33h5s2qNeFmypQpOnDggL755psq3e+8efPKfRFYVcrMK1LeZUN/CW+mwMYeNdInqtb5X85Ih6WcgmJV7FV1AG419W/vrAbdgs0uo4SMjAxt2bJFiYmJGjBggCQpMDBQISEh9jZBQUGSpJEjR9q/P3bsmMaPH6+MjAyHqyBTp05VcnKyw8svL1++rMjISK1atUru7u568sknNXfuXNlstlJr6tWrl3r16uXQ/8cff6wtW7ZYP9xERkbq008/1ebNm9W6dety2zZv3lzp6ekO69LT09W8efNS20+fPt3hMlZWVpYCAqr3vSCBjT3U0d+rWvtA9Tie7W52CQBQKT4+PvLx8VF8fLz69u0rT0/PEm12794tf39/LV++XEOGDJGrq6tTfaxYsUKPPfaYdu3apaSkJD3++ONq06aNJk2q2JvS9+7dq23btumll15yql9nmXq3lGEYioyM1Nq1a/Xll1+qbdu2N9wmNDRUCQkJDus2btyo0NDQUtt7enrK19fX4QMAgNW4ubkpLi5OK1asUKNGjdS/f3/NmDFD+/bts7dp2rSpJKlRo0Zq3ry5fbmiAgICFBsbq06dOmnMmDF66qmnFBsbe8PtWrduLU9PT/Xu3VtTpkzRxIkTnRuck0wNN1OmTNF7772n999/Xw0aNFBaWprS0tJ06dIle5uxY8dq+vTp9uWoqCh9/vnniomJ0Y8//qgXXnhBSUlJioyMNGMIAADUGqNGjdLp06e1bt06DRkyRImJiQoODlZcXFyV7L9v374Ol6BCQ0OVkpKioqLyb6TZsmWLkpKStGTJEi1cuFCrV6+uknrKYmq4Wbx4sTIzMxUWFqYWLVrYP//85z/tbU6cOKHU1FT7cr9+/fT+++/r7bffVo8ePfTRRx8pPj6+3EnIAADcKry8vDR48GDNnDlT27Zt0/jx4zV79uxyt3FxcZFhON4MU1hYWGU1tW3bVnfeeacmTZqkp59+Wi+88EKV7bs0ps65uf6HLM21E5muGj16tEaPHl0NFQEAYC1dunRxmCjs7u5e4kxL06ZNSzwMNzk5We7ujvMQd+7c6bC8Y8cOdejQwam5O8XFxQ53MVcH3i0FAIAFnDt3TgMHDtR7772nffv26ejRo1qzZo1ee+01jRgxwt4uKChICQkJSktL04ULFyRJAwcOVFJSklauXKmUlBTNnj271Cf/nzhxQtHR0Tp06JBWr16tN998U1FRUWXWtGjRIn3yySdKSUlRSkqK/v73v2v+/Pl6+OGHq/4HuEatuFsKAIC6IvfwwVrZj4+Pj/r06aPY2FgdOXJEhYWFCggI0KRJkzRjxgx7u5iYGEVHR+udd95Rq1atdOzYMYWHh2vmzJmaNm2a8vLyNGHCBI0dO1b79+936GPs2LG6dOmSQkJC5OrqqqioqHJv6S4uLtb06dN19OhRubm5qX379nr11Vc1efJk534MJxFuAACoAHe/2+RSz1s/TH2kxvp0qectd7/bKtTW09NT8+bN07x588ptN3z4cA0fPrzE+jlz5pT7XLhrp4ksXry4QjU99dRTeuqppyrUtioRbgAAqACvVm3Ud9MPt+y7peoSwg0AABXk1aoNYaMOYEIxAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFJ5zAwBABWXn5iovP6/G+vPy9FKD+vVrrL/yhIWFqWfPnlq4cKHZpdwQ4QYAgArIzs3V+5/8ry5f90bt6uTm6qqHho+ocMA5e/asZs2apfXr1ys9PV1+fn7q0aOHZs2apf79+0uSbDab1q5dq4iIiGqsvKStW7dqwIAB6tatm5KTk6u1L8INAAAVkJefp8tFRRrUr78aN2xY7f2dz8zUpm1blZefV+FwM2rUKBUUFGjFihVq166d0tPTlZCQoHPnzlVzteXLyMjQ2LFjde+99yo9Pb3a+yPcAADghMYNG6pp4yZml1FCRkaGtmzZosTERA0YMECSFBgYqJCQEHuboKAgSdLIkSPt3x87dkzjx49XRkaG4uPj7W2nTp2q5ORkhxdmXr58WZGRkVq1apXc3d315JNPau7cubLZbOXW9sQTT+ihhx6Sq6urQx/VhQnFAABYgI+Pj3x8fBQfH6/8/PxS2+zevVuStHz5cqWmptqXK2rFihVyc3PTrl279MYbb2jBggVatmxZudssX75cP//8s2bPnu1UXzeDcAMAgAW4ubkpLi5OK1asUKNGjdS/f3/NmDFD+/bts7dp2rSpJKlRo0Zq3ry5fbmiAgICFBsbq06dOmnMmDF66qmnFBsbW2b7lJQUPf/883rvvffk5lZzF4sINwAAWMSoUaN0+vRprVu3TkOGDFFiYqKCg4MVFxdXJfvv27evwyWo0NBQpaSkqKiUSdZFRUV66KGHNGfOHHXs2LFK+q8owg0AABbi5eWlwYMHa+bMmdq2bZvGjx9/w0tCLi4uMgzDYV1hYeFN1ZGdna2kpCRFRkbKzc1Nbm5umjt3rr777ju5ubnpyy+/vKn9l4cJxQAAWFiXLl0cJvG6u7uXONPStGlTHThwwGFdcnKy3N3dHdbt3LnTYXnHjh3q0KGDXF1dS/Tr6+ur/fv3O6z729/+pi+//FIfffSR2rZtW5nhVAjhBgAAJ5zPzKyV/Zw7d06jR4/WhAkT1L17dzVo0EBJSUl67bXXNGLECHu7oKAgJSQkqH///vL09JSfn58GDhyo119/XStXrlRoaKjee+89HThwQL169XLo48SJE4qOjtbkyZP17bff6s0331RMTEyp9bi4uKhbt24O6/z9/eXl5VVifVUj3AAAUAFenl5yc3XVpm1ba6xPN1dXeXl6Vaitj4+P+vTpo9jYWB05ckSFhYUKCAjQpEmTNGPGDHu7mJgYRUdH65133lGrVq107NgxhYeHa+bMmZo2bZry8vI0YcIEjR07tsSZl7Fjx+rSpUsKCQmRq6uroqKi9Pjjj1fpmKuCzbj+IpvFZWVlqWHDhsrMzJSvr2+V7vunM3l6fPVJvf1ggDr6V+xfRtQux4/sUuCewTp+10YFtg+58QaoVbK3rdbuh8bo7vf/oQb9HjS7HFRC9oFvtfv/9dbdnyapQbdgU2rIy8vT0aNH1bZtW3l5Of5/+a38+oWaUN5v78zfb87cAABQQQ3q17+lwkZdxd1SAADAUgg3AADAUgg3AADAUgg3AACU4ha736ZWqKrfnHADAMA1rj647uLFiyZXcuspKCiQpFIfCugM7pYCAOAarq6uatSokc6cOSNJ8vb2dnifEqpHcXGxzp49K29v75t+ySbhBgCA6zRv3lyS7AEHNcPFxUVt2rS56TBJuAEA4Do2m00tWrSQv7//Tb9AEhXn4eEhF5ebnzFDuAEAoAyurq43Pf8DNY8JxQAAwFIINwAAwFK4LAUAqFUKGzbWuYuXlHf+nNmloJLMfuEn4QYAUGvkFBToyNRXdDjlZynlZ7PLQSW5ubrqoeEjTAs4hBsAQK2Rf7lIhoen/qNNK7Xs2sPsclAJ5zMztWnbVuXl5xFuAAC4qqGnp5o2bmJ2GaijmFAMAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAsxdRws3nzZg0fPlwtW7aUzWZTfHx8ue0TExNls9lKfNLS0mqmYAAAUOuZGm5yc3PVo0cPLVq0yKntDh06pNTUVPvH39+/mioEAAB1jZuZnQ8dOlRDhw51ejt/f381atSo6gsCAAB1Xp2cc9OzZ0+1aNFCgwcP1tatW80uBwAA1CKmnrlxVosWLbRkyRL17t1b+fn5WrZsmcLCwrRz504FBweXuk1+fr7y8/Pty1lZWTVVLgAAMEGdCjedOnVSp06d7Mv9+vXTkSNHFBsbq1WrVpW6zbx58zRnzpyaKhEAAJisTl6WulZISIgOHz5c5vfTp09XZmam/XPy5MkarA4AANS0OnXmpjTJyclq0aJFmd97enrK09OzBisCAABmMjXc5OTkOJx1OXr0qJKTk9W4cWO1adNG06dP16lTp7Ry5UpJ0sKFC9W2bVt17dpVeXl5WrZsmb788kt98cUXZg0BAADUMqaGm6SkJN1zzz325ejoaEnSuHHjFBcXp9TUVJ04ccL+fUFBgZ555hmdOnVK3t7e6t69uzZt2uSwDwAAcGszNdyEhYXJMIwyv4+Li3NYnjZtmqZNm1bNVQEAgLqszk8oBgAAuBbhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWEqlws3Jkyf1yy+/2Jd37dqlqVOn6u23366ywgAAACqjUuHmoYce0ldffSVJSktL0+DBg7Vr1y795S9/0dy5c6u0QAAAAGdUKtwcOHBAISEhkqQPP/xQ3bp107Zt2/SPf/xDcXFxVVkfAACAUyoVbgoLC+Xp6SlJ2rRpk37/+99Lku644w6lpqZWXXUAAABOqlS46dq1q5YsWaItW7Zo48aNGjJkiCTp9OnTatKkSZUWCAAA4IxKhZtXX31VS5cuVVhYmB588EH16NFDkrRu3Tr75SoAAAAzuFVmo7CwMP3666/KysqSn5+fff3jjz8ub2/vKisOAADAWZV+zo1hGNqzZ4+WLl2q7OxsSZKHhwfhBgAAmKpSZ26OHz+uIUOG6MSJE8rPz9fgwYPVoEEDvfrqq8rPz9eSJUuquk4AAIAKqdSZm6ioKPXu3VsXLlxQvXr17OtHjhyphISEKisOAADAWZU6c7NlyxZt27ZNHh4eDuuDgoJ06tSpKikMAACgMip15qa4uFhFRUUl1v/yyy9q0KDBTRcFAABQWZUKN7/73e+0cOFC+7LNZlNOTo5mz56t++67r6pqAwAAcFqlLkvFxMQoPDxcXbp0UV5enh566CGlpKTotttu0+rVq6u6RgAAgAqrVLhp3bq1vvvuO33wwQfat2+fcnJy9Nhjj2nMmDEOE4wBAABqWqXCjSS5ubnp4YcfrspaAAAAblqFw826des0dOhQubu7a926deW2vfoiTQAAgJpW4XATERGhtLQ0+fv7KyIiosx2Nput1DupAAAAakKFw01xcXGp/wwAAFCbOH0reGFhoe69916lpKRURz0AAAA3xelw4+7urn379lVHLQAAADetUg/xe/jhh/X3v/+9qmsBAAC4aZW6Ffzy5ct69913tWnTJt11112qX7++w/cLFiyokuIAAACcValwc+DAAQUHB0uSfvrppyotCAAA4GZUKtx89dVXVV0HAABAlajUnJsJEyYoOzu7xPrc3FxNmDDhposCAACorEqFmxUrVujSpUsl1l+6dEkrV6686aIAAAAqy6nLUllZWTIMQ4ZhKDs7W15eXvbvioqKtGHDBvn7+1d5kQAAABXlVLhp1KiRbDabbDabOnbsWOJ7m82mOXPmVFlxAAAAznIq3Hz11VcyDEMDBw7Uv/71LzVu3Nj+nYeHhwIDA9WyZcsqLxIAAKCinAo3AwYMkCQdPXpUAQEBcnGp1JQdAACAalOpW8EDAwOVkZGhXbt26cyZMyVepDl27NgqKQ4AAMBZlQo3n3zyicaMGaOcnBz5+vrKZrPZv7PZbIQbAABgmkpdV3rmmWc0YcIE5eTkKCMjQxcuXLB/zp8/X9U1AgAAVFilws2pU6f05z//Wd7e3lVdDwAAwE2pVLgJDw9XUlJSVdcCAABw0yo152bYsGF67rnn9MMPP+jOO++Uu7u7w/e///3vq6Q4AAAAZ1Uq3EyaNEmSNHfu3BLf2Ww2FRUV3VxVAAAAlVSpcHP9rd8AAAC1hVNzbu677z5lZmbal1955RVlZGTYl8+dO6cuXbpUWXEAAADOcirc/Pvf/1Z+fr59+eWXX3a49fvy5cs6dOhQ1VUHAADgJKfCjWEY5S4DAACYjZdDAQAAS3Eq3NhsNodXLVxdBwAAUFs4dbeUYRgaP368PD09JUl5eXl64oknVL9+fUlymI8DAABgBqfO3IwbN07+/v5q2LChGjZsqIcfflgtW7a0L/v7+zv10szNmzdr+PDhatmypWw2m+Lj42+4TWJiooKDg+Xp6anbb79dcXFxzgwBAABYnFNnbpYvX16lnefm5qpHjx6aMGGC/vCHP9yw/dGjRzVs2DA98cQT+sc//qGEhARNnDhRLVq0UHh4eJXWBgAA6qZKPcSvqgwdOlRDhw6tcPslS5aobdu2iomJkSR17txZ33zzjWJjYwk3AABAUh27W2r79u0aNGiQw7rw8HBt377dpIoAAEBtY+qZG2elpaWpWbNmDuuaNWumrKwsXbp0SfXq1SuxTX5+vsNE56ysrGqvEwAAmKdOnbmpjHnz5tknPDds2FABAQFmlwQAAKpRnQo3zZs3V3p6usO69PR0+fr6lnrWRpKmT5+uzMxM++fkyZM1USoAADBJnbosFRoaqg0bNjis27hxo0JDQ8vcxtPT0/5cHgAAYH2mnrnJyclRcnKykpOTJV251Ts5OVknTpyQdOWsy7XPzXniiSf0888/a9q0afrxxx/1t7/9TR9++KGefvppM8oHAAC1kKnhJikpSb169VKvXr0kSdHR0erVq5dmzZolSUpNTbUHHUlq27at1q9fr40bN6pHjx6KiYnRsmXLuA0cAADYmXpZKiwsrNw3i5f29OGwsDDt3bu3GqsCAAB1WZ2aUAwAAHAjhBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGApbmYXANRGqZmFyj+TZ3YZcFZmodkVAKgFCDfANXw8rpzM/Pv280q5fNLkauCskHPnNURSxsUiNTC7GACmIdwA12jic+V/En8d0kz5DQJMrgbO+vVrX+VJulRYbHYpAExEuAFKEdjYQ/LzMrsMOMvHTb+YXQMA0zGhGAAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWIqb2QUAQFUqbNhYWZels+fPmV0KKiEzP9/sEmABhBsAlnGpSDoy9RUdzrJp52cbzC4HlWQryJenm6vZZaAOI9wAsIxCQzI8PNWtvqEuvx1mdjmohNzDP+rQpBHy+ccXZpeCOoxwA8By6rtKTRs3MbsMVIKXdz25Z543uwzUcUwoBgAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAllIrws2iRYsUFBQkLy8v9enTR7t27SqzbVxcnGw2m8PHy8urBqsFAAC1melPKP7nP/+p6OhoLVmyRH369NHChQsVHh6uQ4cOyd/fv9RtfH19dejQIfuyzWarqXJvyN8lTZ7ZFyR3D7NLQWVk/WR2BQCAm2R6uFmwYIEmTZqkRx99VJK0ZMkSrV+/Xu+++66ef/75Urex2Wxq3rx5TZZZIW55v2hF0/tVb0+e2aXgZrh6Sx48uh8A6ipTw01BQYH27Nmj6dOn29e5uLho0KBB2r59e5nb5eTkKDAwUMXFxQoODtbLL7+srl271kTJ5XItPK96LnlK7bxILVp3M7scVJZHE6l+gNlVAAAqydRw8+uvv6qoqEjNmjVzWN+sWTP9+OOPpW7TqVMnvfvuu+revbsyMzM1f/589evXT99//71at25don1+fr7y8/Pty1lZWVU7iFIUeHeU/HpWez8AAKAk0y9LOSs0NFShoaH25X79+qlz585aunSpXnzxxRLt582bpzlz5tRYfXkXipX/04/KvlTnflr8/9z9bpNXqzZmlwEAqCRT/wLfdtttcnV1VXp6usP69PT0Cs+pcXd3V69evXT48OFSv58+fbqio6Pty1lZWQoIqJ5LDoXpadoxP1fFheP1S7X0gJrgUs9bfTf9QMABgDrK1HDj4eGhu+66SwkJCYqIiJAkFRcXKyEhQZGRkRXaR1FRkfbv36/77ruv1O89PT3l6elZVSWXqzgzQ8WFkv/0FxTY///VSJ+oWrmHD+qHqY+o8MKvhBsAqKNMv3YSHR2tcePGqXfv3goJCdHChQuVm5trv3tq7NixatWqlebNmydJmjt3rvr27avbb79dGRkZev3113X8+HFNnDjRzGE48GgTpAbdgs0uAwCAW5Lp4eaBBx7Q2bNnNWvWLKWlpalnz576/PPP7ZOMT5w4IReX/3vW4IULFzRp0iSlpaXJz89Pd911l7Zt26YuXbqYNQQAAFCLmB5uJCkyMrLMy1CJiYkOy7GxsYqNja2BqgAAQF1UK16/AAAAUFUINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFLczC7AagobNlbWZens+XNml4JKyL14SYUNG5tdBgDgJhBuqtClIunI1Fd0OMumnZ9tMLscVJJt6ivqXlCgBmYXAgCoFMJNFSo0JMPDU93qG+ry22Fml4NKOP39d/rmxCnlXy4yuxQAQCURbqpBfVepaeMmZpeBSsj19JQkZebnc2mxDsolkwIQ4QZw4OnmKltBvr45cUo6ccrscuA0m2wF+XK3eZhdCAATEW6Aa/h4eKj9wufV6Z3/Vf3b7zC7HDjp+Nb1OvfX51XvlQVmlwLARIQb4DrumefVxLueGnBpsc654CZlZZ43uwwAJuM5NwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFJ4QjEAoNbJPXzQ7BJQSbkXL5ldAuEGAFB7uPvdJpd63vph6iNml4JKutQyUJoyV/ln0iSTXmNDuAEA1Bperdqo76YfVHjhV7NLQSX9cugHHcszdDkrw7QaCDcAgFrFq1UbebVqY3YZqKR6Fy9JKT+bWgMTigEAgKUQbgAAgKUQbgAAgKUQbgAAgKUQbgAAgKVwtxQAy/k157J0Js/sMlBJfkaqmnpkml0GKiv3uNkVEG4AWEc99ysno+P3Z2nX6ZMmV4PK8HdJ00r/+yUb4bTOSmspNZkh5Z8zrQTCDQDLaOTtKkmaGNpYE+8KMLkaVMb5X87I63CeUjsvUovW3cwuB5Wx/UvpgqTLOaaVQLgBYDktGrqrgb+X2WWgEo5nu0uSCrw7Sn49zS0GlVPvB+lCkaklMKEYAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYSq0IN4sWLVJQUJC8vLzUp08f7dq1q9z2a9as0R133CEvLy/deeed2rBhQw1VCgAAajvTw80///lPRUdHa/bs2fr222/Vo0cPhYeH68yZM6W237Ztmx588EE99thj2rt3ryIiIhQREaEDBw7UcOUAAKA2Mj3cLFiwQJMmTdKjjz6qLl26aMmSJfL29ta7775bavs33nhDQ4YM0XPPPafOnTvrxRdfVHBwsN56660arhwAANRGpoabgoIC7dmzR4MGDbKvc3Fx0aBBg7R9+/ZSt9m+fbtDe0kKDw8vsz0AALi1uJnZ+a+//qqioiI1a9bMYX2zZs30448/lrpNWlpaqe3T0tJKbZ+fn6/8/Hz7cmZmpiQpKyvrZkovVc7FS7p00aYcN6Na9o/ql52To9xiQ2n79igrJ8fscuCki98fVG6xoawzP8k4/o3Z5aAS8s8cVNZFQ4dPZeicSp+egNot60y2Ll20KTvXpUr/Fl7dl2EYN2xraripCfPmzdOcOXNKrA8ICKjmnp+o5v2jWo153OwKcDNGviDpBZOLwM0JN7sA3KTnJEmTqny/2dnZatiwYbltTA03t912m1xdXZWenu6wPj09Xc2bNy91m+bNmzvVfvr06YqOjrYvFxcX6/z582rSpIlsNttNjsBRVlaWAgICdPLkSfn6+lbpvmsDq49Psv4YGV/dZ/UxMr66r7rGaBiGsrOz1bJlyxu2NTXceHh46K677lJCQoIiIiIkXQkfCQkJioyMLHWb0NBQJSQkaOrUqfZ1GzduVGhoaKntPT095enp6bCuUaNGVVF+mXx9fS37L61k/fFJ1h8j46v7rD5Gxlf3VccYb3TG5irTL0tFR0dr3Lhx6t27t0JCQrRw4ULl5ubq0UcflSSNHTtWrVq10rx58yRJUVFRGjBggGJiYjRs2DB98MEHSkpK0ttvv23mMAAAQC1herh54IEHdPbsWc2aNUtpaWnq2bOnPv/8c/uk4RMnTsjF5f9u6urXr5/ef/99/fWvf9WMGTPUoUMHxcfHq1u3bmYNAQAA1CKmhxtJioyMLPMyVGJiYol1o0eP1ujRo6u5Kud5enpq9uzZJS6DWYXVxydZf4yMr+6z+hgZX91XG8ZoMypyTxUAAEAdYfoTigEAAKoS4QYAAFgK4QYAAFgK4aaCNm/erOHDh6tly5ay2WyKj4+/4TaJiYkKDg6Wp6enbr/9dsXFxVV7nTfD2TEmJibKZrOV+JT1KgyzzZs3T3fffbcaNGggf39/RURE6NChQzfcbs2aNbrjjjvk5eWlO++8Uxs2bKiBap1XmfHFxcWVOH5eXl41VLFzFi9erO7du9ufnREaGqrPPvus3G3qyrG7ytkx1qXjV5pXXnlFNpvN4bllpalrx/Gqioyvrh3DF154oUS9d9xxR7nbmHH8CDcVlJubqx49emjRokUVan/06FENGzZM99xzj5KTkzV16lRNnDhR//73v6u50spzdoxXHTp0SKmpqfaPv79/NVV4c77++mtNmTJFO3bs0MaNG1VYWKjf/e53ys3NLXObbdu26cEHH9Rjjz2mvXv3KiIiQhERETpw4EANVl4xlRmfdOVBW9cev+PHj9dQxc5p3bq1XnnlFe3Zs0dJSUkaOHCgRowYoe+//77U9nXp2F3l7BilunP8rrd7924tXbpU3bt3L7ddXTyOUsXHJ9W9Y9i1a1eHer/5puz3uJl2/Aw4TZKxdu3acttMmzbN6Nq1q8O6Bx54wAgPD6/GyqpORcb41VdfGZKMCxcu1EhNVe3MmTOGJOPrr78us80f//hHY9iwYQ7r+vTpY0yePLm6y7tpFRnf8uXLjYYNG9ZcUVXMz8/PWLZsWanf1eVjd63yxlhXj192drbRoUMHY+PGjcaAAQOMqKioMtvWxePozPjq2jGcPXu20aNHjwq3N+v4ceammmzfvl2DBg1yWBceHq7t27ebVFH16dmzp1q0aKHBgwdr69atZpdTYVffEN+4ceMy29Tl41iR8UlSTk6OAgMDFRAQcMOzBLVFUVGRPvjgA+Xm5pb56pW6fOykio1RqpvHb8qUKRo2bFiJ41OaungcnRmfVPeOYUpKilq2bKl27dppzJgxOnHiRJltzTp+teIhflaUlpZmf8ryVc2aNVNWVpYuXbqkevXqmVRZ1WnRooWWLFmi3r17Kz8/X8uWLVNYWJh27typ4OBgs8srV3FxsaZOnar+/fuX+3Trso5jbZ1XdFVFx9epUye9++676t69uzIzMzV//nz169dP33//vVq3bl2DFVfM/v37FRoaqry8PPn4+Gjt2rXq0qVLqW3r6rFzZox17fhJ0gcffKBvv/1Wu3fvrlD7unYcnR1fXTuGffr0UVxcnDp16qTU1FTNmTNHv/nNb3TgwAE1aNCgRHuzjh/hBpXWqVMnderUyb7cr18/HTlyRLGxsVq1apWJld3YlClTdODAgXKvFddlFR1faGiow1mBfv36qXPnzlq6dKlefPHF6i7TaZ06dVJycrIyMzP10Ucfady4cfr666/L/ONfFzkzxrp2/E6ePKmoqCht3LixVk+arazKjK+uHcOhQ4fa/7l79+7q06ePAgMD9eGHH+qxxx4zsTJHhJtq0rx5c6WnpzusS09Pl6+vryXO2pQlJCSk1geGyMhIffrpp9q8efMN/8uorOPYvHnz6izxpjgzvuu5u7urV69eOnz4cDVVd3M8PDx0++23S5Luuusu7d69W2+88YaWLl1aom1dPHaSc2O8Xm0/fnv27NGZM2cczuwWFRVp8+bNeuutt5Sfny9XV1eHberScazM+K5X24/h9Ro1aqSOHTuWWa9Zx485N9UkNDRUCQkJDus2btxY7rVzK0hOTlaLFi3MLqNUhmEoMjJSa9eu1Zdffqm2bdvecJu6dBwrM77rFRUVaf/+/bX2GF6vuLhY+fn5pX5Xl45decob4/Vq+/G79957tX//fiUnJ9s/vXv31pgxY5ScnFzqH/66dBwrM77r1fZjeL2cnBwdOXKkzHpNO37VOl3ZQrKzs429e/cae/fuNSQZCxYsMPbu3WscP37cMAzDeP75541HHnnE3v7nn382vL29jeeee844ePCgsWjRIsPV1dX4/PPPzRrCDTk7xtjYWCM+Pt5ISUkx9u/fb0RFRRkuLi7Gpk2bzBpCuZ588kmjYcOGRmJiopGammr/XLx40d7mkUceMZ5//nn78tatWw03Nzdj/vz5xsGDB43Zs2cb7u7uxv79+80YQrkqM745c+YY//73v40jR44Ye/bsMf70pz8ZXl5exvfff2/GEMr1/PPPG19//bVx9OhRY9++fcbzzz9v2Gw244svvjAMo24fu6ucHWNdOn5luf5uIiscx2vdaHx17Rg+88wzRmJionH06FFj69atxqBBg4zbbrvNOHPmjGEYtef4EW4q6Optz9d/xo0bZxiGYYwbN84YMGBAiW169uxpeHh4GO3atTOWL19e43U7w9kxvvrqq0b79u0NLy8vo3HjxkZYWJjx5ZdfmlN8BZQ2NkkOx2XAgAH28V714YcfGh07djQ8PDyMrl27GuvXr6/ZwiuoMuObOnWq0aZNG8PDw8No1qyZcd999xnffvttzRdfARMmTDACAwMNDw8Po2nTpsa9995r/6NvGHX72F3l7Bjr0vEry/V//K1wHK91o/HVtWP4wAMPGC1atDA8PDyMVq1aGQ888IBx+PBh+/e15fjxVnAAAGApzLkBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBUGPi4uLUqFEjs8uoVkFBQVq4cKHZZQC3NMINcAs7e/asnnzySbVp00aenp5q3ry5wsPDtXXrVnsbm82m+Ph4p/dd2h/5Bx54QD/99NNNVg0A5XMzuwAA5hk1apQKCgq0YsUKtWvXTunp6UpISNC5c+eqpb969eqpXr161bJvKysoKJCHh4fZZQB1BmdugFtURkaGtmzZoldffVX33HOPAgMDFRISounTp+v3v/+9pCtnXyRp5MiRstls9uUjR45oxIgRatasmXx8fHT33Xdr06ZN9n2HhYXp+PHjevrpp2Wz2WSz2SSVfllq8eLFat++vTw8PNSpUyetWrXK4XubzaZly5Zp5MiR8vb2VocOHbRu3bpyxxYUFKSXX35ZEyZMUIMGDdSmTRu9/fbb9u8TExNls9mUkZFhX5ecnCybzaZjx4451Prpp5+qU6dO8vb21v3336+LFy9qxYoVCgoKkp+fn/785z+rqKjIof/s7Gw9+OCDql+/vlq1aqVFixaV+O0nTpyopk2bytfXVwMHDtR3331n//6FF15Qz549tWzZMrVt21ZeXl7ljheAI8INcIvy8fGRj4+P4uPjlZ+fX2qb3bt3S5KWL1+u1NRU+3JOTo7uu+8+JSQkaO/evRoyZIiGDx+uEydOSJI+/vhjtW7dWnPnzlVqaqpSU1NL3f/atWsVFRWlZ555RgcOHNDkyZP16KOP6quvvnJoN2fOHP3xj3/Uvn37dN9992nMmDE6f/58ueOLiYlR7969tXfvXv3nf/6nnnzySR06dMip3+jixYv6n//5H33wwQf6/PPPlZiYqJEjR2rDhg3asGGDVq1apaVLl+qjjz5y2O71119Xjx49tHfvXj3//POKiorSxo0b7d+PHj1aZ86c0WeffaY9e/YoODhY9957r8OYDh8+rH/961/6+OOPlZyc7FTdwC2v2t87DqDW+uijjww/Pz/Dy8vL6NevnzF9+nTju+++c2gjyVi7du0N99W1a1fjzTfftC8HBgYasbGxDm2WL19uNGzY0L7cr18/Y9KkSQ5tRo8ebdx3330O/f/1r3+1L+fk5BiSjM8++6zMWgIDA42HH37YvlxcXGz4+/sbixcvNgzDML766itDknHhwgV7m7179xqSjKNHj9prlWQcPnzY3mby5MmGt7e3kZ2dbV8XHh5uTJ482aHvIUOGONTzwAMPGEOHDjUMwzC2bNli+Pr6Gnl5eQ5t2rdvbyxdutQwDMOYPXu24e7ubpw5c6bMMQIoG2dugFvYqFGjdPr0aa1bt05DhgxRYmKigoODFRcXV+52OTk5evbZZ9W5c2c1atRIPj4+OnjwoP3MTUUdPHhQ/fv3d1jXv39/HTx40GFd9+7d7f9cv359+fr66syZM+Xu+9ptbDabmjdvfsNtruft7a327dvbl5s1a6agoCD5+Pg4rLt+v6GhoSWWr47pu+++U05Ojpo0aWI/e+bj46OjR4/qyJEj9m0CAwPVtGlTp+oFcAUTioFbnJeXlwYPHqzBgwdr5syZmjhxombPnq3x48eXuc2zzz6rjRs3av78+br99ttVr1493X///SooKKiWGt3d3R2WbTabiouLK72Ni8uV/64zDMP+fWFhYYX2UZlarpWTk6MWLVooMTGxxHfXzkeqX79+hfcJwBHhBoCDLl26ONz67e7uXmLC7NatWzV+/HiNHDlS0pU/2Fcn4l7l4eFRYrvrde7cWVu3btW4ceMc9t2lS5ebG8QNXD0jkpqaKj8/P0mq0nktO3bsKLHcuXNnSVJwcLDS0tLk5uZmn6ANoGpxWQq4RZ07d04DBw7Ue++9p3379uno0aNas2aNXnvtNY0YMcLeLigoSAkJCUpLS9OFCxckSR06dLBPdP3uu+/00EMPlTh7ERQUpM2bN+vUqVP69ddfS63hueeeU1xcnBYvXqyUlBQtWLBAH3/8sZ599tnqG7ik22+/XQEBAXrhhReUkpKi9evXKyYmpsr2v3XrVr322mv66aeftGjRIq1Zs0ZRUVGSpEGDBik0NFQRERH64osvdOzYMW3btk1/+ctflJSUVGU1ALcywg1wi/Lx8VGfPn0UGxur3/72t+rWrZtmzpypSZMm6a233rK3i4mJ0caNGxUQEKBevXpJkhYsWCA/Pz/169dPw4cPV3h4uIKDgx32P3fuXB07dkzt27cvc+5IRESE3njjDc2fP19du3bV0qVLtXz5coWFhVXbuKUrZ6NWr16tH3/8Ud27d9err76ql156qcr2/8wzzygpKUm9evXSSy+9pAULFig8PFzSlctYGzZs0G9/+1s9+uij6tixo/70pz/p+PHjatasWZXVANzKbMa1F50BAADqOM7cAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAAS/n/AGDjqjVAh2CIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stNum plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_stNum\"].plot.hist(bins=range(1, 6), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Station number\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/stNum.png\")\n",
    "plt.show()\n",
    "\n",
    "#scNum plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_scNum\"].plot.hist(bins=range(1, 7), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Sector number\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/scNum.png\")\n",
    "plt.show()\n",
    "\n",
    "#whNum plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_whNum\"].plot.hist(bins=range(-3, 4), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Wheel number\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/whNum.png\")\n",
    "plt.show()\n",
    "\n",
    "#eta_1 plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_eta_1\"].plot.hist(bins=100, histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Eta 1\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/eta_1.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#qeta_1 plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_qeta_1\"].plot.hist(bins=range(-1, 2), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Qeta 1\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/qeta_1.png\")\n",
    "plt.show()\n",
    "\n",
    "#eta_2 plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_eta_2\"].plot.hist(bins=100, histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Eta 2\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/eta_2.png\")\n",
    "plt.show()\n",
    "\n",
    "#qeta_2 plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_qeta_2\"].plot.hist(bins=range(-1, 2), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Qeta 2\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/qeta_2.png\")\n",
    "plt.show()\n",
    "\n",
    "#tag plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_tag\"].plot.hist(bins=range(0, 2), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Tag\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/tag.png\")\n",
    "plt.show()\n",
    "\n",
    "#phi plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_phi\"].plot.hist(bins=100, histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Phi\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/phi.png\")\n",
    "plt.show()\n",
    "\n",
    "#phiB plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_phiB\"].plot.hist(bins=100, histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"PhiB\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/phiB.png\")\n",
    "plt.show()\n",
    "\n",
    "#quality plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 5):\n",
    "    full_data[f\"s{i}_quality\"].plot.hist(bins=range(0, 8), histtype='step', ax=ax, label=f\"Stub {i}\")\n",
    "ax.set_xlabel(\"Quality\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"{OUT_PATH}/Features/quality.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#ptRecoInverse plot\n",
    "fig, ax = plt.subplots()\n",
    "full_data[\"ptRecoInverse\"].plot.hist(bins=100, histtype='step', ax=ax)\n",
    "ax.set_xlabel(\"1 / Pt Reco\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "plt.savefig(f\"{OUT_PATH}/Features/ptRecoInverse.png\")\n",
    "plt.show()\n",
    "\n",
    "#etaExtRecoSt2 plot\n",
    "fig, ax = plt.subplots()\n",
    "full_data[\"etaExtRecoSt2\"].plot.hist(bins=100, histtype='step', ax=ax)\n",
    "ax.set_xlabel(\"Eta Reco\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "plt.savefig(f\"{OUT_PATH}/Features/etaExtRecoSt2.png\")\n",
    "plt.show()\n",
    "\n",
    "#phiExtRecoSt2 plot\n",
    "fig, ax = plt.subplots()\n",
    "full_data[\"phiExtRecoSt2\"].plot.hist(bins=100, histtype='step', ax=ax)\n",
    "ax.set_xlabel(\"Phi Reco\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "plt.savefig(f\"{OUT_PATH}/Features/phiExtRecoSt2.png\")\n",
    "plt.show()\n",
    "\n",
    "#chargeReco plot\n",
    "fig, ax = plt.subplots()\n",
    "full_data[\"chargeReco\"].plot.hist(bins=range(0, 2), histtype='step', ax=ax)\n",
    "ax.set_xlabel(\"Charge Reco\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "plt.savefig(f\"{OUT_PATH}/Features/chargeReco.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_stubs</th>\n",
       "      <th>s1_stNum</th>\n",
       "      <th>s1_scNum</th>\n",
       "      <th>s1_whNum</th>\n",
       "      <th>s1_eta_1</th>\n",
       "      <th>s1_qeta_1</th>\n",
       "      <th>s1_eta_2</th>\n",
       "      <th>s1_qeta_2</th>\n",
       "      <th>s1_tag</th>\n",
       "      <th>s1_phi</th>\n",
       "      <th>...</th>\n",
       "      <th>s4_phiB</th>\n",
       "      <th>s4_quality</th>\n",
       "      <th>ptRecoInverse</th>\n",
       "      <th>etaExtRecoSt2</th>\n",
       "      <th>phiExtRecoSt2</th>\n",
       "      <th>chargeReco</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>hwSignL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-52</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041279</td>\n",
       "      <td>-0.555335</td>\n",
       "      <td>-1.409220</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.554625</td>\n",
       "      <td>-1.418080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.608174</td>\n",
       "      <td>2.628690</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>2.628900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-14</td>\n",
       "      <td>2</td>\n",
       "      <td>-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>722</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>-0.185071</td>\n",
       "      <td>1.230710</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.184875</td>\n",
       "      <td>1.232640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-46</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>-0.485899</td>\n",
       "      <td>-1.515270</td>\n",
       "      <td>0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>-0.467625</td>\n",
       "      <td>-1.527160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1202</td>\n",
       "      <td>...</td>\n",
       "      <td>-18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.037988</td>\n",
       "      <td>0.738465</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>0.818123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426409</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1116</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>-2.863660</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>-2.868880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426410</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>908</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.790361</td>\n",
       "      <td>-1.350280</td>\n",
       "      <td>1</td>\n",
       "      <td>255.5</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>-1.352630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426411</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-232</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.052273</td>\n",
       "      <td>-0.436209</td>\n",
       "      <td>-2.690840</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.424125</td>\n",
       "      <td>-2.694350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426412</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-232</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.075172</td>\n",
       "      <td>-0.440287</td>\n",
       "      <td>-2.716570</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.424125</td>\n",
       "      <td>-2.694350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426413</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>646</td>\n",
       "      <td>...</td>\n",
       "      <td>-8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.132354</td>\n",
       "      <td>0.306403</td>\n",
       "      <td>2.773520</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.315375</td>\n",
       "      <td>2.770710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2925441 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_stubs  s1_stNum  s1_scNum  s1_whNum  s1_eta_1  s1_qeta_1  s1_eta_2  \\\n",
       "0              4         1         9        -1       -52          2       255   \n",
       "2              3         1         5         1         7          0         7   \n",
       "3              2         1         2         0       -14          2       -21   \n",
       "4              4         1         9        -1       -46          2       255   \n",
       "5              3         1         1         1         7          0         7   \n",
       "...          ...       ...       ...       ...       ...        ...       ...   \n",
       "3426409        3         1         6         1         7          0         7   \n",
       "3426410        2         3         9         2        74          2       255   \n",
       "3426411        4         1         7        -1       -40          2       255   \n",
       "3426412        4         1         7        -1       -40          2       255   \n",
       "3426413        3         2         5         1         7          0         7   \n",
       "\n",
       "         s1_qeta_2  s1_tag  s1_phi  ...  s4_phiB  s4_quality  ptRecoInverse  \\\n",
       "0                0       1     632  ...        7           5       0.041279   \n",
       "2                0       1      10  ...       12           2       0.052357   \n",
       "3                1       1     722  ...       18           5       0.042819   \n",
       "4                0       1     140  ...        9           6       0.058421   \n",
       "5                0       1    1202  ...      -18           3       0.037988   \n",
       "...            ...     ...     ...  ...      ...         ...            ...   \n",
       "3426409          0       1    1116  ...        6           5       0.027158   \n",
       "3426410          0       1     908  ...       -9           2       0.018035   \n",
       "3426411          0       1    -232  ...       -9           6       0.052273   \n",
       "3426412          0       1    -232  ...       -9           6       0.075172   \n",
       "3426413          0       1     646  ...       -8           6       0.132354   \n",
       "\n",
       "         etaExtRecoSt2  phiExtRecoSt2  chargeReco   ptL1     etaL1     phiL1  \\\n",
       "0            -0.555335      -1.409220           0   29.0 -0.554625 -1.418080   \n",
       "2             0.608174       2.628690           0   17.5  0.598125  2.628900   \n",
       "3            -0.185071       1.230710           0   30.0 -0.184875  1.232640   \n",
       "4            -0.485899      -1.515270           0   20.5 -0.467625 -1.527160   \n",
       "5             0.738465       0.816011           0   31.0  0.750375  0.818123   \n",
       "...                ...            ...         ...    ...       ...       ...   \n",
       "3426409       0.750462      -2.863660           0   59.0  0.739500 -2.868880   \n",
       "3426410       0.790361      -1.350280           1  255.5  0.804750 -1.352630   \n",
       "3426411      -0.436209      -2.690840           1   22.0 -0.424125 -2.694350   \n",
       "3426412      -0.440287      -2.716570           1   22.0 -0.424125 -2.694350   \n",
       "3426413       0.306403       2.773520           0    8.5  0.315375  2.770710   \n",
       "\n",
       "         hwSignL1  \n",
       "0               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "5               1  \n",
       "...           ...  \n",
       "3426409         1  \n",
       "3426410         0  \n",
       "3426411         0  \n",
       "3426412         0  \n",
       "3426413         1  \n",
       "\n",
       "[2925441 rows x 53 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(full_data,      test_size=0.3, random_state=42)\n",
    "train_data, val_data      = train_test_split(train_val_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 1843027\n",
      "Val dataset: 204781\n",
      "Test dataset: 877633\n"
     ]
    }
   ],
   "source": [
    "# inspect the datasets\n",
    "print(f\"Train dataset: {len(train_data)}\")\n",
    "print(f\"Val dataset: {len(val_data)}\")\n",
    "print(f\"Test dataset: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "for key in normalizations.keys():\n",
    "    train_data[key] = train_data[key] / normalizations[key]\n",
    "    val_data[key]   = val_data[key]   / normalizations[key]\n",
    "    test_data[key]  = test_data[key]  / normalizations[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 09:31:03.911211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10518 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2024-07-05 09:31:03.912690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10532 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size    = len(stub_features)\n",
    "architecture  = [input_size, 64, 32, 16, 8]\n",
    "output_size   = len(target_features)\n",
    "learning_rate = 1e-2\n",
    "num_epochs    = 300\n",
    "batch_size    = 2**8\n",
    "reg_strength  = 1e-3\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor = 0.5\n",
    "patience = 5\n",
    "min_loss_improvement = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "patience = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "classification_weight = 1\n",
    "regression_weight     = 6\n",
    "\n",
    "# Loss and optimizer\n",
    "# regression_criterion     = losses.MeanSquaredError()\n",
    "classification_criterion = losses.BinaryCrossentropy(from_logits=True)\n",
    "regression_criterion     = losses.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "scheduler = CustomLRScheduler(\n",
    "    optimizer, \n",
    "    factor=scale_factor, \n",
    "    patience=patience, \n",
    "    min_improvement=min_loss_improvement, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data[stub_features].values, train_data[target_features].values)).batch(batch_size).shuffle(buffer_size=len(train_data))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((val_data[stub_features].values, val_data[target_features].values)).batch(batch_size)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((test_data[stub_features].values, test_data[target_features].values)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5724\n",
      "Number of trainable parameters: 5724\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = MultiTaskNN(architecture, reg_strength=reg_strength)\n",
    "\n",
    "# Build the model with the batch input shape\n",
    "bs = None  # None allows for variable batch size\n",
    "model.build((bs, input_size))\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params = model.count_params()\n",
    "trainable_vars = [var for var in model.trainable_variables]\n",
    "trainable_params = sum([tf.size(var).numpy() for var in trainable_vars])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=True)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return regression_loss, classification_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(features, targets):\n",
    "    regression_targets = targets[:, :-1]\n",
    "    classification_targets = targets[:, -1]\n",
    "    reg_outputs, class_outputs = model(features, training=False)\n",
    "    regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "    classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "    loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "    return regression_loss, classification_loss, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 09:31:14.535733: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f76b92a9230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-05 09:31:14.535788: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-07-05 09:31:14.535803: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-07-05 09:31:14.544564: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-05 09:31:14.574311: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720164674.683939 3231168 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0276, Classification: 0.1422, Combined: 0.3078\n",
      "Validation Losses - Regression: 0.0207, Classification: 0.1306, Combined: 0.2546\n",
      "-------------\n",
      "Epoch [2/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0172, Classification: 0.1269, Combined: 0.2300\n",
      "Validation Losses - Regression: 0.0154, Classification: 0.1230, Combined: 0.2153\n",
      "-------------\n",
      "Epoch [3/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0162, Classification: 0.1233, Combined: 0.2205\n",
      "Validation Losses - Regression: 0.0169, Classification: 0.1237, Combined: 0.2251\n",
      "-------------\n",
      "Epoch [4/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0156, Classification: 0.1219, Combined: 0.2155\n",
      "Validation Losses - Regression: 0.0164, Classification: 0.1214, Combined: 0.2198\n",
      "-------------\n",
      "Epoch [5/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0154, Classification: 0.1213, Combined: 0.2138\n",
      "Validation Losses - Regression: 0.0144, Classification: 0.1267, Combined: 0.2130\n",
      "-------------\n",
      "Epoch [6/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0154, Classification: 0.1204, Combined: 0.2127\n",
      "Validation Losses - Regression: 0.0146, Classification: 0.1189, Combined: 0.2064\n",
      "-------------\n",
      "Epoch [7/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0152, Classification: 0.1197, Combined: 0.2107\n",
      "Validation Losses - Regression: 0.0139, Classification: 0.1207, Combined: 0.2039\n",
      "-------------\n",
      "Epoch [8/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0150, Classification: 0.1194, Combined: 0.2093\n",
      "Validation Losses - Regression: 0.0140, Classification: 0.1203, Combined: 0.2045\n",
      "-------------\n",
      "Epoch [9/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0151, Classification: 0.1193, Combined: 0.2102\n",
      "Validation Losses - Regression: 0.0143, Classification: 0.1214, Combined: 0.2074\n",
      "-------------\n",
      "Epoch [10/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0151, Classification: 0.1187, Combined: 0.2094\n",
      "Validation Losses - Regression: 0.0158, Classification: 0.1152, Combined: 0.2100\n",
      "-------------\n",
      "Epoch [11/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0150, Classification: 0.1178, Combined: 0.2076\n",
      "Validation Losses - Regression: 0.0151, Classification: 0.1255, Combined: 0.2158\n",
      "-------------\n",
      "Epoch [12/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0149, Classification: 0.1173, Combined: 0.2066\n",
      "Validation Losses - Regression: 0.0140, Classification: 0.1154, Combined: 0.1996\n",
      "-------------\n",
      "Decreasing learning rate to 0.004999999888241291\n",
      "Epoch [13/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1117, Combined: 0.1869\n",
      "Validation Losses - Regression: 0.0122, Classification: 0.1147, Combined: 0.1878\n",
      "-------------\n",
      "Epoch [14/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1109, Combined: 0.1858\n",
      "Validation Losses - Regression: 0.0123, Classification: 0.1119, Combined: 0.1856\n",
      "-------------\n",
      "Epoch [15/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1103, Combined: 0.1853\n",
      "Validation Losses - Regression: 0.0126, Classification: 0.1120, Combined: 0.1876\n",
      "-------------\n",
      "Epoch [16/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1100, Combined: 0.1848\n",
      "Validation Losses - Regression: 0.0124, Classification: 0.1118, Combined: 0.1865\n",
      "-------------\n",
      "Epoch [17/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1097, Combined: 0.1845\n",
      "Validation Losses - Regression: 0.0121, Classification: 0.1095, Combined: 0.1821\n",
      "-------------\n",
      "Epoch [18/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1093, Combined: 0.1839\n",
      "Validation Losses - Regression: 0.0129, Classification: 0.1102, Combined: 0.1874\n",
      "-------------\n",
      "Epoch [19/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1092, Combined: 0.1838\n",
      "Validation Losses - Regression: 0.0131, Classification: 0.1107, Combined: 0.1892\n",
      "-------------\n",
      "Epoch [20/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1095, Combined: 0.1846\n",
      "Validation Losses - Regression: 0.0124, Classification: 0.1125, Combined: 0.1871\n",
      "-------------\n",
      "Epoch [21/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1090, Combined: 0.1836\n",
      "Validation Losses - Regression: 0.0129, Classification: 0.1141, Combined: 0.1916\n",
      "-------------\n",
      "Epoch [22/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1089, Combined: 0.1833\n",
      "Validation Losses - Regression: 0.0122, Classification: 0.1087, Combined: 0.1820\n",
      "-------------\n",
      "Epoch [23/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1088, Combined: 0.1832\n",
      "Validation Losses - Regression: 0.0123, Classification: 0.1090, Combined: 0.1828\n",
      "-------------\n",
      "Decreasing learning rate to 0.0024999999441206455\n",
      "Epoch [24/300]\n",
      "Learning rate: 2.50e-03\n",
      "Train Losses - Regression: 0.0114, Classification: 0.1061, Combined: 0.1746\n",
      "Validation Losses - Regression: 0.0113, Classification: 0.1088, Combined: 0.1763\n",
      "-------------\n",
      "Decreasing learning rate to 0.0012499999720603228\n",
      "Epoch [25/300]\n",
      "Learning rate: 1.25e-03\n",
      "Train Losses - Regression: 0.0110, Classification: 0.1045, Combined: 0.1704\n",
      "Validation Losses - Regression: 0.0110, Classification: 0.1049, Combined: 0.1707\n",
      "-------------\n",
      "Decreasing learning rate to 0.0006249999860301614\n",
      "Epoch [26/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0108, Classification: 0.1037, Combined: 0.1683\n",
      "Validation Losses - Regression: 0.0108, Classification: 0.1043, Combined: 0.1693\n",
      "-------------\n",
      "Decreasing learning rate to 0.0003124999930150807\n",
      "Epoch [27/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1032, Combined: 0.1671\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1042, Combined: 0.1678\n",
      "-------------\n",
      "Epoch [28/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1032, Combined: 0.1670\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1041, Combined: 0.1681\n",
      "-------------\n",
      "Epoch [29/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1031, Combined: 0.1669\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1041, Combined: 0.1680\n",
      "-------------\n",
      "Epoch [30/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1031, Combined: 0.1668\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1677\n",
      "-------------\n",
      "Epoch [31/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1667\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1040, Combined: 0.1676\n",
      "-------------\n",
      "Epoch [32/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1667\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1674\n",
      "-------------\n",
      "Epoch [33/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1666\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1039, Combined: 0.1674\n",
      "-------------\n",
      "Epoch [34/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1666\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1674\n",
      "-------------\n",
      "Epoch [35/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1665\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1040, Combined: 0.1676\n",
      "-------------\n",
      "Epoch [36/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1029, Combined: 0.1665\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1039, Combined: 0.1676\n",
      "-------------\n",
      "Epoch [37/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1029, Combined: 0.1665\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1671\n",
      "-------------\n",
      "Decreasing learning rate to 0.00015624999650754035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# scheduler.on_epoch_begin(epoch, logs=None)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, targets \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m---> 25\u001b[0m     regression_loss, classification_loss, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     running_loss                \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     28\u001b[0m     running_regression_loss     \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m regression_loss\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_regression_losses     = []\n",
    "train_classification_losses = []\n",
    "train_combined_losses       = []\n",
    "val_regression_losses       = []\n",
    "val_classification_losses   = []\n",
    "val_combined_losses         = []\n",
    "\n",
    "learning_rates = []\n",
    "\n",
    "out_file = open(os.path.join(OUT_PATH, LOSS_FNAME), \"w\")\n",
    "out_file.write(\"train_regression_loss,train_classification_loss,train_combined_loss,val_regression_loss,val_classification_loss,val_combined_loss,learning_rate\\n\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    running_loss                = 0.0\n",
    "    running_regression_loss     = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    # scheduler.on_epoch_begin(epoch, logs=None)\n",
    "    \n",
    "    for features, targets in train_dataset:\n",
    "        \n",
    "        regression_loss, classification_loss, loss = train_step(features, targets)\n",
    "        \n",
    "        running_loss                += loss.numpy()\n",
    "        running_regression_loss     += regression_loss.numpy()\n",
    "        running_classification_loss += classification_loss.numpy()\n",
    "        \n",
    "        # scheduler.on_batch_end(batch=None)\n",
    "        \n",
    "        # current_lr = optimizer.lr.numpy()\n",
    "        \n",
    "        # lr_per_batch.append(current_lr)\n",
    "        # print(f\"Learning rate: {current_lr:.2e}\", end=\"\\r\")\n",
    "        \n",
    "\n",
    "        \n",
    "    # Note: Remember to adjust if not using batches of equal sizes\n",
    "    train_loss                = running_loss                / len(train_dataset)\n",
    "    train_regression_loss     = running_regression_loss     / len(train_dataset)\n",
    "    train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "\n",
    "    train_regression_losses.append(train_regression_loss)\n",
    "    train_classification_losses.append(train_classification_loss)\n",
    "    train_combined_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    running_loss                = 0.0\n",
    "    running_regression_loss     = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    for features, targets in val_dataset:\n",
    "        \n",
    "        regression_loss, classification_loss, loss = val_step(features, targets)\n",
    "        \n",
    "        running_loss                += loss.numpy()\n",
    "        running_regression_loss     += regression_loss.numpy()\n",
    "        running_classification_loss += classification_loss.numpy()\n",
    "    \n",
    "    avg_val_loss                = running_loss                / len(val_dataset)\n",
    "    avg_val_regression_loss     = running_regression_loss     / len(val_dataset)\n",
    "    avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "        \n",
    "    val_regression_losses.append(avg_val_regression_loss)\n",
    "    val_classification_losses.append(avg_val_classification_loss)\n",
    "    val_combined_losses.append(avg_val_loss)\n",
    "    \n",
    "    \n",
    "    current_lr = optimizer.lr.numpy()\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Learning rate: {current_lr:.2e}\")\n",
    "    print(f\"Train Losses - Regression: {train_regression_losses[-1]:.4f}, Classification: {train_classification_losses[-1]:.4f}, Combined: {train_combined_losses[-1]:.4f}\")\n",
    "    print(f\"Validation Losses - Regression: {val_regression_losses[-1]:.4f}, Classification: {val_classification_losses[-1]:.4f}, Combined: {val_combined_losses[-1]:.4f}\")\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    \n",
    "    scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_loss})\n",
    "\n",
    "\n",
    "    with open(os.path.join(OUT_PATH, LOSS_FNAME), \"a\") as output_file:\n",
    "        output_file.write(f\"{train_regression_losses[-1]},{train_classification_losses[-1]},{train_combined_losses[-1]},{val_regression_losses[-1]},{val_classification_losses[-1]},{val_combined_losses[-1]},{current_lr}\\n\")\n",
    "\n",
    "    # Check for early stopping based on the new criterion\n",
    "    if avg_val_loss < (1 - early_stopping_threshold) * best_val_loss:  # 0.001 corresponds to 0.1%\n",
    "        epochs_without_improvement = 0\n",
    "        best_val_loss = min(best_val_loss, avg_val_loss)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test data from `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptReco_true</th>\n",
       "      <th>etaReco_true</th>\n",
       "      <th>phiReco_true</th>\n",
       "      <th>chargeReco_true</th>\n",
       "      <th>ptReco_pred</th>\n",
       "      <th>etaReco_pred</th>\n",
       "      <th>phiReco_pred</th>\n",
       "      <th>chargeReco_pred</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>chargeL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.14094</td>\n",
       "      <td>0.725395</td>\n",
       "      <td>1.863000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.845138</td>\n",
       "      <td>0.742052</td>\n",
       "      <td>1.876867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>1.887140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.94190</td>\n",
       "      <td>-0.825676</td>\n",
       "      <td>-2.726760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.148404</td>\n",
       "      <td>-0.828740</td>\n",
       "      <td>-2.722502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.804750</td>\n",
       "      <td>-2.727080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.79640</td>\n",
       "      <td>0.143982</td>\n",
       "      <td>-0.391065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.846815</td>\n",
       "      <td>0.145370</td>\n",
       "      <td>-0.400197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.5</td>\n",
       "      <td>0.141375</td>\n",
       "      <td>-0.414516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.03724</td>\n",
       "      <td>0.803833</td>\n",
       "      <td>1.414660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.492181</td>\n",
       "      <td>0.817047</td>\n",
       "      <td>1.398779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>1.396260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.51210</td>\n",
       "      <td>-0.508647</td>\n",
       "      <td>-2.192450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.836945</td>\n",
       "      <td>-0.502010</td>\n",
       "      <td>-2.190900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.478500</td>\n",
       "      <td>-2.192570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877628</th>\n",
       "      <td>4.12733</td>\n",
       "      <td>-0.517609</td>\n",
       "      <td>-0.834530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.313579</td>\n",
       "      <td>-0.514054</td>\n",
       "      <td>-0.774689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-0.500250</td>\n",
       "      <td>-0.818123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877629</th>\n",
       "      <td>23.83930</td>\n",
       "      <td>0.883597</td>\n",
       "      <td>1.309400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.070823</td>\n",
       "      <td>0.876216</td>\n",
       "      <td>1.311607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>1.309000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877630</th>\n",
       "      <td>6.30444</td>\n",
       "      <td>0.539451</td>\n",
       "      <td>2.764210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.929778</td>\n",
       "      <td>0.508884</td>\n",
       "      <td>2.758275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.500250</td>\n",
       "      <td>2.759800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877631</th>\n",
       "      <td>34.38710</td>\n",
       "      <td>-0.417246</td>\n",
       "      <td>3.093800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.442506</td>\n",
       "      <td>-0.502704</td>\n",
       "      <td>3.086110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.511125</td>\n",
       "      <td>3.087050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877632</th>\n",
       "      <td>9.41212</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>2.661640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.530159</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>2.663181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.661630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>877633 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ptReco_true  etaReco_true  phiReco_true  chargeReco_true  ptReco_pred  \\\n",
       "0           7.14094      0.725395      1.863000              0.0     6.845138   \n",
       "1          22.94190     -0.825676     -2.726760              0.0    20.148404   \n",
       "2          77.79640      0.143982     -0.391065              0.0    81.846815   \n",
       "3           7.03724      0.803833      1.414660              0.0     7.492181   \n",
       "4          36.51210     -0.508647     -2.192450              1.0    33.836945   \n",
       "...             ...           ...           ...              ...          ...   \n",
       "877628      4.12733     -0.517609     -0.834530              1.0     5.313579   \n",
       "877629     23.83930      0.883597      1.309400              1.0    22.070823   \n",
       "877630      6.30444      0.539451      2.764210              0.0     5.929778   \n",
       "877631     34.38710     -0.417246      3.093800              0.0    16.442506   \n",
       "877632      9.41212     -0.005519      2.661640              1.0     9.530159   \n",
       "\n",
       "        etaReco_pred  phiReco_pred  chargeReco_pred  ptL1     etaL1     phiL1  \\\n",
       "0           0.742052      1.876867              0.0   9.0  0.750375  1.887140   \n",
       "1          -0.828740     -2.722502              0.0  27.0 -0.804750 -2.727080   \n",
       "2           0.145370     -0.400197              0.0  93.5  0.141375 -0.414516   \n",
       "3           0.817047      1.398779              0.0   9.5  0.804750  1.396260   \n",
       "4          -0.502010     -2.190900              1.0  41.0 -0.478500 -2.192570   \n",
       "...              ...           ...              ...   ...       ...       ...   \n",
       "877628     -0.514054     -0.774689              1.0   8.5 -0.500250 -0.818123   \n",
       "877629      0.876216      1.311607              1.0  29.0  0.880875  1.309000   \n",
       "877630      0.508884      2.758275              0.0   8.5  0.500250  2.759800   \n",
       "877631     -0.502704      3.086110              1.0  15.0 -0.511125  3.087050   \n",
       "877632     -0.004291      2.663181              1.0  10.0  0.000000  2.661630   \n",
       "\n",
       "        chargeL1  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "877628         1  \n",
       "877629         1  \n",
       "877630         0  \n",
       "877631         1  \n",
       "877632         1  \n",
       "\n",
       "[877633 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = test_data[stub_features].values\n",
    "test_targets  = test_data[target_features].values\n",
    "\n",
    "reg_predictions, class_predictions = model(test_features, training=False)\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            test_features,\n",
    "            test_targets,\n",
    "            reg_predictions.numpy(),\n",
    "            class_predictions.numpy(),\n",
    "            test_data[l1_features].values,\n",
    "        ),\n",
    "        axis=1\n",
    "    ),\n",
    "    columns=stub_features + target_features + [\"ptRecoInverse_pred\", \"etaExtRecoSt2_pred\", \"phiExtRecoSt2_pred\", \"chargeReco_pred\"] + l1_features\n",
    ")\n",
    "\n",
    "test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 1 / (1 + np.exp(-x)))\n",
    "test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "# rescale the features\n",
    "for key in normalizations.keys():\n",
    "    test_df[key] = test_df[key] * normalizations[key]\n",
    "    \n",
    "# rescale the predictions\n",
    "test_df[\"ptRecoInverse_pred\"] = test_df[\"ptRecoInverse_pred\"] * normalizations[\"ptRecoInverse\"]\n",
    "test_df[\"etaExtRecoSt2_pred\"] = test_df[\"etaExtRecoSt2_pred\"] * normalizations[\"etaExtRecoSt2\"]\n",
    "test_df[\"phiExtRecoSt2_pred\"] = test_df[\"phiExtRecoSt2_pred\"] * normalizations[\"phiExtRecoSt2\"]\n",
    "test_df[\"chargeReco_pred\"]    = test_df[\"chargeReco_pred\"]    * normalizations[\"chargeReco\"]\n",
    "\n",
    "# drop features keep only predictions and targets and L1 features\n",
    "test_df = test_df[[\"ptRecoInverse\", \"etaExtRecoSt2\", \"phiExtRecoSt2\", \"chargeReco\", \"ptRecoInverse_pred\", \"etaExtRecoSt2_pred\", \"phiExtRecoSt2_pred\", \"chargeReco_pred\"] + l1_features]\n",
    "\n",
    "# create ptReco_true and etaReco_true and phiReco_true and chargeReco_true\n",
    "# create ptReco_pred and etaReco_pred and phiReco_pred and chargeReco_pred\n",
    "test_df[\"ptReco_true\"]     = test_df[\"ptRecoInverse\"].apply(lambda x: 1 / x)\n",
    "test_df[\"etaReco_true\"]    = test_df[\"etaExtRecoSt2\"]\n",
    "test_df[\"phiReco_true\"]    = test_df[\"phiExtRecoSt2\"]\n",
    "test_df[\"chargeReco_true\"] = test_df[\"chargeReco\"]\n",
    "\n",
    "test_df[\"ptReco_pred\"]     = test_df[\"ptRecoInverse_pred\"].apply(lambda x: 1 / x)\n",
    "test_df[\"etaReco_pred\"]    = test_df[\"etaExtRecoSt2_pred\"]\n",
    "test_df[\"phiReco_pred\"]    = test_df[\"phiExtRecoSt2_pred\"]\n",
    "test_df[\"chargeReco_pred\"] = test_df[\"chargeReco_pred\"]\n",
    "\n",
    "# transform hwSignL1 into chargeL1\n",
    "test_df[\"chargeL1\"] = test_df[\"hwSignL1\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# drop ptRecoInverse and etaExtRecoSt2 and phiExtRecoSt2 and chargeReco\n",
    "# drop ptRecoInverse_pred and etaExtRecoSt2_pred and phiExtRecoSt2_pred and chargeReco_pred\n",
    "test_df = test_df[[\"ptReco_true\", \"etaReco_true\", \"phiReco_true\", \"chargeReco_true\", \"ptReco_pred\", \"etaReco_pred\", \"phiReco_pred\", \"chargeReco_pred\", \"ptL1\", \"etaL1\", \"phiL1\", \"chargeL1\"]]\n",
    "\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Test Set: 97.21%\n",
      "L1 Classification Accuracy on Test Set: 97.28%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (test_df[\"chargeReco_pred\"] == test_df[\"chargeReco_true\"]).sum() / len(test_df)\n",
    "\n",
    "print(f\"Classification Accuracy on Test Set: {accuracy*100:.2f}%\")\n",
    "\n",
    "l1_accuracy = (test_df[\"chargeL1\"] == test_df[\"chargeReco_true\"]).sum() / len(test_df)\n",
    "\n",
    "print(f\"L1 Classification Accuracy on Test Set: {l1_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/eos/user/f/fcufino'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m PLOT_FLAG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(PLOT_PATH):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPLOT_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/eos/user/f/fcufino'"
     ]
    }
   ],
   "source": [
    "prob      = True\n",
    "PLOT_PATH = f\"/eos/user/{USER[0]}/{USER}/nnreco-plots/\"\n",
    "PLOT_FLAG = False\n",
    "\n",
    "if not os.path.exists(PLOT_PATH):\n",
    "    os.makedirs(PLOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_res_pred = (test_df.etaReco_pred - test_df.etaReco_true) / test_df.etaReco_true\n",
    "\n",
    "# if eta_res_pred < -0.5 then add 1 to shift the histogram to the right\n",
    "# mask = np.where(eta_res_pred < -0.5, 1, 0)\n",
    "# eta_res_pred = eta_res_pred + mask\n",
    "\n",
    "# recompute the predicted eta\n",
    "# test_df[\"etaReco_pred\"] = test_df.etaReco_true * (1 + eta_res_pred)\n",
    "\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_l1   = (test_df.ptL1  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_rec_l1   = (test_df.ptL1/1.2  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "\n",
    "eta_res_pred = (test_df.etaReco_pred - test_df.etaReco_true) \n",
    "eta_res_l1   = (test_df.etaL1   - test_df.etaReco_true)\n",
    "\n",
    "phi_res_pred = (test_df.phiReco_pred - test_df.phiReco_true)\n",
    "phi_res_l1   = (test_df.phiL1   - test_df.phiReco_true)\n",
    "\n",
    "\n",
    "rlabel = \"ZeroBias+MuonEG 2023 (13.6 TeV)\"\n",
    "\n",
    "xmin = -1.4\n",
    "xmax = 1.4\n",
    "binw = 0.02\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(pt_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(pt_res_rec_l1, bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1 (recal)\")\n",
    "plt.hist(pt_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta p_T / p_T$\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/pt_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "xmin = -0.19\n",
    "xmax = 0.19\n",
    "binw = 0.0025\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(eta_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(eta_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta \\eta$\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/eta_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "xmin = -0.19\n",
    "xmax = 0.19\n",
    "binw = 0.0025\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(phi_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(phi_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta\\\\varphi$ (rad)\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/phi_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pT resolution vs pT perd true 2D histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 50\n",
    "xbinw = 0.5\n",
    "ymin = -1.5\n",
    "ymax = 1.5\n",
    "ybinw = 0.02\n",
    "\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "heights, binsx, binsy = np.histogram2d(test_df.ptReco_true, pt_res_pred, bins=(np.arange(xmin, xmax+xbinw, xbinw), np.arange(ymin, ymax+ybinw, ybinw)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,9), constrained_layout=True)\n",
    "hep.cms.label(label=\"Preliminary\", data=True, year=2023, rlabel=rlabel)\n",
    "\n",
    "hep.hist2dplot(heights, binsx, binsy, ax=ax, cmap=\"viridis\", flow=None, norm = mpl.colors.LogNorm())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(\"Events\", labelpad=15) # , fontsize=36)\n",
    "# cbar.ax.tick_params(labelsize=36)\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Reco muon $p_T$ (GeV)\")\n",
    "plt.ylabel(\"$\\Delta p_T / p_T$\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}pt_res_vs_pt_reco_true_zoom{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 50\n",
    "binw = 0.5\n",
    "\n",
    "# Compute pT resolution\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_l1   = (test_df.ptL1 / 1.2  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "\n",
    "# Define bins for pT reco true\n",
    "pt_true_bins = np.arange(xmin, xmax+binw, binw)  # From 0 to 50 GeV with bin width 1 GeV\n",
    "\n",
    "# Initialize arrays to store average pT resolution and standard deviation in each bin\n",
    "avg_pt_res_pred = np.zeros(len(pt_true_bins) - 1)\n",
    "std_pt_res_pred = np.zeros(len(pt_true_bins) - 1)\n",
    "avg_pt_res_l1   = np.zeros(len(pt_true_bins) - 1)\n",
    "std_pt_res_l1   = np.zeros(len(pt_true_bins) - 1)\n",
    "\n",
    "\n",
    "# Loop through the bins to calculate average pT resolution and standard deviation\n",
    "for i in range(len(pt_true_bins) - 1):\n",
    "    lower_bound = pt_true_bins[i]\n",
    "    upper_bound = pt_true_bins[i + 1]\n",
    "    \n",
    "    # Mask data points that fall within the current bin\n",
    "    mask = (test_df.ptReco_true >= lower_bound) & (test_df.ptReco_true < upper_bound)\n",
    "    \n",
    "    # Compute the average pT resolution for the current bin\n",
    "    avg_pt_res_pred[i] = np.mean(pt_res_pred[mask])\n",
    "    avg_pt_res_l1[i]   = np.mean(pt_res_l1[mask])\n",
    "    \n",
    "    # Compute the standard deviation for the current bin\n",
    "    std_pt_res_pred[i] = np.std(pt_res_pred[mask]) # / np.sqrt(len(pt_res_pred[mask]))\n",
    "    std_pt_res_l1[i]   = np.std(pt_res_l1[mask])   # / np.sqrt(len(pt_res_l1[mask]))\n",
    "\n",
    "\n",
    "# Create the profile plot\n",
    "fig, ax = plt.subplots(figsize=(12,9), constrained_layout=True)\n",
    "hep.cms.label(ax=ax, data=True, year=2023, label=\"Preliminary\", rlabel=rlabel)\n",
    "plt.plot(pt_true_bins[:-1], avg_pt_res_l1, \"-\", label=\"L1\", color=\"C0\")\n",
    "plt.fill_between(pt_true_bins[:-1], avg_pt_res_l1 - std_pt_res_l1, avg_pt_res_l1 + std_pt_res_l1, alpha=0.3, color=\"C0\")\n",
    "\n",
    "plt.plot(pt_true_bins[:-1], avg_pt_res_pred, \"-\", label=\"NN\", color=\"C1\")\n",
    "plt.fill_between(pt_true_bins[:-1], avg_pt_res_pred - std_pt_res_pred, avg_pt_res_pred + std_pt_res_pred, alpha=0.3, color=\"C1\")\n",
    "\n",
    "ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=2, zorder=0)\n",
    "\n",
    "# Plotting the average pT resolution with error bars\n",
    "\n",
    "# ax.errorbar(\n",
    "#     x=pt_true_bins[:-1], \n",
    "#     y=avg_pt_res_l1, \n",
    "#     yerr=std_pt_res_l1, \n",
    "#     marker='o', \n",
    "#     linestyle=\"\",\n",
    "#     markersize=12,\n",
    "#     elinewidth=3,\n",
    "#     capthick=3,\n",
    "#     capsize=5,\n",
    "#     label=\"L1\",\n",
    "#     color=PALETTE[-1],\n",
    "# )\n",
    "\n",
    "\n",
    "# ax.errorbar(\n",
    "#     x=pt_true_bins[:-1], \n",
    "#     y=avg_pt_res_pred, \n",
    "#     yerr=std_pt_res_pred, \n",
    "#     marker='o', \n",
    "#     linestyle=\"\",\n",
    "#     markersize=12,\n",
    "#     elinewidth=3,\n",
    "#     capthick=3,\n",
    "#     capsize=5,\n",
    "#     label=\"NN Prediction\",\n",
    "#     color=PALETTE[0],\n",
    "# )\n",
    "\n",
    "\n",
    "# ax.legend(fontsize=36, loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Reco muon $p_T$ [GeV]\")\n",
    "plt.ylabel(\"$\\Delta p_T / p_T$\")\n",
    "\n",
    "# ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}pt_res_vs_pt_reco_true_profile{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.xlim(5,xmax)\n",
    "plt.ylim(-2,2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_regression_losses, label=\"Train\")\n",
    "ax.plot(val_regression_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Regression Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_classification_losses, label=\"Train\")\n",
    "ax.plot(val_classification_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_combined_losses, label=\"Train\")\n",
    "ax.plot(val_combined_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Combined Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(learning_rates, lw=3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}learning_rate{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(learning_rates, lw=3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}log_learning_rate{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform multiple trainings to compute average loss (TAKES A LOT OF TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=True)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return regression_loss, classification_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(features, targets):\n",
    "    regression_targets = targets[:, :-1]\n",
    "    classification_targets = targets[:, -1]\n",
    "    reg_outputs, class_outputs = model(features, training=False)\n",
    "    regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "    classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "    loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "    return regression_loss, classification_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_ = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(FILE_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_data_ = pd.concat([full_data_, pd.read_csv(FILE_PATH + file)], ignore_index=True)\n",
    "        \n",
    "full_data_ = full_data_.iloc[:, :-1]\n",
    "\n",
    "# drop rows with ptL1 == 4.5\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] != 4.5]\n",
    "\n",
    "# drop rows with reco eta > 1 and < -1\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] < 1]\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] > -1]\n",
    "\n",
    "# drop rows with ptL1 > 50\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] < 50]\n",
    "\n",
    "# drop rows with ptReco > 50\n",
    "full_data_ = full_data_[full_data_[\"ptReco\"] < 50]\n",
    "\n",
    "\n",
    "# mask_1 = (full_data_.etaL1 == 0) & (np.abs(full_data_.etaExtRecoSt2) == 0)\n",
    "# mask_2 = (full_data_.etaL1 != 0)\n",
    "\n",
    "# mask = mask_1 | mask_2\n",
    "\n",
    "# full_data_ = full_data_[mask]\n",
    "\n",
    "full_data_[\"ptRecoInverse\"] = 1 / full_data_[\"ptReco\"]\n",
    "\n",
    "full_data = full_data_[stub_features + target_features + l1_features]\n",
    "\n",
    "full_data[\"chargeReco\"] = full_data[\"chargeReco\"].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_criterion = losses.BinaryCrossentropy(from_logits=True)\n",
    "regression_criterion     = losses.MeanAbsoluteError()\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor = 0.5\n",
    "patience = 5\n",
    "min_loss_improvement = 0.1\n",
    "\n",
    "classification_weight = 1\n",
    "regression_weight     = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regression_losses_all     = []\n",
    "train_classification_losses_all = []\n",
    "train_combined_losses_all       = []\n",
    "val_regression_losses_all       = []\n",
    "val_classification_losses_all   = []\n",
    "val_combined_losses_all         = []\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    input_size    = len(stub_features)\n",
    "    architecture  = [input_size, 64, 32, 16, 8]\n",
    "    output_size   = len(target_features)\n",
    "    learning_rate = 1e-2\n",
    "    num_epochs    = 300\n",
    "    batch_size    = 2**10\n",
    "    reg_strength  = 1e-3\n",
    "\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    patience = 10  # Number of epochs to wait before stopping\n",
    "    early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "    train_val_data, test_data = train_test_split(full_data,      test_size=0.3, random_state=42)\n",
    "    train_data, val_data      = train_test_split(train_val_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    # normalize the data\n",
    "    for key in normalizations.keys():\n",
    "        train_data[key] = train_data[key] / normalizations[key]\n",
    "        val_data[key]   = val_data[key]   / normalizations[key]\n",
    "        test_data[key]  = test_data[key]  / normalizations[key]\n",
    "        \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data[stub_features].values, train_data[target_features].values)).batch(batch_size).shuffle(buffer_size=len(train_data))\n",
    "    val_dataset   = tf.data.Dataset.from_tensor_slices((val_data[stub_features].values, val_data[target_features].values)).batch(batch_size)\n",
    "    test_dataset  = tf.data.Dataset.from_tensor_slices((test_data[stub_features].values, test_data[target_features].values)).batch(batch_size)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = MultiTaskNN(architecture, reg_strength=reg_strength)\n",
    "\n",
    "    # Build the model with the batch input shape\n",
    "    bs = None  # None allows for variable batch size\n",
    "    model.build((bs, input_size))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    scheduler = CustomLRScheduler(\n",
    "        optimizer, \n",
    "        factor=scale_factor, \n",
    "        patience=patience, \n",
    "        min_improvement=min_loss_improvement, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "    train_regression_losses     = []\n",
    "    train_classification_losses = []\n",
    "    train_combined_losses       = []\n",
    "    val_regression_losses       = []\n",
    "    val_classification_losses   = []\n",
    "    val_combined_losses         = []\n",
    "\n",
    "    learning_rates = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(features, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            regression_targets = targets[:, :-1]\n",
    "            classification_targets = targets[:, -1]\n",
    "            reg_outputs, class_outputs = model(features, training=True)\n",
    "            regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "            classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "            loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        return regression_loss, classification_loss, loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(features, targets):\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=False)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "        return regression_loss, classification_loss, loss\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        running_loss                = 0.0\n",
    "        running_regression_loss     = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        \n",
    "        \n",
    "        for features, targets in train_dataset:\n",
    "            \n",
    "            regression_loss, classification_loss, loss = train_step(features, targets)\n",
    "            \n",
    "            running_loss                += loss.numpy()\n",
    "            running_regression_loss     += regression_loss.numpy()\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "\n",
    "\n",
    "            \n",
    "        # Note: Remember to adjust if not using batches of equal sizes\n",
    "        train_loss                = running_loss                / len(train_dataset)\n",
    "        train_regression_loss     = running_regression_loss     / len(train_dataset)\n",
    "        train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "\n",
    "        train_regression_losses.append(train_regression_loss)\n",
    "        train_classification_losses.append(train_classification_loss)\n",
    "        train_combined_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        running_loss                = 0.0\n",
    "        running_regression_loss     = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        \n",
    "        for features, targets in val_dataset:\n",
    "            \n",
    "            regression_loss, classification_loss, loss = val_step(features, targets)\n",
    "            \n",
    "            running_loss                += loss.numpy()\n",
    "            running_regression_loss     += regression_loss.numpy()\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "        \n",
    "        avg_val_loss                = running_loss                / len(val_dataset)\n",
    "        avg_val_regression_loss     = running_regression_loss     / len(val_dataset)\n",
    "        avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "            \n",
    "        val_regression_losses.append(avg_val_regression_loss)\n",
    "        val_classification_losses.append(avg_val_classification_loss)\n",
    "        val_combined_losses.append(avg_val_loss)\n",
    "        \n",
    "        \n",
    "        current_lr = optimizer.lr.numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        print(f\"Train Losses - Regression: {train_regression_losses[-1]:.4f}, Classification: {train_classification_losses[-1]:.4f}, Combined: {train_combined_losses[-1]:.4f}\")\n",
    "        print(f\"Validation Losses - Regression: {val_regression_losses[-1]:.4f}, Classification: {val_classification_losses[-1]:.4f}, Combined: {val_combined_losses[-1]:.4f}\")\n",
    "        print(\"-------------\")\n",
    "        \n",
    "        \n",
    "        scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_loss})\n",
    "\n",
    "        \n",
    "\n",
    "        # Check for early stopping based on the new criterion\n",
    "        if avg_val_loss < (1 - early_stopping_threshold) * best_val_loss:  # 0.001 corresponds to 0.1%\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = min(best_val_loss, avg_val_loss)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "            break\n",
    "        \n",
    "    train_regression_losses_all.append(train_regression_losses)\n",
    "    train_classification_losses_all.append(train_classification_losses)\n",
    "    train_combined_losses_all.append(train_combined_losses)\n",
    "    val_regression_losses_all.append(val_regression_losses)\n",
    "    val_classification_losses_all.append(val_classification_losses)\n",
    "    val_combined_losses_all.append(val_combined_losses)\n",
    "    \n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    del train_data\n",
    "    del val_data\n",
    "    del test_data\n",
    "    del train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and std of the losses for each epoch\n",
    "# the lists in the lists are of different lengths, so we need to pad them with the last value\n",
    "# then we can compute the mean and std\n",
    "\n",
    "train_regression_losses_all      = [x + [x[-1]] * (max(map(len, train_regression_losses_all)) - len(x)) for x in train_regression_losses_all]\n",
    "train_classification_losses_all  = [x + [x[-1]] * (max(map(len, train_classification_losses_all)) - len(x)) for x in train_classification_losses_all]\n",
    "train_combined_losses_all        = [x + [x[-1]] * (max(map(len, train_combined_losses_all)) - len(x)) for x in train_combined_losses_all]\n",
    "\n",
    "val_regression_losses_all        = [x + [x[-1]] * (max(map(len, val_regression_losses_all)) - len(x)) for x in val_regression_losses_all]\n",
    "val_classification_losses_all    = [x + [x[-1]] * (max(map(len, val_classification_losses_all)) - len(x)) for x in val_classification_losses_all]\n",
    "val_combined_losses_all          = [x + [x[-1]] * (max(map(len, val_combined_losses_all)) - len(x)) for x in val_combined_losses_all]\n",
    "\n",
    "train_regression_losses_mean     = np.mean(np.array(train_regression_losses_all), axis=0)[1:]\n",
    "train_regression_losses_std      = np.std(np.array(train_regression_losses_all), axis=0)[1:]\n",
    "train_classification_losses_mean = np.mean(np.array(train_classification_losses_all), axis=0)[1:]\n",
    "train_classification_losses_std  = np.std(np.array(train_classification_losses_all), axis=0)[1:]\n",
    "train_combined_losses_mean       = np.mean(np.array(train_combined_losses_all), axis=0)[1:]\n",
    "train_combined_losses_std        = np.std(np.array(train_combined_losses_all), axis=0)[1:]\n",
    "\n",
    "val_regression_losses_mean       = np.mean(np.array(val_regression_losses_all), axis=0)[1:]\n",
    "val_regression_losses_std        = np.std(np.array(val_regression_losses_all), axis=0)[1:]\n",
    "val_classification_losses_mean   = np.mean(np.array(val_classification_losses_all), axis=0)[1:]\n",
    "val_classification_losses_std    = np.std(np.array(val_classification_losses_all), axis=0)[1:]\n",
    "val_combined_losses_mean         = np.mean(np.array(val_combined_losses_all), axis=0)[1:]\n",
    "val_combined_losses_std          = np.std(np.array(val_combined_losses_all), axis=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# REGRESSION LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_regression_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_regression_losses_mean)), train_regression_losses_mean - train_regression_losses_std, train_regression_losses_mean + train_regression_losses_std, alpha=0.3)\n",
    "ax.plot(val_regression_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_regression_losses_mean)), val_regression_losses_mean - val_regression_losses_std, val_regression_losses_mean + val_regression_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Regression Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}regression_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# CLASSIFICATION LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_classification_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_classification_losses_mean)), train_classification_losses_mean - train_classification_losses_std, train_classification_losses_mean + train_classification_losses_std, alpha=0.3)\n",
    "ax.plot(val_classification_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_classification_losses_mean)), val_classification_losses_mean - val_classification_losses_std, val_classification_losses_mean + val_classification_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}classification_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# COMBINED LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_combined_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_combined_losses_mean)), train_combined_losses_mean - train_combined_losses_std, train_combined_losses_mean + train_combined_losses_std, alpha=0.3)\n",
    "ax.plot(val_combined_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_combined_losses_mean)), val_combined_losses_mean - val_combined_losses_std, val_combined_losses_mean + val_combined_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Combined Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}combined_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
