{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMTF input stubs fit with a Deep Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mplhep as hep\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hep.style.use(\"CMS\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# mpl.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training device: {'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.getenv(\"USER\")\n",
    "\n",
    "# FILE_PATH = \"/eos/cms/store/cmst3/group/daql1scout/ml_data/run3/bmtf_stubs_refit/\"\n",
    "FILE_PATH = \"/mnt/ml_data/run3/bmtf_stubs_refit/\"\n",
    "\n",
    "FILE_NAME = \"rereco\"\n",
    "\n",
    "OUT_PATH = \".\"\n",
    "LOSS_FNAME = \"losses.csv\"\n",
    "\n",
    "# FIGSIZE = (12, 9)\n",
    "\n",
    "petroff_10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler('color', petroff_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model for regression+classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNN(tf.keras.Model):\n",
    "    def __init__(self, architecture, reg_strength=0.01):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "\n",
    "        # Check if the architecture list has at least 2 values (input size and one hidden layer)\n",
    "        if len(architecture) < 2:\n",
    "            raise ValueError(\"Architecture must contain at least input size and one hidden layer.\")\n",
    "\n",
    "        self.layers_list = []\n",
    "\n",
    "        # Iterate over the architecture list to dynamically create dense layers followed by batch normalization\n",
    "        for i in range(1, len(architecture)):\n",
    "            self.layers_list.append(layers.Dense(architecture[i], kernel_regularizer=regularizers.l2(reg_strength)))\n",
    "            self.layers_list.append(layers.Activation('elu'))\n",
    "\n",
    "            # TODO: It looks like there might be a typo in the comment.\n",
    "            #  The code does not currently include a batch normalization layer after each dense layer.\n",
    "            #  It only has dense layers followed by activation layers.\n",
    "\n",
    "        # Separate heads for regression and classification tasks\n",
    "        self.regression_head = layers.Dense(3, kernel_regularizer=regularizers.l2(reg_strength))  # for pt, eta, phi\n",
    "        self.classification_head = layers.Dense(1, kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "        # self.classification_head = layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "\n",
    "    #forward pass of the neural network.\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        # Where all the multiplications are done\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "\n",
    "        reg_output = self.regression_head(x)\n",
    "        class_output = self.classification_head(x)\n",
    "        return reg_output, class_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback is designed to adjust the learning rate of the optimizer during training based on the validation loss.\\\n",
    "It adjusts the learning rate based on the validation loss, reducing it if the loss does not improve sufficiently over a number of epochs (patience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate scheduler callback\n",
    "class CustomLRScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        optimizer, \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_improvement=0.01, \n",
    "        verbose=True\n",
    "    ):\n",
    "        super(CustomLRScheduler, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_improvement = min_improvement\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            self.optimizer.lr.assign(self.min_lr)\n",
    "        if epoch < self.decrease_epoch:\n",
    "            self.increase_flag = True\n",
    "        else:\n",
    "            self.increase_flag = False\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if loss:\n",
    "            relative_improvement = (self.best_loss - loss) / self.best_loss\n",
    "\n",
    "            if relative_improvement < self.min_improvement:\n",
    "                self.patience_counter += 1\n",
    "            else:\n",
    "                self.patience_counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "            if self.patience_counter >= self.patience:\n",
    "                self._decrease_lr()\n",
    "\n",
    "    def _decrease_lr(self):\n",
    "        old_lr = self.optimizer.lr.numpy()\n",
    "        new_lr = old_lr * self.factor\n",
    "        self.optimizer.lr.assign(new_lr)\n",
    "        if self.verbose:\n",
    "            print(f\"Decreasing learning rate to {new_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stubs_norm         = 2**2\n",
    "station_norm         = 2**2\n",
    "sector_norm          = 2**3\n",
    "wheel_norm           = 2**1\n",
    "eta_norm             = 2**8\n",
    "qeta_norm            = 2**1\n",
    "tag_norm             = 2**0\n",
    "phi_norm             = 2**11\n",
    "phib_norm            = 2**9\n",
    "quality_norm         = 2**3\n",
    "reco_pt_norm         = 2**0\n",
    "reco_pt_inverse_norm = 2**0\n",
    "reco_eta_norm        = 2**2\n",
    "reco_phi_norm        = 2**2\n",
    "reco_charge_norm     = 2**0\n",
    "\n",
    "normalizations = {\n",
    "    \"n_stubs\": n_stubs_norm,\n",
    "    \"s1_stNum\": station_norm,\n",
    "    \"s1_scNum\": sector_norm,\n",
    "    \"s1_whNum\": wheel_norm,\n",
    "    \"s1_eta_1\": eta_norm,\n",
    "    \"s1_qeta_1\": qeta_norm,\n",
    "    \"s1_eta_2\": eta_norm,\n",
    "    \"s1_qeta_2\": qeta_norm,\n",
    "    \"s1_tag\": tag_norm,\n",
    "    \"s1_phi\": phi_norm,\n",
    "    \"s1_phiB\": phib_norm,\n",
    "    \"s1_quality\": quality_norm,\n",
    "    \"s2_stNum\": station_norm,\n",
    "    \"s2_scNum\": sector_norm,\n",
    "    \"s2_whNum\": wheel_norm,\n",
    "    \"s2_eta_1\": eta_norm,\n",
    "    \"s2_qeta_1\": qeta_norm,\n",
    "    \"s2_eta_2\": eta_norm,\n",
    "    \"s2_qeta_2\": qeta_norm,\n",
    "    \"s2_tag\": tag_norm,\n",
    "    \"s2_phi\": phi_norm,\n",
    "    \"s2_phiB\": phib_norm,\n",
    "    \"s2_quality\": quality_norm,\n",
    "    \"s3_stNum\": station_norm,\n",
    "    \"s3_scNum\": sector_norm,\n",
    "    \"s3_whNum\": wheel_norm,\n",
    "    \"s3_eta_1\": eta_norm,\n",
    "    \"s3_qeta_1\": qeta_norm,\n",
    "    \"s3_eta_2\": eta_norm,\n",
    "    \"s3_qeta_2\": qeta_norm,\n",
    "    \"s3_tag\": tag_norm,\n",
    "    \"s3_phi\": phi_norm,\n",
    "    \"s3_phiB\": phib_norm,\n",
    "    \"s3_quality\": quality_norm,\n",
    "    \"s4_stNum\": station_norm,\n",
    "    \"s4_scNum\": sector_norm,\n",
    "    \"s4_whNum\": wheel_norm,\n",
    "    \"s4_eta_1\": eta_norm,\n",
    "    \"s4_qeta_1\": qeta_norm,\n",
    "    \"s4_eta_2\": eta_norm,\n",
    "    \"s4_qeta_2\": qeta_norm,\n",
    "    \"s4_tag\": tag_norm,\n",
    "    \"s4_phi\": phi_norm,\n",
    "    \"s4_phiB\": phib_norm,\n",
    "    \"s4_quality\": quality_norm,\n",
    "    # \"ptReco\": reco_pt_norm,\n",
    "    \"ptRecoInverse\": reco_pt_inverse_norm,\n",
    "    \"etaExtRecoSt2\": reco_eta_norm,\n",
    "    \"phiExtRecoSt2\": reco_phi_norm,\n",
    "    \"chargeReco\": reco_charge_norm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features \n",
    "\n",
    "| Feature                  | Description                                                                                  |\n",
    "|--------------------------|----------------------------------------------------------------------------------------------|\n",
    "| **n_stubs**              | The total number of stubs detected for a muon.                                               |\n",
    "| **s1_stNum, s2_stNum, s3_stNum, s4_stNum** | The specific muon station where the stub was detected.                            |\n",
    "| **s1_scNum, s2_scNum, s3_scNum, s4_scNum** | The sector within the muon station where the stub was detected.                       |\n",
    "| **s1_whNum, s2_whNum, s3_whNum, s4_whNum** | The wheel of the muon detector where the stub was detected.                          |\n",
    "| **s1_eta_1, s2_eta_1, s3_eta_1, s4_eta_1, s1_eta_2, s2_eta_2, s3_eta_2, s4_eta_2** | The pseudorapidity of the stub.                                                       |\n",
    "| **s1_qeta_1, s2_qeta_1, s3_qeta_1, s4_qeta_1, s1_qeta_2, s2_qeta_2, s3_qeta_2, s4_qeta_2** |----- ----                                            |\n",
    "| **s1_tag, s2_tag, s3_tag, s4_tag** | A tag indicating specific characteristics of the stub.                                  |\n",
    "| **s1_phi, s2_phi, s3_phi, s4_phi** | The azimuthal angle of the stub.                                                          |\n",
    "| **s1_phiB, s2_phiB, s3_phiB, s4_phiB** | The bending angle of the stub in the phi plane.                                           |\n",
    "| **s1_quality, s2_quality, s3_quality, s4_quality** | A quality metric for the stub.                                                           |\n",
    "| **ptReco, ptRecoInverse** | The transverse momentum of the muon, reconstructed from the stubs.                              |\n",
    "| **etaExtRecoSt2** | The reconstructed pseudorapidity of the muon from the stubs.                                   |\n",
    "| **phiExtRecoSt2** | The reconstructed azimuthal angle of the muon from the stubs.                                      |\n",
    "| **chargeReco** | The reconstructed charge of the muon from the stubs.                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_features = [\n",
    "    'n_stubs', \n",
    "    's1_stNum', 's1_scNum', 's1_whNum', 's1_eta_1', 's1_qeta_1', 's1_eta_2', 's1_qeta_2', 's1_tag', 's1_phi', 's1_phiB', 's1_quality', \n",
    "    's2_stNum', 's2_scNum', 's2_whNum', 's2_eta_1', 's2_qeta_1', 's2_eta_2', 's2_qeta_2', 's2_tag', 's2_phi', 's2_phiB', 's2_quality', \n",
    "    's3_stNum', 's3_scNum', 's3_whNum', 's3_eta_1', 's3_qeta_1', 's3_eta_2', 's3_qeta_2', 's3_tag', 's3_phi', 's3_phiB', 's3_quality', \n",
    "    's4_stNum', 's4_scNum', 's4_whNum', 's4_eta_1', 's4_qeta_1', 's4_eta_2', 's4_qeta_2', 's4_tag', 's4_phi', 's4_phiB', 's4_quality'\n",
    "]\n",
    "\n",
    "target_features = [\n",
    "    'ptRecoInverse', 'etaExtRecoSt2', 'phiExtRecoSt2', 'chargeReco',\n",
    "]\n",
    "\n",
    "l1_features = [\n",
    "    'ptL1', 'etaL1', 'phiL1', 'hwSignL1',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_ = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(FILE_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_data_ = pd.concat([full_data_, pd.read_csv(FILE_PATH + file)], ignore_index=True)\n",
    "        \n",
    "full_data_ = full_data_.iloc[:, :-1]\n",
    "\n",
    "# drop rows with ptL1 == 4.5\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] != 4.5]\n",
    "\n",
    "# drop rows with reco eta > 1 and < -1\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] < 1]\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] > -1]\n",
    "\n",
    "# drop rows with ptL1 > 50\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] < 256]\n",
    "\n",
    "# drop rows with ptReco > 50\n",
    "full_data_ = full_data_[full_data_[\"ptReco\"] < 256]\n",
    "\n",
    "\n",
    "# mask_1 = (full_data_.etaL1 == 0) & (np.abs(full_data_.etaExtRecoSt2) == 0)\n",
    "# mask_2 = (full_data_.etaL1 != 0)\n",
    "\n",
    "# mask = mask_1 | mask_2\n",
    "\n",
    "# full_data_ = full_data_[mask]\n",
    "\n",
    "full_data_[\"ptRecoInverse\"] = 1 / full_data_[\"ptReco\"]\n",
    "\n",
    "full_data = full_data_[stub_features + target_features + l1_features]\n",
    "\n",
    "full_data[\"chargeReco\"] = full_data[\"chargeReco\"].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_stubs</th>\n",
       "      <th>s1_stNum</th>\n",
       "      <th>s1_scNum</th>\n",
       "      <th>s1_whNum</th>\n",
       "      <th>s1_eta_1</th>\n",
       "      <th>s1_qeta_1</th>\n",
       "      <th>s1_eta_2</th>\n",
       "      <th>s1_qeta_2</th>\n",
       "      <th>s1_tag</th>\n",
       "      <th>s1_phi</th>\n",
       "      <th>...</th>\n",
       "      <th>s4_phiB</th>\n",
       "      <th>s4_quality</th>\n",
       "      <th>ptRecoInverse</th>\n",
       "      <th>etaExtRecoSt2</th>\n",
       "      <th>phiExtRecoSt2</th>\n",
       "      <th>chargeReco</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>hwSignL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-52</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041279</td>\n",
       "      <td>-0.555335</td>\n",
       "      <td>-1.409220</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.554625</td>\n",
       "      <td>-1.418080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.608174</td>\n",
       "      <td>2.628690</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>2.628900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-14</td>\n",
       "      <td>2</td>\n",
       "      <td>-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>722</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>-0.185071</td>\n",
       "      <td>1.230710</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.184875</td>\n",
       "      <td>1.232640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-46</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>-0.485899</td>\n",
       "      <td>-1.515270</td>\n",
       "      <td>0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>-0.467625</td>\n",
       "      <td>-1.527160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1202</td>\n",
       "      <td>...</td>\n",
       "      <td>-18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.037988</td>\n",
       "      <td>0.738465</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>0.818123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426409</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1116</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>-2.863660</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>-2.868880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426410</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>908</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.790361</td>\n",
       "      <td>-1.350280</td>\n",
       "      <td>1</td>\n",
       "      <td>255.5</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>-1.352630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426411</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-232</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.052273</td>\n",
       "      <td>-0.436209</td>\n",
       "      <td>-2.690840</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.424125</td>\n",
       "      <td>-2.694350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426412</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-232</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.075172</td>\n",
       "      <td>-0.440287</td>\n",
       "      <td>-2.716570</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.424125</td>\n",
       "      <td>-2.694350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426413</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>646</td>\n",
       "      <td>...</td>\n",
       "      <td>-8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.132354</td>\n",
       "      <td>0.306403</td>\n",
       "      <td>2.773520</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.315375</td>\n",
       "      <td>2.770710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2925441 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_stubs  s1_stNum  s1_scNum  s1_whNum  s1_eta_1  s1_qeta_1  s1_eta_2  \\\n",
       "0              4         1         9        -1       -52          2       255   \n",
       "2              3         1         5         1         7          0         7   \n",
       "3              2         1         2         0       -14          2       -21   \n",
       "4              4         1         9        -1       -46          2       255   \n",
       "5              3         1         1         1         7          0         7   \n",
       "...          ...       ...       ...       ...       ...        ...       ...   \n",
       "3426409        3         1         6         1         7          0         7   \n",
       "3426410        2         3         9         2        74          2       255   \n",
       "3426411        4         1         7        -1       -40          2       255   \n",
       "3426412        4         1         7        -1       -40          2       255   \n",
       "3426413        3         2         5         1         7          0         7   \n",
       "\n",
       "         s1_qeta_2  s1_tag  s1_phi  ...  s4_phiB  s4_quality  ptRecoInverse  \\\n",
       "0                0       1     632  ...        7           5       0.041279   \n",
       "2                0       1      10  ...       12           2       0.052357   \n",
       "3                1       1     722  ...       18           5       0.042819   \n",
       "4                0       1     140  ...        9           6       0.058421   \n",
       "5                0       1    1202  ...      -18           3       0.037988   \n",
       "...            ...     ...     ...  ...      ...         ...            ...   \n",
       "3426409          0       1    1116  ...        6           5       0.027158   \n",
       "3426410          0       1     908  ...       -9           2       0.018035   \n",
       "3426411          0       1    -232  ...       -9           6       0.052273   \n",
       "3426412          0       1    -232  ...       -9           6       0.075172   \n",
       "3426413          0       1     646  ...       -8           6       0.132354   \n",
       "\n",
       "         etaExtRecoSt2  phiExtRecoSt2  chargeReco   ptL1     etaL1     phiL1  \\\n",
       "0            -0.555335      -1.409220           0   29.0 -0.554625 -1.418080   \n",
       "2             0.608174       2.628690           0   17.5  0.598125  2.628900   \n",
       "3            -0.185071       1.230710           0   30.0 -0.184875  1.232640   \n",
       "4            -0.485899      -1.515270           0   20.5 -0.467625 -1.527160   \n",
       "5             0.738465       0.816011           0   31.0  0.750375  0.818123   \n",
       "...                ...            ...         ...    ...       ...       ...   \n",
       "3426409       0.750462      -2.863660           0   59.0  0.739500 -2.868880   \n",
       "3426410       0.790361      -1.350280           1  255.5  0.804750 -1.352630   \n",
       "3426411      -0.436209      -2.690840           1   22.0 -0.424125 -2.694350   \n",
       "3426412      -0.440287      -2.716570           1   22.0 -0.424125 -2.694350   \n",
       "3426413       0.306403       2.773520           0    8.5  0.315375  2.770710   \n",
       "\n",
       "         hwSignL1  \n",
       "0               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "5               1  \n",
       "...           ...  \n",
       "3426409         1  \n",
       "3426410         0  \n",
       "3426411         0  \n",
       "3426412         0  \n",
       "3426413         1  \n",
       "\n",
       "[2925441 rows x 53 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(full_data,      test_size=0.3, random_state=42)\n",
    "train_data, val_data      = train_test_split(train_val_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 1843027\n",
      "Val dataset: 204781\n",
      "Test dataset: 877633\n"
     ]
    }
   ],
   "source": [
    "# inspect the datasets\n",
    "print(f\"Train dataset: {len(train_data)}\")\n",
    "print(f\"Val dataset: {len(val_data)}\")\n",
    "print(f\"Test dataset: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "for key in normalizations.keys():\n",
    "    train_data[key] = train_data[key] / normalizations[key]\n",
    "    val_data[key]   = val_data[key]   / normalizations[key]\n",
    "    test_data[key]  = test_data[key]  / normalizations[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 09:31:03.911211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10518 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2024-07-05 09:31:03.912690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10532 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size    = len(stub_features)\n",
    "architecture  = [input_size, 64, 32, 16, 8]\n",
    "output_size   = len(target_features)\n",
    "learning_rate = 1e-2\n",
    "num_epochs    = 300\n",
    "batch_size    = 2**8\n",
    "reg_strength  = 1e-3\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor = 0.5\n",
    "patience = 5\n",
    "min_loss_improvement = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "patience = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "classification_weight = 1\n",
    "regression_weight     = 6\n",
    "\n",
    "# Loss and optimizer\n",
    "# regression_criterion     = losses.MeanSquaredError()\n",
    "classification_criterion = losses.BinaryCrossentropy(from_logits=True)\n",
    "regression_criterion     = losses.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "scheduler = CustomLRScheduler(\n",
    "    optimizer, \n",
    "    factor=scale_factor, \n",
    "    patience=patience, \n",
    "    min_improvement=min_loss_improvement, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data[stub_features].values, train_data[target_features].values)).batch(batch_size).shuffle(buffer_size=len(train_data))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((val_data[stub_features].values, val_data[target_features].values)).batch(batch_size)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((test_data[stub_features].values, test_data[target_features].values)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5724\n",
      "Number of trainable parameters: 5724\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = MultiTaskNN(architecture, reg_strength=reg_strength)\n",
    "\n",
    "# Build the model with the batch input shape\n",
    "bs = None  # None allows for variable batch size\n",
    "model.build((bs, input_size))\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params = model.count_params()\n",
    "trainable_vars = [var for var in model.trainable_variables]\n",
    "trainable_params = sum([tf.size(var).numpy() for var in trainable_vars])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=True)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return regression_loss, classification_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(features, targets):\n",
    "    regression_targets = targets[:, :-1]\n",
    "    classification_targets = targets[:, -1]\n",
    "    reg_outputs, class_outputs = model(features, training=False)\n",
    "    regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "    classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "    loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "    return regression_loss, classification_loss, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 09:31:14.535733: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f76b92a9230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-05 09:31:14.535788: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-07-05 09:31:14.535803: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-07-05 09:31:14.544564: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-05 09:31:14.574311: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720164674.683939 3231168 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0276, Classification: 0.1422, Combined: 0.3078\n",
      "Validation Losses - Regression: 0.0207, Classification: 0.1306, Combined: 0.2546\n",
      "-------------\n",
      "Epoch [2/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0172, Classification: 0.1269, Combined: 0.2300\n",
      "Validation Losses - Regression: 0.0154, Classification: 0.1230, Combined: 0.2153\n",
      "-------------\n",
      "Epoch [3/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0162, Classification: 0.1233, Combined: 0.2205\n",
      "Validation Losses - Regression: 0.0169, Classification: 0.1237, Combined: 0.2251\n",
      "-------------\n",
      "Epoch [4/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0156, Classification: 0.1219, Combined: 0.2155\n",
      "Validation Losses - Regression: 0.0164, Classification: 0.1214, Combined: 0.2198\n",
      "-------------\n",
      "Epoch [5/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0154, Classification: 0.1213, Combined: 0.2138\n",
      "Validation Losses - Regression: 0.0144, Classification: 0.1267, Combined: 0.2130\n",
      "-------------\n",
      "Epoch [6/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0154, Classification: 0.1204, Combined: 0.2127\n",
      "Validation Losses - Regression: 0.0146, Classification: 0.1189, Combined: 0.2064\n",
      "-------------\n",
      "Epoch [7/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0152, Classification: 0.1197, Combined: 0.2107\n",
      "Validation Losses - Regression: 0.0139, Classification: 0.1207, Combined: 0.2039\n",
      "-------------\n",
      "Epoch [8/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0150, Classification: 0.1194, Combined: 0.2093\n",
      "Validation Losses - Regression: 0.0140, Classification: 0.1203, Combined: 0.2045\n",
      "-------------\n",
      "Epoch [9/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0151, Classification: 0.1193, Combined: 0.2102\n",
      "Validation Losses - Regression: 0.0143, Classification: 0.1214, Combined: 0.2074\n",
      "-------------\n",
      "Epoch [10/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0151, Classification: 0.1187, Combined: 0.2094\n",
      "Validation Losses - Regression: 0.0158, Classification: 0.1152, Combined: 0.2100\n",
      "-------------\n",
      "Epoch [11/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0150, Classification: 0.1178, Combined: 0.2076\n",
      "Validation Losses - Regression: 0.0151, Classification: 0.1255, Combined: 0.2158\n",
      "-------------\n",
      "Epoch [12/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0149, Classification: 0.1173, Combined: 0.2066\n",
      "Validation Losses - Regression: 0.0140, Classification: 0.1154, Combined: 0.1996\n",
      "-------------\n",
      "Decreasing learning rate to 0.004999999888241291\n",
      "Epoch [13/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1117, Combined: 0.1869\n",
      "Validation Losses - Regression: 0.0122, Classification: 0.1147, Combined: 0.1878\n",
      "-------------\n",
      "Epoch [14/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1109, Combined: 0.1858\n",
      "Validation Losses - Regression: 0.0123, Classification: 0.1119, Combined: 0.1856\n",
      "-------------\n",
      "Epoch [15/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1103, Combined: 0.1853\n",
      "Validation Losses - Regression: 0.0126, Classification: 0.1120, Combined: 0.1876\n",
      "-------------\n",
      "Epoch [16/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1100, Combined: 0.1848\n",
      "Validation Losses - Regression: 0.0124, Classification: 0.1118, Combined: 0.1865\n",
      "-------------\n",
      "Epoch [17/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1097, Combined: 0.1845\n",
      "Validation Losses - Regression: 0.0121, Classification: 0.1095, Combined: 0.1821\n",
      "-------------\n",
      "Epoch [18/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1093, Combined: 0.1839\n",
      "Validation Losses - Regression: 0.0129, Classification: 0.1102, Combined: 0.1874\n",
      "-------------\n",
      "Epoch [19/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1092, Combined: 0.1838\n",
      "Validation Losses - Regression: 0.0131, Classification: 0.1107, Combined: 0.1892\n",
      "-------------\n",
      "Epoch [20/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0125, Classification: 0.1095, Combined: 0.1846\n",
      "Validation Losses - Regression: 0.0124, Classification: 0.1125, Combined: 0.1871\n",
      "-------------\n",
      "Epoch [21/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1090, Combined: 0.1836\n",
      "Validation Losses - Regression: 0.0129, Classification: 0.1141, Combined: 0.1916\n",
      "-------------\n",
      "Epoch [22/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1089, Combined: 0.1833\n",
      "Validation Losses - Regression: 0.0122, Classification: 0.1087, Combined: 0.1820\n",
      "-------------\n",
      "Epoch [23/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0124, Classification: 0.1088, Combined: 0.1832\n",
      "Validation Losses - Regression: 0.0123, Classification: 0.1090, Combined: 0.1828\n",
      "-------------\n",
      "Decreasing learning rate to 0.0024999999441206455\n",
      "Epoch [24/300]\n",
      "Learning rate: 2.50e-03\n",
      "Train Losses - Regression: 0.0114, Classification: 0.1061, Combined: 0.1746\n",
      "Validation Losses - Regression: 0.0113, Classification: 0.1088, Combined: 0.1763\n",
      "-------------\n",
      "Decreasing learning rate to 0.0012499999720603228\n",
      "Epoch [25/300]\n",
      "Learning rate: 1.25e-03\n",
      "Train Losses - Regression: 0.0110, Classification: 0.1045, Combined: 0.1704\n",
      "Validation Losses - Regression: 0.0110, Classification: 0.1049, Combined: 0.1707\n",
      "-------------\n",
      "Decreasing learning rate to 0.0006249999860301614\n",
      "Epoch [26/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0108, Classification: 0.1037, Combined: 0.1683\n",
      "Validation Losses - Regression: 0.0108, Classification: 0.1043, Combined: 0.1693\n",
      "-------------\n",
      "Decreasing learning rate to 0.0003124999930150807\n",
      "Epoch [27/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1032, Combined: 0.1671\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1042, Combined: 0.1678\n",
      "-------------\n",
      "Epoch [28/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1032, Combined: 0.1670\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1041, Combined: 0.1681\n",
      "-------------\n",
      "Epoch [29/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1031, Combined: 0.1669\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1041, Combined: 0.1680\n",
      "-------------\n",
      "Epoch [30/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1031, Combined: 0.1668\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1677\n",
      "-------------\n",
      "Epoch [31/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1667\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1040, Combined: 0.1676\n",
      "-------------\n",
      "Epoch [32/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1667\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1674\n",
      "-------------\n",
      "Epoch [33/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1666\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1039, Combined: 0.1674\n",
      "-------------\n",
      "Epoch [34/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1666\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1674\n",
      "-------------\n",
      "Epoch [35/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1030, Combined: 0.1665\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1040, Combined: 0.1676\n",
      "-------------\n",
      "Epoch [36/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1029, Combined: 0.1665\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1039, Combined: 0.1676\n",
      "-------------\n",
      "Epoch [37/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1029, Combined: 0.1665\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1038, Combined: 0.1671\n",
      "-------------\n",
      "Decreasing learning rate to 0.00015624999650754035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# scheduler.on_epoch_begin(epoch, logs=None)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, targets \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m---> 25\u001b[0m     regression_loss, classification_loss, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     running_loss                \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     28\u001b[0m     running_regression_loss     \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m regression_loss\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_regression_losses     = []\n",
    "train_classification_losses = []\n",
    "train_combined_losses       = []\n",
    "val_regression_losses       = []\n",
    "val_classification_losses   = []\n",
    "val_combined_losses         = []\n",
    "\n",
    "learning_rates = []\n",
    "\n",
    "out_file = open(os.path.join(OUT_PATH, LOSS_FNAME), \"w\")\n",
    "out_file.write(\"train_regression_loss,train_classification_loss,train_combined_loss,val_regression_loss,val_classification_loss,val_combined_loss,learning_rate\\n\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    running_loss                = 0.0\n",
    "    running_regression_loss     = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    # scheduler.on_epoch_begin(epoch, logs=None)\n",
    "    \n",
    "    for features, targets in train_dataset:\n",
    "        \n",
    "        regression_loss, classification_loss, loss = train_step(features, targets)\n",
    "        \n",
    "        running_loss                += loss.numpy()\n",
    "        running_regression_loss     += regression_loss.numpy()\n",
    "        running_classification_loss += classification_loss.numpy()\n",
    "        \n",
    "        # scheduler.on_batch_end(batch=None)\n",
    "        \n",
    "        # current_lr = optimizer.lr.numpy()\n",
    "        \n",
    "        # lr_per_batch.append(current_lr)\n",
    "        # print(f\"Learning rate: {current_lr:.2e}\", end=\"\\r\")\n",
    "        \n",
    "\n",
    "        \n",
    "    # Note: Remember to adjust if not using batches of equal sizes\n",
    "    train_loss                = running_loss                / len(train_dataset)\n",
    "    train_regression_loss     = running_regression_loss     / len(train_dataset)\n",
    "    train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "\n",
    "    train_regression_losses.append(train_regression_loss)\n",
    "    train_classification_losses.append(train_classification_loss)\n",
    "    train_combined_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    running_loss                = 0.0\n",
    "    running_regression_loss     = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    for features, targets in val_dataset:\n",
    "        \n",
    "        regression_loss, classification_loss, loss = val_step(features, targets)\n",
    "        \n",
    "        running_loss                += loss.numpy()\n",
    "        running_regression_loss     += regression_loss.numpy()\n",
    "        running_classification_loss += classification_loss.numpy()\n",
    "    \n",
    "    avg_val_loss                = running_loss                / len(val_dataset)\n",
    "    avg_val_regression_loss     = running_regression_loss     / len(val_dataset)\n",
    "    avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "        \n",
    "    val_regression_losses.append(avg_val_regression_loss)\n",
    "    val_classification_losses.append(avg_val_classification_loss)\n",
    "    val_combined_losses.append(avg_val_loss)\n",
    "    \n",
    "    \n",
    "    current_lr = optimizer.lr.numpy()\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Learning rate: {current_lr:.2e}\")\n",
    "    print(f\"Train Losses - Regression: {train_regression_losses[-1]:.4f}, Classification: {train_classification_losses[-1]:.4f}, Combined: {train_combined_losses[-1]:.4f}\")\n",
    "    print(f\"Validation Losses - Regression: {val_regression_losses[-1]:.4f}, Classification: {val_classification_losses[-1]:.4f}, Combined: {val_combined_losses[-1]:.4f}\")\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    \n",
    "    scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_loss})\n",
    "\n",
    "\n",
    "    with open(os.path.join(OUT_PATH, LOSS_FNAME), \"a\") as output_file:\n",
    "        output_file.write(f\"{train_regression_losses[-1]},{train_classification_losses[-1]},{train_combined_losses[-1]},{val_regression_losses[-1]},{val_classification_losses[-1]},{val_combined_losses[-1]},{current_lr}\\n\")\n",
    "\n",
    "    # Check for early stopping based on the new criterion\n",
    "    if avg_val_loss < (1 - early_stopping_threshold) * best_val_loss:  # 0.001 corresponds to 0.1%\n",
    "        epochs_without_improvement = 0\n",
    "        best_val_loss = min(best_val_loss, avg_val_loss)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test data from `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptReco_true</th>\n",
       "      <th>etaReco_true</th>\n",
       "      <th>phiReco_true</th>\n",
       "      <th>chargeReco_true</th>\n",
       "      <th>ptReco_pred</th>\n",
       "      <th>etaReco_pred</th>\n",
       "      <th>phiReco_pred</th>\n",
       "      <th>chargeReco_pred</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>chargeL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.14094</td>\n",
       "      <td>0.725395</td>\n",
       "      <td>1.863000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.845138</td>\n",
       "      <td>0.742052</td>\n",
       "      <td>1.876867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>1.887140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.94190</td>\n",
       "      <td>-0.825676</td>\n",
       "      <td>-2.726760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.148404</td>\n",
       "      <td>-0.828740</td>\n",
       "      <td>-2.722502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.804750</td>\n",
       "      <td>-2.727080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.79640</td>\n",
       "      <td>0.143982</td>\n",
       "      <td>-0.391065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.846815</td>\n",
       "      <td>0.145370</td>\n",
       "      <td>-0.400197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.5</td>\n",
       "      <td>0.141375</td>\n",
       "      <td>-0.414516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.03724</td>\n",
       "      <td>0.803833</td>\n",
       "      <td>1.414660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.492181</td>\n",
       "      <td>0.817047</td>\n",
       "      <td>1.398779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>1.396260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.51210</td>\n",
       "      <td>-0.508647</td>\n",
       "      <td>-2.192450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.836945</td>\n",
       "      <td>-0.502010</td>\n",
       "      <td>-2.190900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.478500</td>\n",
       "      <td>-2.192570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877628</th>\n",
       "      <td>4.12733</td>\n",
       "      <td>-0.517609</td>\n",
       "      <td>-0.834530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.313579</td>\n",
       "      <td>-0.514054</td>\n",
       "      <td>-0.774689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-0.500250</td>\n",
       "      <td>-0.818123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877629</th>\n",
       "      <td>23.83930</td>\n",
       "      <td>0.883597</td>\n",
       "      <td>1.309400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.070823</td>\n",
       "      <td>0.876216</td>\n",
       "      <td>1.311607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>1.309000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877630</th>\n",
       "      <td>6.30444</td>\n",
       "      <td>0.539451</td>\n",
       "      <td>2.764210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.929778</td>\n",
       "      <td>0.508884</td>\n",
       "      <td>2.758275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.500250</td>\n",
       "      <td>2.759800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877631</th>\n",
       "      <td>34.38710</td>\n",
       "      <td>-0.417246</td>\n",
       "      <td>3.093800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.442506</td>\n",
       "      <td>-0.502704</td>\n",
       "      <td>3.086110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.511125</td>\n",
       "      <td>3.087050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877632</th>\n",
       "      <td>9.41212</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>2.661640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.530159</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>2.663181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.661630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>877633 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ptReco_true  etaReco_true  phiReco_true  chargeReco_true  ptReco_pred  \\\n",
       "0           7.14094      0.725395      1.863000              0.0     6.845138   \n",
       "1          22.94190     -0.825676     -2.726760              0.0    20.148404   \n",
       "2          77.79640      0.143982     -0.391065              0.0    81.846815   \n",
       "3           7.03724      0.803833      1.414660              0.0     7.492181   \n",
       "4          36.51210     -0.508647     -2.192450              1.0    33.836945   \n",
       "...             ...           ...           ...              ...          ...   \n",
       "877628      4.12733     -0.517609     -0.834530              1.0     5.313579   \n",
       "877629     23.83930      0.883597      1.309400              1.0    22.070823   \n",
       "877630      6.30444      0.539451      2.764210              0.0     5.929778   \n",
       "877631     34.38710     -0.417246      3.093800              0.0    16.442506   \n",
       "877632      9.41212     -0.005519      2.661640              1.0     9.530159   \n",
       "\n",
       "        etaReco_pred  phiReco_pred  chargeReco_pred  ptL1     etaL1     phiL1  \\\n",
       "0           0.742052      1.876867              0.0   9.0  0.750375  1.887140   \n",
       "1          -0.828740     -2.722502              0.0  27.0 -0.804750 -2.727080   \n",
       "2           0.145370     -0.400197              0.0  93.5  0.141375 -0.414516   \n",
       "3           0.817047      1.398779              0.0   9.5  0.804750  1.396260   \n",
       "4          -0.502010     -2.190900              1.0  41.0 -0.478500 -2.192570   \n",
       "...              ...           ...              ...   ...       ...       ...   \n",
       "877628     -0.514054     -0.774689              1.0   8.5 -0.500250 -0.818123   \n",
       "877629      0.876216      1.311607              1.0  29.0  0.880875  1.309000   \n",
       "877630      0.508884      2.758275              0.0   8.5  0.500250  2.759800   \n",
       "877631     -0.502704      3.086110              1.0  15.0 -0.511125  3.087050   \n",
       "877632     -0.004291      2.663181              1.0  10.0  0.000000  2.661630   \n",
       "\n",
       "        chargeL1  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "877628         1  \n",
       "877629         1  \n",
       "877630         0  \n",
       "877631         1  \n",
       "877632         1  \n",
       "\n",
       "[877633 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = test_data[stub_features].values\n",
    "test_targets  = test_data[target_features].values\n",
    "\n",
    "reg_predictions, class_predictions = model(test_features, training=False)\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            test_features,\n",
    "            test_targets,\n",
    "            reg_predictions.numpy(),\n",
    "            class_predictions.numpy(),\n",
    "            test_data[l1_features].values,\n",
    "        ),\n",
    "        axis=1\n",
    "    ),\n",
    "    columns=stub_features + target_features + [\"ptRecoInverse_pred\", \"etaExtRecoSt2_pred\", \"phiExtRecoSt2_pred\", \"chargeReco_pred\"] + l1_features\n",
    ")\n",
    "\n",
    "test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 1 / (1 + np.exp(-x)))\n",
    "test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "# rescale the features\n",
    "for key in normalizations.keys():\n",
    "    test_df[key] = test_df[key] * normalizations[key]\n",
    "    \n",
    "# rescale the predictions\n",
    "test_df[\"ptRecoInverse_pred\"] = test_df[\"ptRecoInverse_pred\"] * normalizations[\"ptRecoInverse\"]\n",
    "test_df[\"etaExtRecoSt2_pred\"] = test_df[\"etaExtRecoSt2_pred\"] * normalizations[\"etaExtRecoSt2\"]\n",
    "test_df[\"phiExtRecoSt2_pred\"] = test_df[\"phiExtRecoSt2_pred\"] * normalizations[\"phiExtRecoSt2\"]\n",
    "test_df[\"chargeReco_pred\"]    = test_df[\"chargeReco_pred\"]    * normalizations[\"chargeReco\"]\n",
    "\n",
    "# drop features keep only predictions and targets and L1 features\n",
    "test_df = test_df[[\"ptRecoInverse\", \"etaExtRecoSt2\", \"phiExtRecoSt2\", \"chargeReco\", \"ptRecoInverse_pred\", \"etaExtRecoSt2_pred\", \"phiExtRecoSt2_pred\", \"chargeReco_pred\"] + l1_features]\n",
    "\n",
    "# create ptReco_true and etaReco_true and phiReco_true and chargeReco_true\n",
    "# create ptReco_pred and etaReco_pred and phiReco_pred and chargeReco_pred\n",
    "test_df[\"ptReco_true\"]     = test_df[\"ptRecoInverse\"].apply(lambda x: 1 / x)\n",
    "test_df[\"etaReco_true\"]    = test_df[\"etaExtRecoSt2\"]\n",
    "test_df[\"phiReco_true\"]    = test_df[\"phiExtRecoSt2\"]\n",
    "test_df[\"chargeReco_true\"] = test_df[\"chargeReco\"]\n",
    "\n",
    "test_df[\"ptReco_pred\"]     = test_df[\"ptRecoInverse_pred\"].apply(lambda x: 1 / x)\n",
    "test_df[\"etaReco_pred\"]    = test_df[\"etaExtRecoSt2_pred\"]\n",
    "test_df[\"phiReco_pred\"]    = test_df[\"phiExtRecoSt2_pred\"]\n",
    "test_df[\"chargeReco_pred\"] = test_df[\"chargeReco_pred\"]\n",
    "\n",
    "# transform hwSignL1 into chargeL1\n",
    "test_df[\"chargeL1\"] = test_df[\"hwSignL1\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# drop ptRecoInverse and etaExtRecoSt2 and phiExtRecoSt2 and chargeReco\n",
    "# drop ptRecoInverse_pred and etaExtRecoSt2_pred and phiExtRecoSt2_pred and chargeReco_pred\n",
    "test_df = test_df[[\"ptReco_true\", \"etaReco_true\", \"phiReco_true\", \"chargeReco_true\", \"ptReco_pred\", \"etaReco_pred\", \"phiReco_pred\", \"chargeReco_pred\", \"ptL1\", \"etaL1\", \"phiL1\", \"chargeL1\"]]\n",
    "\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Test Set: 97.21%\n",
      "L1 Classification Accuracy on Test Set: 97.28%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (test_df[\"chargeReco_pred\"] == test_df[\"chargeReco_true\"]).sum() / len(test_df)\n",
    "\n",
    "print(f\"Classification Accuracy on Test Set: {accuracy*100:.2f}%\")\n",
    "\n",
    "l1_accuracy = (test_df[\"chargeL1\"] == test_df[\"chargeReco_true\"]).sum() / len(test_df)\n",
    "\n",
    "print(f\"L1 Classification Accuracy on Test Set: {l1_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/eos/user/f/fcufino'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m PLOT_FLAG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(PLOT_PATH):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPLOT_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/eos/user/f/fcufino'"
     ]
    }
   ],
   "source": [
    "prob      = True\n",
    "PLOT_PATH = f\"/eos/user/{USER[0]}/{USER}/nnreco-plots/\"\n",
    "PLOT_FLAG = False\n",
    "\n",
    "if not os.path.exists(PLOT_PATH):\n",
    "    os.makedirs(PLOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_res_pred = (test_df.etaReco_pred - test_df.etaReco_true) / test_df.etaReco_true\n",
    "\n",
    "# if eta_res_pred < -0.5 then add 1 to shift the histogram to the right\n",
    "# mask = np.where(eta_res_pred < -0.5, 1, 0)\n",
    "# eta_res_pred = eta_res_pred + mask\n",
    "\n",
    "# recompute the predicted eta\n",
    "# test_df[\"etaReco_pred\"] = test_df.etaReco_true * (1 + eta_res_pred)\n",
    "\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_l1   = (test_df.ptL1  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_rec_l1   = (test_df.ptL1/1.2  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "\n",
    "eta_res_pred = (test_df.etaReco_pred - test_df.etaReco_true) \n",
    "eta_res_l1   = (test_df.etaL1   - test_df.etaReco_true)\n",
    "\n",
    "phi_res_pred = (test_df.phiReco_pred - test_df.phiReco_true)\n",
    "phi_res_l1   = (test_df.phiL1   - test_df.phiReco_true)\n",
    "\n",
    "\n",
    "rlabel = \"ZeroBias+MuonEG 2023 (13.6 TeV)\"\n",
    "\n",
    "xmin = -1.4\n",
    "xmax = 1.4\n",
    "binw = 0.02\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(pt_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(pt_res_rec_l1, bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1 (recal)\")\n",
    "plt.hist(pt_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta p_T / p_T$\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/pt_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "xmin = -0.19\n",
    "xmax = 0.19\n",
    "binw = 0.0025\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(eta_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(eta_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta \\eta$\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/eta_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "xmin = -0.19\n",
    "xmax = 0.19\n",
    "binw = 0.0025\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(phi_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(phi_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta\\\\varphi$ (rad)\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/phi_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pT resolution vs pT perd true 2D histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 50\n",
    "xbinw = 0.5\n",
    "ymin = -1.5\n",
    "ymax = 1.5\n",
    "ybinw = 0.02\n",
    "\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "heights, binsx, binsy = np.histogram2d(test_df.ptReco_true, pt_res_pred, bins=(np.arange(xmin, xmax+xbinw, xbinw), np.arange(ymin, ymax+ybinw, ybinw)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,9), constrained_layout=True)\n",
    "hep.cms.label(label=\"Preliminary\", data=True, year=2023, rlabel=rlabel)\n",
    "\n",
    "hep.hist2dplot(heights, binsx, binsy, ax=ax, cmap=\"viridis\", flow=None, norm = mpl.colors.LogNorm())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(\"Events\", labelpad=15) # , fontsize=36)\n",
    "# cbar.ax.tick_params(labelsize=36)\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Reco muon $p_T$ (GeV)\")\n",
    "plt.ylabel(\"$\\Delta p_T / p_T$\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}pt_res_vs_pt_reco_true_zoom{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 50\n",
    "binw = 0.5\n",
    "\n",
    "# Compute pT resolution\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_l1   = (test_df.ptL1 / 1.2  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "\n",
    "# Define bins for pT reco true\n",
    "pt_true_bins = np.arange(xmin, xmax+binw, binw)  # From 0 to 50 GeV with bin width 1 GeV\n",
    "\n",
    "# Initialize arrays to store average pT resolution and standard deviation in each bin\n",
    "avg_pt_res_pred = np.zeros(len(pt_true_bins) - 1)\n",
    "std_pt_res_pred = np.zeros(len(pt_true_bins) - 1)\n",
    "avg_pt_res_l1   = np.zeros(len(pt_true_bins) - 1)\n",
    "std_pt_res_l1   = np.zeros(len(pt_true_bins) - 1)\n",
    "\n",
    "\n",
    "# Loop through the bins to calculate average pT resolution and standard deviation\n",
    "for i in range(len(pt_true_bins) - 1):\n",
    "    lower_bound = pt_true_bins[i]\n",
    "    upper_bound = pt_true_bins[i + 1]\n",
    "    \n",
    "    # Mask data points that fall within the current bin\n",
    "    mask = (test_df.ptReco_true >= lower_bound) & (test_df.ptReco_true < upper_bound)\n",
    "    \n",
    "    # Compute the average pT resolution for the current bin\n",
    "    avg_pt_res_pred[i] = np.mean(pt_res_pred[mask])\n",
    "    avg_pt_res_l1[i]   = np.mean(pt_res_l1[mask])\n",
    "    \n",
    "    # Compute the standard deviation for the current bin\n",
    "    std_pt_res_pred[i] = np.std(pt_res_pred[mask]) # / np.sqrt(len(pt_res_pred[mask]))\n",
    "    std_pt_res_l1[i]   = np.std(pt_res_l1[mask])   # / np.sqrt(len(pt_res_l1[mask]))\n",
    "\n",
    "\n",
    "# Create the profile plot\n",
    "fig, ax = plt.subplots(figsize=(12,9), constrained_layout=True)\n",
    "hep.cms.label(ax=ax, data=True, year=2023, label=\"Preliminary\", rlabel=rlabel)\n",
    "plt.plot(pt_true_bins[:-1], avg_pt_res_l1, \"-\", label=\"L1\", color=\"C0\")\n",
    "plt.fill_between(pt_true_bins[:-1], avg_pt_res_l1 - std_pt_res_l1, avg_pt_res_l1 + std_pt_res_l1, alpha=0.3, color=\"C0\")\n",
    "\n",
    "plt.plot(pt_true_bins[:-1], avg_pt_res_pred, \"-\", label=\"NN\", color=\"C1\")\n",
    "plt.fill_between(pt_true_bins[:-1], avg_pt_res_pred - std_pt_res_pred, avg_pt_res_pred + std_pt_res_pred, alpha=0.3, color=\"C1\")\n",
    "\n",
    "ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=2, zorder=0)\n",
    "\n",
    "# Plotting the average pT resolution with error bars\n",
    "\n",
    "# ax.errorbar(\n",
    "#     x=pt_true_bins[:-1], \n",
    "#     y=avg_pt_res_l1, \n",
    "#     yerr=std_pt_res_l1, \n",
    "#     marker='o', \n",
    "#     linestyle=\"\",\n",
    "#     markersize=12,\n",
    "#     elinewidth=3,\n",
    "#     capthick=3,\n",
    "#     capsize=5,\n",
    "#     label=\"L1\",\n",
    "#     color=PALETTE[-1],\n",
    "# )\n",
    "\n",
    "\n",
    "# ax.errorbar(\n",
    "#     x=pt_true_bins[:-1], \n",
    "#     y=avg_pt_res_pred, \n",
    "#     yerr=std_pt_res_pred, \n",
    "#     marker='o', \n",
    "#     linestyle=\"\",\n",
    "#     markersize=12,\n",
    "#     elinewidth=3,\n",
    "#     capthick=3,\n",
    "#     capsize=5,\n",
    "#     label=\"NN Prediction\",\n",
    "#     color=PALETTE[0],\n",
    "# )\n",
    "\n",
    "\n",
    "# ax.legend(fontsize=36, loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Reco muon $p_T$ [GeV]\")\n",
    "plt.ylabel(\"$\\Delta p_T / p_T$\")\n",
    "\n",
    "# ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}pt_res_vs_pt_reco_true_profile{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.xlim(5,xmax)\n",
    "plt.ylim(-2,2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_regression_losses, label=\"Train\")\n",
    "ax.plot(val_regression_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Regression Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_classification_losses, label=\"Train\")\n",
    "ax.plot(val_classification_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_combined_losses, label=\"Train\")\n",
    "ax.plot(val_combined_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Combined Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(learning_rates, lw=3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}learning_rate{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(learning_rates, lw=3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}log_learning_rate{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform multiple trainings to compute average loss (TAKES A LOT OF TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=True)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return regression_loss, classification_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(features, targets):\n",
    "    regression_targets = targets[:, :-1]\n",
    "    classification_targets = targets[:, -1]\n",
    "    reg_outputs, class_outputs = model(features, training=False)\n",
    "    regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "    classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "    loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "    return regression_loss, classification_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_ = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(FILE_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_data_ = pd.concat([full_data_, pd.read_csv(FILE_PATH + file)], ignore_index=True)\n",
    "        \n",
    "full_data_ = full_data_.iloc[:, :-1]\n",
    "\n",
    "# drop rows with ptL1 == 4.5\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] != 4.5]\n",
    "\n",
    "# drop rows with reco eta > 1 and < -1\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] < 1]\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] > -1]\n",
    "\n",
    "# drop rows with ptL1 > 50\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] < 50]\n",
    "\n",
    "# drop rows with ptReco > 50\n",
    "full_data_ = full_data_[full_data_[\"ptReco\"] < 50]\n",
    "\n",
    "\n",
    "# mask_1 = (full_data_.etaL1 == 0) & (np.abs(full_data_.etaExtRecoSt2) == 0)\n",
    "# mask_2 = (full_data_.etaL1 != 0)\n",
    "\n",
    "# mask = mask_1 | mask_2\n",
    "\n",
    "# full_data_ = full_data_[mask]\n",
    "\n",
    "full_data_[\"ptRecoInverse\"] = 1 / full_data_[\"ptReco\"]\n",
    "\n",
    "full_data = full_data_[stub_features + target_features + l1_features]\n",
    "\n",
    "full_data[\"chargeReco\"] = full_data[\"chargeReco\"].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_criterion = losses.BinaryCrossentropy(from_logits=True)\n",
    "regression_criterion     = losses.MeanAbsoluteError()\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor = 0.5\n",
    "patience = 5\n",
    "min_loss_improvement = 0.1\n",
    "\n",
    "classification_weight = 1\n",
    "regression_weight     = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regression_losses_all     = []\n",
    "train_classification_losses_all = []\n",
    "train_combined_losses_all       = []\n",
    "val_regression_losses_all       = []\n",
    "val_classification_losses_all   = []\n",
    "val_combined_losses_all         = []\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    input_size    = len(stub_features)\n",
    "    architecture  = [input_size, 64, 32, 16, 8]\n",
    "    output_size   = len(target_features)\n",
    "    learning_rate = 1e-2\n",
    "    num_epochs    = 300\n",
    "    batch_size    = 2**10\n",
    "    reg_strength  = 1e-3\n",
    "\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    patience = 10  # Number of epochs to wait before stopping\n",
    "    early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "    train_val_data, test_data = train_test_split(full_data,      test_size=0.3, random_state=42)\n",
    "    train_data, val_data      = train_test_split(train_val_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    # normalize the data\n",
    "    for key in normalizations.keys():\n",
    "        train_data[key] = train_data[key] / normalizations[key]\n",
    "        val_data[key]   = val_data[key]   / normalizations[key]\n",
    "        test_data[key]  = test_data[key]  / normalizations[key]\n",
    "        \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data[stub_features].values, train_data[target_features].values)).batch(batch_size).shuffle(buffer_size=len(train_data))\n",
    "    val_dataset   = tf.data.Dataset.from_tensor_slices((val_data[stub_features].values, val_data[target_features].values)).batch(batch_size)\n",
    "    test_dataset  = tf.data.Dataset.from_tensor_slices((test_data[stub_features].values, test_data[target_features].values)).batch(batch_size)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = MultiTaskNN(architecture, reg_strength=reg_strength)\n",
    "\n",
    "    # Build the model with the batch input shape\n",
    "    bs = None  # None allows for variable batch size\n",
    "    model.build((bs, input_size))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    scheduler = CustomLRScheduler(\n",
    "        optimizer, \n",
    "        factor=scale_factor, \n",
    "        patience=patience, \n",
    "        min_improvement=min_loss_improvement, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "    train_regression_losses     = []\n",
    "    train_classification_losses = []\n",
    "    train_combined_losses       = []\n",
    "    val_regression_losses       = []\n",
    "    val_classification_losses   = []\n",
    "    val_combined_losses         = []\n",
    "\n",
    "    learning_rates = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(features, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            regression_targets = targets[:, :-1]\n",
    "            classification_targets = targets[:, -1]\n",
    "            reg_outputs, class_outputs = model(features, training=True)\n",
    "            regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "            classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "            loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        return regression_loss, classification_loss, loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(features, targets):\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=False)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "        return regression_loss, classification_loss, loss\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        running_loss                = 0.0\n",
    "        running_regression_loss     = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        \n",
    "        \n",
    "        for features, targets in train_dataset:\n",
    "            \n",
    "            regression_loss, classification_loss, loss = train_step(features, targets)\n",
    "            \n",
    "            running_loss                += loss.numpy()\n",
    "            running_regression_loss     += regression_loss.numpy()\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "\n",
    "\n",
    "            \n",
    "        # Note: Remember to adjust if not using batches of equal sizes\n",
    "        train_loss                = running_loss                / len(train_dataset)\n",
    "        train_regression_loss     = running_regression_loss     / len(train_dataset)\n",
    "        train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "\n",
    "        train_regression_losses.append(train_regression_loss)\n",
    "        train_classification_losses.append(train_classification_loss)\n",
    "        train_combined_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        running_loss                = 0.0\n",
    "        running_regression_loss     = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        \n",
    "        for features, targets in val_dataset:\n",
    "            \n",
    "            regression_loss, classification_loss, loss = val_step(features, targets)\n",
    "            \n",
    "            running_loss                += loss.numpy()\n",
    "            running_regression_loss     += regression_loss.numpy()\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "        \n",
    "        avg_val_loss                = running_loss                / len(val_dataset)\n",
    "        avg_val_regression_loss     = running_regression_loss     / len(val_dataset)\n",
    "        avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "            \n",
    "        val_regression_losses.append(avg_val_regression_loss)\n",
    "        val_classification_losses.append(avg_val_classification_loss)\n",
    "        val_combined_losses.append(avg_val_loss)\n",
    "        \n",
    "        \n",
    "        current_lr = optimizer.lr.numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        print(f\"Train Losses - Regression: {train_regression_losses[-1]:.4f}, Classification: {train_classification_losses[-1]:.4f}, Combined: {train_combined_losses[-1]:.4f}\")\n",
    "        print(f\"Validation Losses - Regression: {val_regression_losses[-1]:.4f}, Classification: {val_classification_losses[-1]:.4f}, Combined: {val_combined_losses[-1]:.4f}\")\n",
    "        print(\"-------------\")\n",
    "        \n",
    "        \n",
    "        scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_loss})\n",
    "\n",
    "        \n",
    "\n",
    "        # Check for early stopping based on the new criterion\n",
    "        if avg_val_loss < (1 - early_stopping_threshold) * best_val_loss:  # 0.001 corresponds to 0.1%\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = min(best_val_loss, avg_val_loss)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "            break\n",
    "        \n",
    "    train_regression_losses_all.append(train_regression_losses)\n",
    "    train_classification_losses_all.append(train_classification_losses)\n",
    "    train_combined_losses_all.append(train_combined_losses)\n",
    "    val_regression_losses_all.append(val_regression_losses)\n",
    "    val_classification_losses_all.append(val_classification_losses)\n",
    "    val_combined_losses_all.append(val_combined_losses)\n",
    "    \n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    del train_data\n",
    "    del val_data\n",
    "    del test_data\n",
    "    del train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and std of the losses for each epoch\n",
    "# the lists in the lists are of different lengths, so we need to pad them with the last value\n",
    "# then we can compute the mean and std\n",
    "\n",
    "train_regression_losses_all      = [x + [x[-1]] * (max(map(len, train_regression_losses_all)) - len(x)) for x in train_regression_losses_all]\n",
    "train_classification_losses_all  = [x + [x[-1]] * (max(map(len, train_classification_losses_all)) - len(x)) for x in train_classification_losses_all]\n",
    "train_combined_losses_all        = [x + [x[-1]] * (max(map(len, train_combined_losses_all)) - len(x)) for x in train_combined_losses_all]\n",
    "\n",
    "val_regression_losses_all        = [x + [x[-1]] * (max(map(len, val_regression_losses_all)) - len(x)) for x in val_regression_losses_all]\n",
    "val_classification_losses_all    = [x + [x[-1]] * (max(map(len, val_classification_losses_all)) - len(x)) for x in val_classification_losses_all]\n",
    "val_combined_losses_all          = [x + [x[-1]] * (max(map(len, val_combined_losses_all)) - len(x)) for x in val_combined_losses_all]\n",
    "\n",
    "train_regression_losses_mean     = np.mean(np.array(train_regression_losses_all), axis=0)[1:]\n",
    "train_regression_losses_std      = np.std(np.array(train_regression_losses_all), axis=0)[1:]\n",
    "train_classification_losses_mean = np.mean(np.array(train_classification_losses_all), axis=0)[1:]\n",
    "train_classification_losses_std  = np.std(np.array(train_classification_losses_all), axis=0)[1:]\n",
    "train_combined_losses_mean       = np.mean(np.array(train_combined_losses_all), axis=0)[1:]\n",
    "train_combined_losses_std        = np.std(np.array(train_combined_losses_all), axis=0)[1:]\n",
    "\n",
    "val_regression_losses_mean       = np.mean(np.array(val_regression_losses_all), axis=0)[1:]\n",
    "val_regression_losses_std        = np.std(np.array(val_regression_losses_all), axis=0)[1:]\n",
    "val_classification_losses_mean   = np.mean(np.array(val_classification_losses_all), axis=0)[1:]\n",
    "val_classification_losses_std    = np.std(np.array(val_classification_losses_all), axis=0)[1:]\n",
    "val_combined_losses_mean         = np.mean(np.array(val_combined_losses_all), axis=0)[1:]\n",
    "val_combined_losses_std          = np.std(np.array(val_combined_losses_all), axis=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# REGRESSION LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_regression_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_regression_losses_mean)), train_regression_losses_mean - train_regression_losses_std, train_regression_losses_mean + train_regression_losses_std, alpha=0.3)\n",
    "ax.plot(val_regression_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_regression_losses_mean)), val_regression_losses_mean - val_regression_losses_std, val_regression_losses_mean + val_regression_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Regression Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}regression_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# CLASSIFICATION LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_classification_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_classification_losses_mean)), train_classification_losses_mean - train_classification_losses_std, train_classification_losses_mean + train_classification_losses_std, alpha=0.3)\n",
    "ax.plot(val_classification_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_classification_losses_mean)), val_classification_losses_mean - val_classification_losses_std, val_classification_losses_mean + val_classification_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}classification_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# COMBINED LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_combined_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_combined_losses_mean)), train_combined_losses_mean - train_combined_losses_std, train_combined_losses_mean + train_combined_losses_std, alpha=0.3)\n",
    "ax.plot(val_combined_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_combined_losses_mean)), val_combined_losses_mean - val_combined_losses_std, val_combined_losses_mean + val_combined_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Combined Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}combined_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
