{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMTF input stubs fit with a Deep Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 16:41:49.262661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-03 16:41:49.262711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-03 16:41:49.263702: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-03 16:41:49.270249: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mplhep as hep\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.style.use(\"CMS\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training device: {'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.getenv(\"USER\")\n",
    "\n",
    "# FILE_PATH = \"/eos/cms/store/cmst3/group/daql1scout/ml_data/run3/bmtf_stubs_refit/\"\n",
    "FILE_PATH = \"/mnt/ml_data/run3/bmtf_stubs_refit/\"\n",
    "\n",
    "FILE_NAME = \"rereco\"\n",
    "\n",
    "OUT_PATH = \".\"\n",
    "LOSS_FNAME = \"losses.csv\"\n",
    "\n",
    "FIGSIZE = (12, 9)\n",
    "\n",
    "petroff_10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler('color', petroff_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model for regression+classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNN(tf.keras.Model):\n",
    "    def __init__(self, architecture, reg_strength=0.01):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "\n",
    "        # Check if the architecture list has at least 2 values (input size and one hidden layer)\n",
    "        if len(architecture) < 2:\n",
    "            raise ValueError(\"Architecture must contain at least input size and one hidden layer.\")\n",
    "\n",
    "        self.layers_list = []\n",
    "\n",
    "        # Iterate over the architecture list to dynamically create dense layers followed by batch normalization\n",
    "        for i in range(1, len(architecture)):\n",
    "            self.layers_list.append(layers.Dense(architecture[i], kernel_regularizer=regularizers.l2(reg_strength)))\n",
    "            self.layers_list.append(layers.Activation('elu'))\n",
    "\n",
    "        # Separate heads for regression and classification tasks\n",
    "        self.regression_head = layers.Dense(3, kernel_regularizer=regularizers.l2(reg_strength))  # for pt, eta, phi\n",
    "        self.classification_head = layers.Dense(1, kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "        # self.classification_head = layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "\n",
    "        reg_output = self.regression_head(x)\n",
    "        class_output = self.classification_head(x)\n",
    "        return reg_output, class_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate scheduler callback\n",
    "class CustomLRScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        optimizer, \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_improvement=0.01, \n",
    "        verbose=True\n",
    "    ):\n",
    "        super(CustomLRScheduler, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_improvement = min_improvement\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            self.optimizer.lr.assign(self.min_lr)\n",
    "        if epoch < self.decrease_epoch:\n",
    "            self.increase_flag = True\n",
    "        else:\n",
    "            self.increase_flag = False\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if loss:\n",
    "            relative_improvement = (self.best_loss - loss) / self.best_loss\n",
    "\n",
    "            if relative_improvement < self.min_improvement:\n",
    "                self.patience_counter += 1\n",
    "            else:\n",
    "                self.patience_counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "            if self.patience_counter >= self.patience:\n",
    "                self._decrease_lr()\n",
    "\n",
    "    def _decrease_lr(self):\n",
    "        old_lr = self.optimizer.lr.numpy()\n",
    "        new_lr = old_lr * self.factor\n",
    "        self.optimizer.lr.assign(new_lr)\n",
    "        if self.verbose:\n",
    "            print(f\"Decreasing learning rate to {new_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stubs_norm         = 2**2\n",
    "station_norm         = 2**2\n",
    "sector_norm          = 2**3\n",
    "wheel_norm           = 2**1\n",
    "eta_norm             = 2**8\n",
    "qeta_norm            = 2**1\n",
    "tag_norm             = 2**0\n",
    "phi_norm             = 2**11\n",
    "phib_norm            = 2**9\n",
    "quality_norm         = 2**3\n",
    "reco_pt_norm         = 2**0\n",
    "reco_pt_inverse_norm = 2**0\n",
    "reco_eta_norm        = 2**2\n",
    "reco_phi_norm        = 2**2\n",
    "reco_charge_norm     = 2**0\n",
    "\n",
    "normalizations = {\n",
    "    \"n_stubs\": n_stubs_norm,\n",
    "    \"s1_stNum\": station_norm,\n",
    "    \"s1_scNum\": sector_norm,\n",
    "    \"s1_whNum\": wheel_norm,\n",
    "    \"s1_eta_1\": eta_norm,\n",
    "    \"s1_qeta_1\": qeta_norm,\n",
    "    \"s1_eta_2\": eta_norm,\n",
    "    \"s1_qeta_2\": qeta_norm,\n",
    "    \"s1_tag\": tag_norm,\n",
    "    \"s1_phi\": phi_norm,\n",
    "    \"s1_phiB\": phib_norm,\n",
    "    \"s1_quality\": quality_norm,\n",
    "    \"s2_stNum\": station_norm,\n",
    "    \"s2_scNum\": sector_norm,\n",
    "    \"s2_whNum\": wheel_norm,\n",
    "    \"s2_eta_1\": eta_norm,\n",
    "    \"s2_qeta_1\": qeta_norm,\n",
    "    \"s2_eta_2\": eta_norm,\n",
    "    \"s2_qeta_2\": qeta_norm,\n",
    "    \"s2_tag\": tag_norm,\n",
    "    \"s2_phi\": phi_norm,\n",
    "    \"s2_phiB\": phib_norm,\n",
    "    \"s2_quality\": quality_norm,\n",
    "    \"s3_stNum\": station_norm,\n",
    "    \"s3_scNum\": sector_norm,\n",
    "    \"s3_whNum\": wheel_norm,\n",
    "    \"s3_eta_1\": eta_norm,\n",
    "    \"s3_qeta_1\": qeta_norm,\n",
    "    \"s3_eta_2\": eta_norm,\n",
    "    \"s3_qeta_2\": qeta_norm,\n",
    "    \"s3_tag\": tag_norm,\n",
    "    \"s3_phi\": phi_norm,\n",
    "    \"s3_phiB\": phib_norm,\n",
    "    \"s3_quality\": quality_norm,\n",
    "    \"s4_stNum\": station_norm,\n",
    "    \"s4_scNum\": sector_norm,\n",
    "    \"s4_whNum\": wheel_norm,\n",
    "    \"s4_eta_1\": eta_norm,\n",
    "    \"s4_qeta_1\": qeta_norm,\n",
    "    \"s4_eta_2\": eta_norm,\n",
    "    \"s4_qeta_2\": qeta_norm,\n",
    "    \"s4_tag\": tag_norm,\n",
    "    \"s4_phi\": phi_norm,\n",
    "    \"s4_phiB\": phib_norm,\n",
    "    \"s4_quality\": quality_norm,\n",
    "    # \"ptReco\": reco_pt_norm,\n",
    "    \"ptRecoInverse\": reco_pt_inverse_norm,\n",
    "    \"etaExtRecoSt2\": reco_eta_norm,\n",
    "    \"phiExtRecoSt2\": reco_phi_norm,\n",
    "    \"chargeReco\": reco_charge_norm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_features = [\n",
    "    'n_stubs', \n",
    "    's1_stNum', 's1_scNum', 's1_whNum', 's1_eta_1', 's1_qeta_1', 's1_eta_2', 's1_qeta_2', 's1_tag', 's1_phi', 's1_phiB', 's1_quality', \n",
    "    's2_stNum', 's2_scNum', 's2_whNum', 's2_eta_1', 's2_qeta_1', 's2_eta_2', 's2_qeta_2', 's2_tag', 's2_phi', 's2_phiB', 's2_quality', \n",
    "    's3_stNum', 's3_scNum', 's3_whNum', 's3_eta_1', 's3_qeta_1', 's3_eta_2', 's3_qeta_2', 's3_tag', 's3_phi', 's3_phiB', 's3_quality', \n",
    "    's4_stNum', 's4_scNum', 's4_whNum', 's4_eta_1', 's4_qeta_1', 's4_eta_2', 's4_qeta_2', 's4_tag', 's4_phi', 's4_phiB', 's4_quality'\n",
    "]\n",
    "\n",
    "target_features = [\n",
    "    'ptRecoInverse', 'etaExtRecoSt2', 'phiExtRecoSt2', 'chargeReco',\n",
    "]\n",
    "\n",
    "l1_features = [\n",
    "    'ptL1', 'etaL1', 'phiL1', 'hwSignL1',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_ = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(FILE_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_data_ = pd.concat([full_data_, pd.read_csv(FILE_PATH + file)], ignore_index=True)\n",
    "        \n",
    "full_data_ = full_data_.iloc[:, :-1]\n",
    "\n",
    "# drop rows with ptL1 == 4.5\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] != 4.5]\n",
    "\n",
    "# drop rows with reco eta > 1 and < -1\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] < 1]\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] > -1]\n",
    "\n",
    "# drop rows with ptL1 > 50\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] < 256]\n",
    "\n",
    "# drop rows with ptReco > 50\n",
    "full_data_ = full_data_[full_data_[\"ptReco\"] < 256]\n",
    "\n",
    "\n",
    "# mask_1 = (full_data_.etaL1 == 0) & (np.abs(full_data_.etaExtRecoSt2) == 0)\n",
    "# mask_2 = (full_data_.etaL1 != 0)\n",
    "\n",
    "# mask = mask_1 | mask_2\n",
    "\n",
    "# full_data_ = full_data_[mask]\n",
    "\n",
    "full_data_[\"ptRecoInverse\"] = 1 / full_data_[\"ptReco\"]\n",
    "\n",
    "full_data = full_data_[stub_features + target_features + l1_features]\n",
    "\n",
    "full_data[\"chargeReco\"] = full_data[\"chargeReco\"].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_stubs</th>\n",
       "      <th>s1_stNum</th>\n",
       "      <th>s1_scNum</th>\n",
       "      <th>s1_whNum</th>\n",
       "      <th>s1_eta_1</th>\n",
       "      <th>s1_qeta_1</th>\n",
       "      <th>s1_eta_2</th>\n",
       "      <th>s1_qeta_2</th>\n",
       "      <th>s1_tag</th>\n",
       "      <th>s1_phi</th>\n",
       "      <th>...</th>\n",
       "      <th>s4_phiB</th>\n",
       "      <th>s4_quality</th>\n",
       "      <th>ptRecoInverse</th>\n",
       "      <th>etaExtRecoSt2</th>\n",
       "      <th>phiExtRecoSt2</th>\n",
       "      <th>chargeReco</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>hwSignL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-52</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041279</td>\n",
       "      <td>-0.555335</td>\n",
       "      <td>-1.409220</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.554625</td>\n",
       "      <td>-1.418080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.608174</td>\n",
       "      <td>2.628690</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>2.628900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-14</td>\n",
       "      <td>2</td>\n",
       "      <td>-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>722</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>-0.185071</td>\n",
       "      <td>1.230710</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.184875</td>\n",
       "      <td>1.232640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-46</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.058421</td>\n",
       "      <td>-0.485899</td>\n",
       "      <td>-1.515270</td>\n",
       "      <td>0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>-0.467625</td>\n",
       "      <td>-1.527160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1202</td>\n",
       "      <td>...</td>\n",
       "      <td>-18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.037988</td>\n",
       "      <td>0.738465</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>0.818123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426409</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1116</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>-2.863660</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>-2.868880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426410</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>908</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.790361</td>\n",
       "      <td>-1.350280</td>\n",
       "      <td>1</td>\n",
       "      <td>255.5</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>-1.352630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426411</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-232</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.052273</td>\n",
       "      <td>-0.436209</td>\n",
       "      <td>-2.690840</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.424125</td>\n",
       "      <td>-2.694350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426412</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-232</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.075172</td>\n",
       "      <td>-0.440287</td>\n",
       "      <td>-2.716570</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.424125</td>\n",
       "      <td>-2.694350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426413</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>646</td>\n",
       "      <td>...</td>\n",
       "      <td>-8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.132354</td>\n",
       "      <td>0.306403</td>\n",
       "      <td>2.773520</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.315375</td>\n",
       "      <td>2.770710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2925441 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_stubs  s1_stNum  s1_scNum  s1_whNum  s1_eta_1  s1_qeta_1  s1_eta_2  \\\n",
       "0              4         1         9        -1       -52          2       255   \n",
       "2              3         1         5         1         7          0         7   \n",
       "3              2         1         2         0       -14          2       -21   \n",
       "4              4         1         9        -1       -46          2       255   \n",
       "5              3         1         1         1         7          0         7   \n",
       "...          ...       ...       ...       ...       ...        ...       ...   \n",
       "3426409        3         1         6         1         7          0         7   \n",
       "3426410        2         3         9         2        74          2       255   \n",
       "3426411        4         1         7        -1       -40          2       255   \n",
       "3426412        4         1         7        -1       -40          2       255   \n",
       "3426413        3         2         5         1         7          0         7   \n",
       "\n",
       "         s1_qeta_2  s1_tag  s1_phi  ...  s4_phiB  s4_quality  ptRecoInverse  \\\n",
       "0                0       1     632  ...        7           5       0.041279   \n",
       "2                0       1      10  ...       12           2       0.052357   \n",
       "3                1       1     722  ...       18           5       0.042819   \n",
       "4                0       1     140  ...        9           6       0.058421   \n",
       "5                0       1    1202  ...      -18           3       0.037988   \n",
       "...            ...     ...     ...  ...      ...         ...            ...   \n",
       "3426409          0       1    1116  ...        6           5       0.027158   \n",
       "3426410          0       1     908  ...       -9           2       0.018035   \n",
       "3426411          0       1    -232  ...       -9           6       0.052273   \n",
       "3426412          0       1    -232  ...       -9           6       0.075172   \n",
       "3426413          0       1     646  ...       -8           6       0.132354   \n",
       "\n",
       "         etaExtRecoSt2  phiExtRecoSt2  chargeReco   ptL1     etaL1     phiL1  \\\n",
       "0            -0.555335      -1.409220           0   29.0 -0.554625 -1.418080   \n",
       "2             0.608174       2.628690           0   17.5  0.598125  2.628900   \n",
       "3            -0.185071       1.230710           0   30.0 -0.184875  1.232640   \n",
       "4            -0.485899      -1.515270           0   20.5 -0.467625 -1.527160   \n",
       "5             0.738465       0.816011           0   31.0  0.750375  0.818123   \n",
       "...                ...            ...         ...    ...       ...       ...   \n",
       "3426409       0.750462      -2.863660           0   59.0  0.739500 -2.868880   \n",
       "3426410       0.790361      -1.350280           1  255.5  0.804750 -1.352630   \n",
       "3426411      -0.436209      -2.690840           1   22.0 -0.424125 -2.694350   \n",
       "3426412      -0.440287      -2.716570           1   22.0 -0.424125 -2.694350   \n",
       "3426413       0.306403       2.773520           0    8.5  0.315375  2.770710   \n",
       "\n",
       "         hwSignL1  \n",
       "0               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "5               1  \n",
       "...           ...  \n",
       "3426409         1  \n",
       "3426410         0  \n",
       "3426411         0  \n",
       "3426412         0  \n",
       "3426413         1  \n",
       "\n",
       "[2925441 rows x 53 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(full_data,      test_size=0.3, random_state=42)\n",
    "train_data, val_data      = train_test_split(train_val_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 1843027\n",
      "Val dataset: 204781\n",
      "Test dataset: 877633\n"
     ]
    }
   ],
   "source": [
    "# inspect the datasets\n",
    "print(f\"Train dataset: {len(train_data)}\")\n",
    "print(f\"Val dataset: {len(val_data)}\")\n",
    "print(f\"Test dataset: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "for key in normalizations.keys():\n",
    "    train_data[key] = train_data[key] / normalizations[key]\n",
    "    val_data[key]   = val_data[key]   / normalizations[key]\n",
    "    test_data[key]  = test_data[key]  / normalizations[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 16:42:28.572028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10266 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2024-07-03 16:42:28.572610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10396 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size    = len(stub_features)\n",
    "architecture  = [input_size, 64, 32, 16, 8]\n",
    "output_size   = len(target_features)\n",
    "learning_rate = 1e-2\n",
    "num_epochs    = 300\n",
    "batch_size    = 2**8\n",
    "reg_strength  = 1e-3\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor = 0.5\n",
    "patience = 5\n",
    "min_loss_improvement = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "patience = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "classification_weight = 1\n",
    "regression_weight     = 6\n",
    "\n",
    "# Loss and optimizer\n",
    "# regression_criterion     = losses.MeanSquaredError()\n",
    "classification_criterion = losses.BinaryCrossentropy(from_logits=True)\n",
    "regression_criterion     = losses.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "scheduler = CustomLRScheduler(\n",
    "    optimizer, \n",
    "    factor=scale_factor, \n",
    "    patience=patience, \n",
    "    min_improvement=min_loss_improvement, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data[stub_features].values, train_data[target_features].values)).batch(batch_size).shuffle(buffer_size=len(train_data))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((val_data[stub_features].values, val_data[target_features].values)).batch(batch_size)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((test_data[stub_features].values, test_data[target_features].values)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5724\n",
      "Number of trainable parameters: 5724\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = MultiTaskNN(architecture, reg_strength=reg_strength)\n",
    "\n",
    "# Build the model with the batch input shape\n",
    "bs = None  # None allows for variable batch size\n",
    "model.build((bs, input_size))\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params = model.count_params()\n",
    "trainable_vars = [var for var in model.trainable_variables]\n",
    "trainable_params = sum([tf.size(var).numpy() for var in trainable_vars])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=True)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return regression_loss, classification_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(features, targets):\n",
    "    regression_targets = targets[:, :-1]\n",
    "    classification_targets = targets[:, -1]\n",
    "    reg_outputs, class_outputs = model(features, training=False)\n",
    "    regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "    classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "    loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "    return regression_loss, classification_loss, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 16:42:35.416302: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f6c3587ae80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-03 16:42:35.416346: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-07-03 16:42:35.416355: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-07-03 16:42:35.422986: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-03 16:42:35.444448: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720017755.518500 2889189 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0274, Classification: 0.1429, Combined: 0.3076\n",
      "Validation Losses - Regression: 0.0176, Classification: 0.1327, Combined: 0.2381\n",
      "-------------\n",
      "Epoch [2/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0163, Classification: 0.1274, Combined: 0.2250\n",
      "Validation Losses - Regression: 0.0150, Classification: 0.1245, Combined: 0.2147\n",
      "-------------\n",
      "Epoch [3/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0154, Classification: 0.1233, Combined: 0.2155\n",
      "Validation Losses - Regression: 0.0173, Classification: 0.1280, Combined: 0.2316\n",
      "-------------\n",
      "Epoch [4/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0151, Classification: 0.1215, Combined: 0.2121\n",
      "Validation Losses - Regression: 0.0144, Classification: 0.1260, Combined: 0.2125\n",
      "-------------\n",
      "Epoch [5/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0149, Classification: 0.1209, Combined: 0.2103\n",
      "Validation Losses - Regression: 0.0147, Classification: 0.1203, Combined: 0.2087\n",
      "-------------\n",
      "Epoch [6/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0148, Classification: 0.1202, Combined: 0.2090\n",
      "Validation Losses - Regression: 0.0137, Classification: 0.1248, Combined: 0.2067\n",
      "-------------\n",
      "Epoch [7/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0146, Classification: 0.1197, Combined: 0.2076\n",
      "Validation Losses - Regression: 0.0135, Classification: 0.1166, Combined: 0.1974\n",
      "-------------\n",
      "Epoch [8/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0146, Classification: 0.1193, Combined: 0.2068\n",
      "Validation Losses - Regression: 0.0160, Classification: 0.1191, Combined: 0.2153\n",
      "-------------\n",
      "Epoch [9/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0144, Classification: 0.1189, Combined: 0.2053\n",
      "Validation Losses - Regression: 0.0144, Classification: 0.1195, Combined: 0.2057\n",
      "-------------\n",
      "Epoch [10/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0144, Classification: 0.1187, Combined: 0.2050\n",
      "Validation Losses - Regression: 0.0149, Classification: 0.1171, Combined: 0.2068\n",
      "-------------\n",
      "Epoch [11/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0143, Classification: 0.1187, Combined: 0.2043\n",
      "Validation Losses - Regression: 0.0136, Classification: 0.1202, Combined: 0.2015\n",
      "-------------\n",
      "Epoch [12/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0143, Classification: 0.1185, Combined: 0.2045\n",
      "Validation Losses - Regression: 0.0140, Classification: 0.1205, Combined: 0.2048\n",
      "-------------\n",
      "Epoch [13/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0142, Classification: 0.1181, Combined: 0.2036\n",
      "Validation Losses - Regression: 0.0139, Classification: 0.1223, Combined: 0.2058\n",
      "-------------\n",
      "Epoch [14/300]\n",
      "Learning rate: 1.00e-02\n",
      "Train Losses - Regression: 0.0142, Classification: 0.1181, Combined: 0.2033\n",
      "Validation Losses - Regression: 0.0151, Classification: 0.1174, Combined: 0.2081\n",
      "-------------\n",
      "Decreasing learning rate to 0.004999999888241291\n",
      "Epoch [15/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0123, Classification: 0.1135, Combined: 0.1874\n",
      "Validation Losses - Regression: 0.0121, Classification: 0.1176, Combined: 0.1903\n",
      "-------------\n",
      "Epoch [16/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0123, Classification: 0.1132, Combined: 0.1867\n",
      "Validation Losses - Regression: 0.0123, Classification: 0.1132, Combined: 0.1867\n",
      "-------------\n",
      "Epoch [17/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0123, Classification: 0.1128, Combined: 0.1863\n",
      "Validation Losses - Regression: 0.0120, Classification: 0.1136, Combined: 0.1856\n",
      "-------------\n",
      "Epoch [18/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1126, Combined: 0.1860\n",
      "Validation Losses - Regression: 0.0120, Classification: 0.1125, Combined: 0.1848\n",
      "-------------\n",
      "Epoch [19/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1121, Combined: 0.1856\n",
      "Validation Losses - Regression: 0.0118, Classification: 0.1106, Combined: 0.1816\n",
      "-------------\n",
      "Epoch [20/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1118, Combined: 0.1852\n",
      "Validation Losses - Regression: 0.0123, Classification: 0.1130, Combined: 0.1868\n",
      "-------------\n",
      "Epoch [21/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1116, Combined: 0.1848\n",
      "Validation Losses - Regression: 0.0120, Classification: 0.1144, Combined: 0.1862\n",
      "-------------\n",
      "Epoch [22/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1112, Combined: 0.1845\n",
      "Validation Losses - Regression: 0.0119, Classification: 0.1121, Combined: 0.1837\n",
      "-------------\n",
      "Epoch [23/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1109, Combined: 0.1841\n",
      "Validation Losses - Regression: 0.0120, Classification: 0.1113, Combined: 0.1831\n",
      "-------------\n",
      "Epoch [24/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1108, Combined: 0.1841\n",
      "Validation Losses - Regression: 0.0120, Classification: 0.1114, Combined: 0.1832\n",
      "-------------\n",
      "Epoch [25/300]\n",
      "Learning rate: 5.00e-03\n",
      "Train Losses - Regression: 0.0122, Classification: 0.1106, Combined: 0.1837\n",
      "Validation Losses - Regression: 0.0124, Classification: 0.1102, Combined: 0.1848\n",
      "-------------\n",
      "Decreasing learning rate to 0.0024999999441206455\n",
      "Epoch [26/300]\n",
      "Learning rate: 2.50e-03\n",
      "Train Losses - Regression: 0.0113, Classification: 0.1078, Combined: 0.1758\n",
      "Validation Losses - Regression: 0.0111, Classification: 0.1093, Combined: 0.1760\n",
      "-------------\n",
      "Decreasing learning rate to 0.0012499999720603228\n",
      "Epoch [27/300]\n",
      "Learning rate: 1.25e-03\n",
      "Train Losses - Regression: 0.0110, Classification: 0.1062, Combined: 0.1719\n",
      "Validation Losses - Regression: 0.0109, Classification: 0.1068, Combined: 0.1722\n",
      "-------------\n",
      "Decreasing learning rate to 0.0006249999860301614\n",
      "Epoch [28/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1052, Combined: 0.1697\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1063, Combined: 0.1703\n",
      "-------------\n",
      "Epoch [29/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1050, Combined: 0.1694\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1060, Combined: 0.1703\n",
      "-------------\n",
      "Epoch [30/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1050, Combined: 0.1693\n",
      "Validation Losses - Regression: 0.0108, Classification: 0.1061, Combined: 0.1709\n",
      "-------------\n",
      "Epoch [31/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1048, Combined: 0.1691\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1057, Combined: 0.1696\n",
      "-------------\n",
      "Epoch [32/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1047, Combined: 0.1690\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1061, Combined: 0.1702\n",
      "-------------\n",
      "Epoch [33/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1046, Combined: 0.1688\n",
      "Validation Losses - Regression: 0.0108, Classification: 0.1059, Combined: 0.1706\n",
      "-------------\n",
      "Epoch [34/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1046, Combined: 0.1687\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1061, Combined: 0.1703\n",
      "-------------\n",
      "Epoch [35/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1045, Combined: 0.1686\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1054, Combined: 0.1694\n",
      "-------------\n",
      "Epoch [36/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1044, Combined: 0.1685\n",
      "Validation Losses - Regression: 0.0107, Classification: 0.1054, Combined: 0.1699\n",
      "-------------\n",
      "Epoch [37/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1044, Combined: 0.1684\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1052, Combined: 0.1691\n",
      "-------------\n",
      "Epoch [38/300]\n",
      "Learning rate: 6.25e-04\n",
      "Train Losses - Regression: 0.0107, Classification: 0.1043, Combined: 0.1684\n",
      "Validation Losses - Regression: 0.0108, Classification: 0.1055, Combined: 0.1700\n",
      "-------------\n",
      "Decreasing learning rate to 0.0003124999930150807\n",
      "Epoch [39/300]\n",
      "Learning rate: 3.12e-04\n",
      "Train Losses - Regression: 0.0106, Classification: 0.1039, Combined: 0.1673\n",
      "Validation Losses - Regression: 0.0106, Classification: 0.1051, Combined: 0.1684\n",
      "-------------\n",
      "Decreasing learning rate to 0.00015624999650754035\n",
      "Epoch [40/300]\n",
      "Learning rate: 1.56e-04\n",
      "Train Losses - Regression: 0.0105, Classification: 0.1036, Combined: 0.1667\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1048, Combined: 0.1679\n",
      "-------------\n",
      "Decreasing learning rate to 7.812499825377017e-05\n",
      "Epoch [41/300]\n",
      "Learning rate: 7.81e-05\n",
      "Train Losses - Regression: 0.0105, Classification: 0.1035, Combined: 0.1664\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1048, Combined: 0.1677\n",
      "-------------\n",
      "Decreasing learning rate to 3.9062499126885086e-05\n",
      "Epoch [42/300]\n",
      "Learning rate: 3.91e-05\n",
      "Train Losses - Regression: 0.0105, Classification: 0.1034, Combined: 0.1662\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1676\n",
      "-------------\n",
      "Decreasing learning rate to 1.9531249563442543e-05\n",
      "Epoch [43/300]\n",
      "Learning rate: 1.95e-05\n",
      "Train Losses - Regression: 0.0105, Classification: 0.1034, Combined: 0.1661\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1676\n",
      "-------------\n",
      "Decreasing learning rate to 9.765624781721272e-06\n",
      "Epoch [44/300]\n",
      "Learning rate: 9.77e-06\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1034, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 4.882812390860636e-06\n",
      "Epoch [45/300]\n",
      "Learning rate: 4.88e-06\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1034, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 2.441406195430318e-06\n",
      "Epoch [46/300]\n",
      "Learning rate: 2.44e-06\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 1.220703097715159e-06\n",
      "Epoch [47/300]\n",
      "Learning rate: 1.22e-06\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 6.103515488575795e-07\n",
      "Epoch [48/300]\n",
      "Learning rate: 6.10e-07\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 3.0517577442878974e-07\n",
      "Epoch [49/300]\n",
      "Learning rate: 3.05e-07\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 1.5258788721439487e-07\n",
      "Epoch [50/300]\n",
      "Learning rate: 1.53e-07\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 7.629394360719743e-08\n",
      "Epoch [51/300]\n",
      "Learning rate: 7.63e-08\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 3.814697180359872e-08\n",
      "Epoch [52/300]\n",
      "Learning rate: 3.81e-08\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 1.907348590179936e-08\n",
      "Epoch [53/300]\n",
      "Learning rate: 1.91e-08\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 9.53674295089968e-09\n",
      "Epoch [54/300]\n",
      "Learning rate: 9.54e-09\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 4.76837147544984e-09\n",
      "Epoch [55/300]\n",
      "Learning rate: 4.77e-09\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 2.38418573772492e-09\n",
      "Epoch [56/300]\n",
      "Learning rate: 2.38e-09\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 1.19209286886246e-09\n",
      "Epoch [57/300]\n",
      "Learning rate: 1.19e-09\n",
      "Train Losses - Regression: 0.0104, Classification: 0.1033, Combined: 0.1660\n",
      "Validation Losses - Regression: 0.0105, Classification: 0.1047, Combined: 0.1675\n",
      "-------------\n",
      "Decreasing learning rate to 5.9604643443123e-10\n",
      "Early stopping triggered after 57 epochs!\n"
     ]
    }
   ],
   "source": [
    "train_regression_losses     = []\n",
    "train_classification_losses = []\n",
    "train_combined_losses       = []\n",
    "val_regression_losses       = []\n",
    "val_classification_losses   = []\n",
    "val_combined_losses         = []\n",
    "\n",
    "learning_rates = []\n",
    "\n",
    "out_file = open(os.path.join(OUT_PATH, LOSS_FNAME), \"w\")\n",
    "out_file.write(\"train_regression_loss,train_classification_loss,train_combined_loss,val_regression_loss,val_classification_loss,val_combined_loss,learning_rate\\n\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    running_loss                = 0.0\n",
    "    running_regression_loss     = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    # scheduler.on_epoch_begin(epoch, logs=None)\n",
    "    \n",
    "    for features, targets in train_dataset:\n",
    "        \n",
    "        regression_loss, classification_loss, loss = train_step(features, targets)\n",
    "        \n",
    "        running_loss                += loss.numpy()\n",
    "        running_regression_loss     += regression_loss.numpy()\n",
    "        running_classification_loss += classification_loss.numpy()\n",
    "        \n",
    "        # scheduler.on_batch_end(batch=None)\n",
    "        \n",
    "        # current_lr = optimizer.lr.numpy()\n",
    "        \n",
    "        # lr_per_batch.append(current_lr)\n",
    "        # print(f\"Learning rate: {current_lr:.2e}\", end=\"\\r\")\n",
    "        \n",
    "\n",
    "        \n",
    "    # Note: Remember to adjust if not using batches of equal sizes\n",
    "    train_loss                = running_loss                / len(train_dataset)\n",
    "    train_regression_loss     = running_regression_loss     / len(train_dataset)\n",
    "    train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "\n",
    "    train_regression_losses.append(train_regression_loss)\n",
    "    train_classification_losses.append(train_classification_loss)\n",
    "    train_combined_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    running_loss                = 0.0\n",
    "    running_regression_loss     = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    for features, targets in val_dataset:\n",
    "        \n",
    "        regression_loss, classification_loss, loss = val_step(features, targets)\n",
    "        \n",
    "        running_loss                += loss.numpy()\n",
    "        running_regression_loss     += regression_loss.numpy()\n",
    "        running_classification_loss += classification_loss.numpy()\n",
    "    \n",
    "    avg_val_loss                = running_loss                / len(val_dataset)\n",
    "    avg_val_regression_loss     = running_regression_loss     / len(val_dataset)\n",
    "    avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "        \n",
    "    val_regression_losses.append(avg_val_regression_loss)\n",
    "    val_classification_losses.append(avg_val_classification_loss)\n",
    "    val_combined_losses.append(avg_val_loss)\n",
    "    \n",
    "    \n",
    "    current_lr = optimizer.lr.numpy()\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Learning rate: {current_lr:.2e}\")\n",
    "    print(f\"Train Losses - Regression: {train_regression_losses[-1]:.4f}, Classification: {train_classification_losses[-1]:.4f}, Combined: {train_combined_losses[-1]:.4f}\")\n",
    "    print(f\"Validation Losses - Regression: {val_regression_losses[-1]:.4f}, Classification: {val_classification_losses[-1]:.4f}, Combined: {val_combined_losses[-1]:.4f}\")\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    \n",
    "    scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_loss})\n",
    "\n",
    "\n",
    "    with open(os.path.join(OUT_PATH, LOSS_FNAME), \"a\") as output_file:\n",
    "        output_file.write(f\"{train_regression_losses[-1]},{train_classification_losses[-1]},{train_combined_losses[-1]},{val_regression_losses[-1]},{val_classification_losses[-1]},{val_combined_losses[-1]},{current_lr}\\n\")\n",
    "\n",
    "    # Check for early stopping based on the new criterion\n",
    "    if avg_val_loss < (1 - early_stopping_threshold) * best_val_loss:  # 0.001 corresponds to 0.1%\n",
    "        epochs_without_improvement = 0\n",
    "        best_val_loss = min(best_val_loss, avg_val_loss)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test data from `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptReco_true</th>\n",
       "      <th>etaReco_true</th>\n",
       "      <th>phiReco_true</th>\n",
       "      <th>chargeReco_true</th>\n",
       "      <th>ptReco_pred</th>\n",
       "      <th>etaReco_pred</th>\n",
       "      <th>phiReco_pred</th>\n",
       "      <th>chargeReco_pred</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>chargeL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.14094</td>\n",
       "      <td>0.725395</td>\n",
       "      <td>1.863000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.845138</td>\n",
       "      <td>0.742052</td>\n",
       "      <td>1.876867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>1.887140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.94190</td>\n",
       "      <td>-0.825676</td>\n",
       "      <td>-2.726760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.148404</td>\n",
       "      <td>-0.828740</td>\n",
       "      <td>-2.722502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.804750</td>\n",
       "      <td>-2.727080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.79640</td>\n",
       "      <td>0.143982</td>\n",
       "      <td>-0.391065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.846815</td>\n",
       "      <td>0.145370</td>\n",
       "      <td>-0.400197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.5</td>\n",
       "      <td>0.141375</td>\n",
       "      <td>-0.414516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.03724</td>\n",
       "      <td>0.803833</td>\n",
       "      <td>1.414660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.492181</td>\n",
       "      <td>0.817047</td>\n",
       "      <td>1.398779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.804750</td>\n",
       "      <td>1.396260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.51210</td>\n",
       "      <td>-0.508647</td>\n",
       "      <td>-2.192450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.836945</td>\n",
       "      <td>-0.502010</td>\n",
       "      <td>-2.190900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.478500</td>\n",
       "      <td>-2.192570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877628</th>\n",
       "      <td>4.12733</td>\n",
       "      <td>-0.517609</td>\n",
       "      <td>-0.834530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.313579</td>\n",
       "      <td>-0.514054</td>\n",
       "      <td>-0.774689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-0.500250</td>\n",
       "      <td>-0.818123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877629</th>\n",
       "      <td>23.83930</td>\n",
       "      <td>0.883597</td>\n",
       "      <td>1.309400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.070823</td>\n",
       "      <td>0.876216</td>\n",
       "      <td>1.311607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>1.309000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877630</th>\n",
       "      <td>6.30444</td>\n",
       "      <td>0.539451</td>\n",
       "      <td>2.764210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.929778</td>\n",
       "      <td>0.508884</td>\n",
       "      <td>2.758275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.500250</td>\n",
       "      <td>2.759800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877631</th>\n",
       "      <td>34.38710</td>\n",
       "      <td>-0.417246</td>\n",
       "      <td>3.093800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.442506</td>\n",
       "      <td>-0.502704</td>\n",
       "      <td>3.086110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.511125</td>\n",
       "      <td>3.087050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877632</th>\n",
       "      <td>9.41212</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>2.661640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.530159</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>2.663181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.661630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>877633 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ptReco_true  etaReco_true  phiReco_true  chargeReco_true  ptReco_pred  \\\n",
       "0           7.14094      0.725395      1.863000              0.0     6.845138   \n",
       "1          22.94190     -0.825676     -2.726760              0.0    20.148404   \n",
       "2          77.79640      0.143982     -0.391065              0.0    81.846815   \n",
       "3           7.03724      0.803833      1.414660              0.0     7.492181   \n",
       "4          36.51210     -0.508647     -2.192450              1.0    33.836945   \n",
       "...             ...           ...           ...              ...          ...   \n",
       "877628      4.12733     -0.517609     -0.834530              1.0     5.313579   \n",
       "877629     23.83930      0.883597      1.309400              1.0    22.070823   \n",
       "877630      6.30444      0.539451      2.764210              0.0     5.929778   \n",
       "877631     34.38710     -0.417246      3.093800              0.0    16.442506   \n",
       "877632      9.41212     -0.005519      2.661640              1.0     9.530159   \n",
       "\n",
       "        etaReco_pred  phiReco_pred  chargeReco_pred  ptL1     etaL1     phiL1  \\\n",
       "0           0.742052      1.876867              0.0   9.0  0.750375  1.887140   \n",
       "1          -0.828740     -2.722502              0.0  27.0 -0.804750 -2.727080   \n",
       "2           0.145370     -0.400197              0.0  93.5  0.141375 -0.414516   \n",
       "3           0.817047      1.398779              0.0   9.5  0.804750  1.396260   \n",
       "4          -0.502010     -2.190900              1.0  41.0 -0.478500 -2.192570   \n",
       "...              ...           ...              ...   ...       ...       ...   \n",
       "877628     -0.514054     -0.774689              1.0   8.5 -0.500250 -0.818123   \n",
       "877629      0.876216      1.311607              1.0  29.0  0.880875  1.309000   \n",
       "877630      0.508884      2.758275              0.0   8.5  0.500250  2.759800   \n",
       "877631     -0.502704      3.086110              1.0  15.0 -0.511125  3.087050   \n",
       "877632     -0.004291      2.663181              1.0  10.0  0.000000  2.661630   \n",
       "\n",
       "        chargeL1  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "877628         1  \n",
       "877629         1  \n",
       "877630         0  \n",
       "877631         1  \n",
       "877632         1  \n",
       "\n",
       "[877633 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = test_data[stub_features].values\n",
    "test_targets  = test_data[target_features].values\n",
    "\n",
    "reg_predictions, class_predictions = model(test_features, training=False)\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            test_features,\n",
    "            test_targets,\n",
    "            reg_predictions.numpy(),\n",
    "            class_predictions.numpy(),\n",
    "            test_data[l1_features].values,\n",
    "        ),\n",
    "        axis=1\n",
    "    ),\n",
    "    columns=stub_features + target_features + [\"ptRecoInverse_pred\", \"etaExtRecoSt2_pred\", \"phiExtRecoSt2_pred\", \"chargeReco_pred\"] + l1_features\n",
    ")\n",
    "\n",
    "test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 1 / (1 + np.exp(-x)))\n",
    "test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "# rescale the features\n",
    "for key in normalizations.keys():\n",
    "    test_df[key] = test_df[key] * normalizations[key]\n",
    "    \n",
    "# rescale the predictions\n",
    "test_df[\"ptRecoInverse_pred\"] = test_df[\"ptRecoInverse_pred\"] * normalizations[\"ptRecoInverse\"]\n",
    "test_df[\"etaExtRecoSt2_pred\"] = test_df[\"etaExtRecoSt2_pred\"] * normalizations[\"etaExtRecoSt2\"]\n",
    "test_df[\"phiExtRecoSt2_pred\"] = test_df[\"phiExtRecoSt2_pred\"] * normalizations[\"phiExtRecoSt2\"]\n",
    "test_df[\"chargeReco_pred\"]    = test_df[\"chargeReco_pred\"]    * normalizations[\"chargeReco\"]\n",
    "\n",
    "# drop features keep only predictions and targets and L1 features\n",
    "test_df = test_df[[\"ptRecoInverse\", \"etaExtRecoSt2\", \"phiExtRecoSt2\", \"chargeReco\", \"ptRecoInverse_pred\", \"etaExtRecoSt2_pred\", \"phiExtRecoSt2_pred\", \"chargeReco_pred\"] + l1_features]\n",
    "\n",
    "# create ptReco_true and etaReco_true and phiReco_true and chargeReco_true\n",
    "# create ptReco_pred and etaReco_pred and phiReco_pred and chargeReco_pred\n",
    "test_df[\"ptReco_true\"]     = test_df[\"ptRecoInverse\"].apply(lambda x: 1 / x)\n",
    "test_df[\"etaReco_true\"]    = test_df[\"etaExtRecoSt2\"]\n",
    "test_df[\"phiReco_true\"]    = test_df[\"phiExtRecoSt2\"]\n",
    "test_df[\"chargeReco_true\"] = test_df[\"chargeReco\"]\n",
    "\n",
    "test_df[\"ptReco_pred\"]     = test_df[\"ptRecoInverse_pred\"].apply(lambda x: 1 / x)\n",
    "test_df[\"etaReco_pred\"]    = test_df[\"etaExtRecoSt2_pred\"]\n",
    "test_df[\"phiReco_pred\"]    = test_df[\"phiExtRecoSt2_pred\"]\n",
    "test_df[\"chargeReco_pred\"] = test_df[\"chargeReco_pred\"]\n",
    "\n",
    "# transform hwSignL1 into chargeL1\n",
    "test_df[\"chargeL1\"] = test_df[\"hwSignL1\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# drop ptRecoInverse and etaExtRecoSt2 and phiExtRecoSt2 and chargeReco\n",
    "# drop ptRecoInverse_pred and etaExtRecoSt2_pred and phiExtRecoSt2_pred and chargeReco_pred\n",
    "test_df = test_df[[\"ptReco_true\", \"etaReco_true\", \"phiReco_true\", \"chargeReco_true\", \"ptReco_pred\", \"etaReco_pred\", \"phiReco_pred\", \"chargeReco_pred\", \"ptL1\", \"etaL1\", \"phiL1\", \"chargeL1\"]]\n",
    "\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Test Set: 97.21%\n",
      "L1 Classification Accuracy on Test Set: 97.28%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (test_df[\"chargeReco_pred\"] == test_df[\"chargeReco_true\"]).sum() / len(test_df)\n",
    "\n",
    "print(f\"Classification Accuracy on Test Set: {accuracy*100:.2f}%\")\n",
    "\n",
    "l1_accuracy = (test_df[\"chargeL1\"] == test_df[\"chargeReco_true\"]).sum() / len(test_df)\n",
    "\n",
    "print(f\"L1 Classification Accuracy on Test Set: {l1_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/eos/user/f/fcufino'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m PLOT_FLAG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(PLOT_PATH):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPLOT_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/eos/user/f/fcufino'"
     ]
    }
   ],
   "source": [
    "prob      = True\n",
    "PLOT_PATH = f\"/eos/user/{USER[0]}/{USER}/nnreco-plots/\"\n",
    "PLOT_FLAG = False\n",
    "\n",
    "if not os.path.exists(PLOT_PATH):\n",
    "    os.makedirs(PLOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_res_pred = (test_df.etaReco_pred - test_df.etaReco_true) / test_df.etaReco_true\n",
    "\n",
    "# if eta_res_pred < -0.5 then add 1 to shift the histogram to the right\n",
    "# mask = np.where(eta_res_pred < -0.5, 1, 0)\n",
    "# eta_res_pred = eta_res_pred + mask\n",
    "\n",
    "# recompute the predicted eta\n",
    "# test_df[\"etaReco_pred\"] = test_df.etaReco_true * (1 + eta_res_pred)\n",
    "\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_l1   = (test_df.ptL1  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_rec_l1   = (test_df.ptL1/1.2  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "\n",
    "eta_res_pred = (test_df.etaReco_pred - test_df.etaReco_true) \n",
    "eta_res_l1   = (test_df.etaL1   - test_df.etaReco_true)\n",
    "\n",
    "phi_res_pred = (test_df.phiReco_pred - test_df.phiReco_true)\n",
    "phi_res_l1   = (test_df.phiL1   - test_df.phiReco_true)\n",
    "\n",
    "\n",
    "rlabel = \"ZeroBias+MuonEG 2023 (13.6 TeV)\"\n",
    "\n",
    "xmin = -1.4\n",
    "xmax = 1.4\n",
    "binw = 0.02\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(pt_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(pt_res_rec_l1, bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1 (recal)\")\n",
    "plt.hist(pt_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta p_T / p_T$\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/pt_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "xmin = -0.19\n",
    "xmax = 0.19\n",
    "binw = 0.0025\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(eta_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(eta_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta \\eta$\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/eta_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "xmin = -0.19\n",
    "xmax = 0.19\n",
    "binw = 0.0025\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,9))\n",
    "hep.cms.label(\"Preliminary\", loc=2, data=True, year=2024, rlabel=rlabel)\n",
    "plt.hist(phi_res_l1,     bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"L1\")\n",
    "plt.hist(phi_res_pred,   bins=np.arange(xmin,xmax+binw,binw), histtype=\"step\", lw=2, density=True, label=\"NN reconstruction\")\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=3, zorder=0, alpha=0.3)\n",
    "plt.xlabel(\"$\\Delta\\\\varphi$ (rad)\")\n",
    "plt.ylabel(f\"Events (%) / {binw}\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "fig.savefig(f\"{PLOT_PATH}/phi_res_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pT resolution vs pT perd true 2D histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 50\n",
    "xbinw = 0.5\n",
    "ymin = -1.5\n",
    "ymax = 1.5\n",
    "ybinw = 0.02\n",
    "\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "heights, binsx, binsy = np.histogram2d(test_df.ptReco_true, pt_res_pred, bins=(np.arange(xmin, xmax+xbinw, xbinw), np.arange(ymin, ymax+ybinw, ybinw)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,9), constrained_layout=True)\n",
    "hep.cms.label(label=\"Preliminary\", data=True, year=2023, rlabel=rlabel)\n",
    "\n",
    "hep.hist2dplot(heights, binsx, binsy, ax=ax, cmap=\"viridis\", flow=None, norm = mpl.colors.LogNorm())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(\"Events\", labelpad=15) # , fontsize=36)\n",
    "# cbar.ax.tick_params(labelsize=36)\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Reco muon $p_T$ (GeV)\")\n",
    "plt.ylabel(\"$\\Delta p_T / p_T$\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}pt_res_vs_pt_reco_true_zoom{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 50\n",
    "binw = 0.5\n",
    "\n",
    "# Compute pT resolution\n",
    "pt_res_pred = (test_df.ptReco_pred - test_df.ptReco_true) / test_df.ptReco_true\n",
    "pt_res_l1   = (test_df.ptL1 / 1.2  - test_df.ptReco_true) / test_df.ptReco_true\n",
    "\n",
    "# Define bins for pT reco true\n",
    "pt_true_bins = np.arange(xmin, xmax+binw, binw)  # From 0 to 50 GeV with bin width 1 GeV\n",
    "\n",
    "# Initialize arrays to store average pT resolution and standard deviation in each bin\n",
    "avg_pt_res_pred = np.zeros(len(pt_true_bins) - 1)\n",
    "std_pt_res_pred = np.zeros(len(pt_true_bins) - 1)\n",
    "avg_pt_res_l1   = np.zeros(len(pt_true_bins) - 1)\n",
    "std_pt_res_l1   = np.zeros(len(pt_true_bins) - 1)\n",
    "\n",
    "\n",
    "# Loop through the bins to calculate average pT resolution and standard deviation\n",
    "for i in range(len(pt_true_bins) - 1):\n",
    "    lower_bound = pt_true_bins[i]\n",
    "    upper_bound = pt_true_bins[i + 1]\n",
    "    \n",
    "    # Mask data points that fall within the current bin\n",
    "    mask = (test_df.ptReco_true >= lower_bound) & (test_df.ptReco_true < upper_bound)\n",
    "    \n",
    "    # Compute the average pT resolution for the current bin\n",
    "    avg_pt_res_pred[i] = np.mean(pt_res_pred[mask])\n",
    "    avg_pt_res_l1[i]   = np.mean(pt_res_l1[mask])\n",
    "    \n",
    "    # Compute the standard deviation for the current bin\n",
    "    std_pt_res_pred[i] = np.std(pt_res_pred[mask]) # / np.sqrt(len(pt_res_pred[mask]))\n",
    "    std_pt_res_l1[i]   = np.std(pt_res_l1[mask])   # / np.sqrt(len(pt_res_l1[mask]))\n",
    "\n",
    "\n",
    "# Create the profile plot\n",
    "fig, ax = plt.subplots(figsize=(12,9), constrained_layout=True)\n",
    "hep.cms.label(ax=ax, data=True, year=2023, label=\"Preliminary\", rlabel=rlabel)\n",
    "plt.plot(pt_true_bins[:-1], avg_pt_res_l1, \"-\", label=\"L1\", color=\"C0\")\n",
    "plt.fill_between(pt_true_bins[:-1], avg_pt_res_l1 - std_pt_res_l1, avg_pt_res_l1 + std_pt_res_l1, alpha=0.3, color=\"C0\")\n",
    "\n",
    "plt.plot(pt_true_bins[:-1], avg_pt_res_pred, \"-\", label=\"NN\", color=\"C1\")\n",
    "plt.fill_between(pt_true_bins[:-1], avg_pt_res_pred - std_pt_res_pred, avg_pt_res_pred + std_pt_res_pred, alpha=0.3, color=\"C1\")\n",
    "\n",
    "ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=2, zorder=0)\n",
    "\n",
    "# Plotting the average pT resolution with error bars\n",
    "\n",
    "# ax.errorbar(\n",
    "#     x=pt_true_bins[:-1], \n",
    "#     y=avg_pt_res_l1, \n",
    "#     yerr=std_pt_res_l1, \n",
    "#     marker='o', \n",
    "#     linestyle=\"\",\n",
    "#     markersize=12,\n",
    "#     elinewidth=3,\n",
    "#     capthick=3,\n",
    "#     capsize=5,\n",
    "#     label=\"L1\",\n",
    "#     color=PALETTE[-1],\n",
    "# )\n",
    "\n",
    "\n",
    "# ax.errorbar(\n",
    "#     x=pt_true_bins[:-1], \n",
    "#     y=avg_pt_res_pred, \n",
    "#     yerr=std_pt_res_pred, \n",
    "#     marker='o', \n",
    "#     linestyle=\"\",\n",
    "#     markersize=12,\n",
    "#     elinewidth=3,\n",
    "#     capthick=3,\n",
    "#     capsize=5,\n",
    "#     label=\"NN Prediction\",\n",
    "#     color=PALETTE[0],\n",
    "# )\n",
    "\n",
    "\n",
    "# ax.legend(fontsize=36, loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Reco muon $p_T$ [GeV]\")\n",
    "plt.ylabel(\"$\\Delta p_T / p_T$\")\n",
    "\n",
    "# ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}pt_res_vs_pt_reco_true_profile{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.xlim(5,xmax)\n",
    "plt.ylim(-2,2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_regression_losses, label=\"Train\")\n",
    "ax.plot(val_regression_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Regression Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_classification_losses, label=\"Train\")\n",
    "ax.plot(val_classification_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(train_combined_losses, label=\"Train\")\n",
    "ax.plot(val_combined_losses, label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Combined Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(learning_rates, lw=3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}learning_rate{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "\n",
    "ax.plot(learning_rates, lw=3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}log_learning_rate{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform multiple trainings to compute average loss (TAKES A LOT OF TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=True)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return regression_loss, classification_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(features, targets):\n",
    "    regression_targets = targets[:, :-1]\n",
    "    classification_targets = targets[:, -1]\n",
    "    reg_outputs, class_outputs = model(features, training=False)\n",
    "    regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "    classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "    loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "    return regression_loss, classification_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_ = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(FILE_PATH):\n",
    "    if file.endswith(\".csv\"):\n",
    "        full_data_ = pd.concat([full_data_, pd.read_csv(FILE_PATH + file)], ignore_index=True)\n",
    "        \n",
    "full_data_ = full_data_.iloc[:, :-1]\n",
    "\n",
    "# drop rows with ptL1 == 4.5\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] != 4.5]\n",
    "\n",
    "# drop rows with reco eta > 1 and < -1\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] < 1]\n",
    "full_data_ = full_data_[full_data_[\"etaExtRecoSt2\"] > -1]\n",
    "\n",
    "# drop rows with ptL1 > 50\n",
    "full_data_ = full_data_[full_data_[\"ptL1\"] < 50]\n",
    "\n",
    "# drop rows with ptReco > 50\n",
    "full_data_ = full_data_[full_data_[\"ptReco\"] < 50]\n",
    "\n",
    "\n",
    "# mask_1 = (full_data_.etaL1 == 0) & (np.abs(full_data_.etaExtRecoSt2) == 0)\n",
    "# mask_2 = (full_data_.etaL1 != 0)\n",
    "\n",
    "# mask = mask_1 | mask_2\n",
    "\n",
    "# full_data_ = full_data_[mask]\n",
    "\n",
    "full_data_[\"ptRecoInverse\"] = 1 / full_data_[\"ptReco\"]\n",
    "\n",
    "full_data = full_data_[stub_features + target_features + l1_features]\n",
    "\n",
    "full_data[\"chargeReco\"] = full_data[\"chargeReco\"].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_criterion = losses.BinaryCrossentropy(from_logits=True)\n",
    "regression_criterion     = losses.MeanAbsoluteError()\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor = 0.5\n",
    "patience = 5\n",
    "min_loss_improvement = 0.1\n",
    "\n",
    "classification_weight = 1\n",
    "regression_weight     = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regression_losses_all     = []\n",
    "train_classification_losses_all = []\n",
    "train_combined_losses_all       = []\n",
    "val_regression_losses_all       = []\n",
    "val_classification_losses_all   = []\n",
    "val_combined_losses_all         = []\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    input_size    = len(stub_features)\n",
    "    architecture  = [input_size, 64, 32, 16, 8]\n",
    "    output_size   = len(target_features)\n",
    "    learning_rate = 1e-2\n",
    "    num_epochs    = 300\n",
    "    batch_size    = 2**10\n",
    "    reg_strength  = 1e-3\n",
    "\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    patience = 10  # Number of epochs to wait before stopping\n",
    "    early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "    train_val_data, test_data = train_test_split(full_data,      test_size=0.3, random_state=42)\n",
    "    train_data, val_data      = train_test_split(train_val_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    # normalize the data\n",
    "    for key in normalizations.keys():\n",
    "        train_data[key] = train_data[key] / normalizations[key]\n",
    "        val_data[key]   = val_data[key]   / normalizations[key]\n",
    "        test_data[key]  = test_data[key]  / normalizations[key]\n",
    "        \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data[stub_features].values, train_data[target_features].values)).batch(batch_size).shuffle(buffer_size=len(train_data))\n",
    "    val_dataset   = tf.data.Dataset.from_tensor_slices((val_data[stub_features].values, val_data[target_features].values)).batch(batch_size)\n",
    "    test_dataset  = tf.data.Dataset.from_tensor_slices((test_data[stub_features].values, test_data[target_features].values)).batch(batch_size)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = MultiTaskNN(architecture, reg_strength=reg_strength)\n",
    "\n",
    "    # Build the model with the batch input shape\n",
    "    bs = None  # None allows for variable batch size\n",
    "    model.build((bs, input_size))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    scheduler = CustomLRScheduler(\n",
    "        optimizer, \n",
    "        factor=scale_factor, \n",
    "        patience=patience, \n",
    "        min_improvement=min_loss_improvement, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "    train_regression_losses     = []\n",
    "    train_classification_losses = []\n",
    "    train_combined_losses       = []\n",
    "    val_regression_losses       = []\n",
    "    val_classification_losses   = []\n",
    "    val_combined_losses         = []\n",
    "\n",
    "    learning_rates = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(features, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            regression_targets = targets[:, :-1]\n",
    "            classification_targets = targets[:, -1]\n",
    "            reg_outputs, class_outputs = model(features, training=True)\n",
    "            regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "            classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "            loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        return regression_loss, classification_loss, loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(features, targets):\n",
    "        regression_targets = targets[:, :-1]\n",
    "        classification_targets = targets[:, -1]\n",
    "        reg_outputs, class_outputs = model(features, training=False)\n",
    "        regression_loss = regression_criterion(regression_targets, reg_outputs)\n",
    "        classification_loss = classification_criterion(classification_targets, class_outputs)\n",
    "        loss = regression_weight * regression_loss + classification_weight * classification_loss\n",
    "        return regression_loss, classification_loss, loss\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        running_loss                = 0.0\n",
    "        running_regression_loss     = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        \n",
    "        \n",
    "        for features, targets in train_dataset:\n",
    "            \n",
    "            regression_loss, classification_loss, loss = train_step(features, targets)\n",
    "            \n",
    "            running_loss                += loss.numpy()\n",
    "            running_regression_loss     += regression_loss.numpy()\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "\n",
    "\n",
    "            \n",
    "        # Note: Remember to adjust if not using batches of equal sizes\n",
    "        train_loss                = running_loss                / len(train_dataset)\n",
    "        train_regression_loss     = running_regression_loss     / len(train_dataset)\n",
    "        train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "\n",
    "        train_regression_losses.append(train_regression_loss)\n",
    "        train_classification_losses.append(train_classification_loss)\n",
    "        train_combined_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        running_loss                = 0.0\n",
    "        running_regression_loss     = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        \n",
    "        for features, targets in val_dataset:\n",
    "            \n",
    "            regression_loss, classification_loss, loss = val_step(features, targets)\n",
    "            \n",
    "            running_loss                += loss.numpy()\n",
    "            running_regression_loss     += regression_loss.numpy()\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "        \n",
    "        avg_val_loss                = running_loss                / len(val_dataset)\n",
    "        avg_val_regression_loss     = running_regression_loss     / len(val_dataset)\n",
    "        avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "            \n",
    "        val_regression_losses.append(avg_val_regression_loss)\n",
    "        val_classification_losses.append(avg_val_classification_loss)\n",
    "        val_combined_losses.append(avg_val_loss)\n",
    "        \n",
    "        \n",
    "        current_lr = optimizer.lr.numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        print(f\"Train Losses - Regression: {train_regression_losses[-1]:.4f}, Classification: {train_classification_losses[-1]:.4f}, Combined: {train_combined_losses[-1]:.4f}\")\n",
    "        print(f\"Validation Losses - Regression: {val_regression_losses[-1]:.4f}, Classification: {val_classification_losses[-1]:.4f}, Combined: {val_combined_losses[-1]:.4f}\")\n",
    "        print(\"-------------\")\n",
    "        \n",
    "        \n",
    "        scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_loss})\n",
    "\n",
    "        \n",
    "\n",
    "        # Check for early stopping based on the new criterion\n",
    "        if avg_val_loss < (1 - early_stopping_threshold) * best_val_loss:  # 0.001 corresponds to 0.1%\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = min(best_val_loss, avg_val_loss)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "            break\n",
    "        \n",
    "    train_regression_losses_all.append(train_regression_losses)\n",
    "    train_classification_losses_all.append(train_classification_losses)\n",
    "    train_combined_losses_all.append(train_combined_losses)\n",
    "    val_regression_losses_all.append(val_regression_losses)\n",
    "    val_classification_losses_all.append(val_classification_losses)\n",
    "    val_combined_losses_all.append(val_combined_losses)\n",
    "    \n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del train_dataset\n",
    "    del val_dataset\n",
    "    del test_dataset\n",
    "    del train_data\n",
    "    del val_data\n",
    "    del test_data\n",
    "    del train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and std of the losses for each epoch\n",
    "# the lists in the lists are of different lengths, so we need to pad them with the last value\n",
    "# then we can compute the mean and std\n",
    "\n",
    "train_regression_losses_all      = [x + [x[-1]] * (max(map(len, train_regression_losses_all)) - len(x)) for x in train_regression_losses_all]\n",
    "train_classification_losses_all  = [x + [x[-1]] * (max(map(len, train_classification_losses_all)) - len(x)) for x in train_classification_losses_all]\n",
    "train_combined_losses_all        = [x + [x[-1]] * (max(map(len, train_combined_losses_all)) - len(x)) for x in train_combined_losses_all]\n",
    "\n",
    "val_regression_losses_all        = [x + [x[-1]] * (max(map(len, val_regression_losses_all)) - len(x)) for x in val_regression_losses_all]\n",
    "val_classification_losses_all    = [x + [x[-1]] * (max(map(len, val_classification_losses_all)) - len(x)) for x in val_classification_losses_all]\n",
    "val_combined_losses_all          = [x + [x[-1]] * (max(map(len, val_combined_losses_all)) - len(x)) for x in val_combined_losses_all]\n",
    "\n",
    "train_regression_losses_mean     = np.mean(np.array(train_regression_losses_all), axis=0)[1:]\n",
    "train_regression_losses_std      = np.std(np.array(train_regression_losses_all), axis=0)[1:]\n",
    "train_classification_losses_mean = np.mean(np.array(train_classification_losses_all), axis=0)[1:]\n",
    "train_classification_losses_std  = np.std(np.array(train_classification_losses_all), axis=0)[1:]\n",
    "train_combined_losses_mean       = np.mean(np.array(train_combined_losses_all), axis=0)[1:]\n",
    "train_combined_losses_std        = np.std(np.array(train_combined_losses_all), axis=0)[1:]\n",
    "\n",
    "val_regression_losses_mean       = np.mean(np.array(val_regression_losses_all), axis=0)[1:]\n",
    "val_regression_losses_std        = np.std(np.array(val_regression_losses_all), axis=0)[1:]\n",
    "val_classification_losses_mean   = np.mean(np.array(val_classification_losses_all), axis=0)[1:]\n",
    "val_classification_losses_std    = np.std(np.array(val_classification_losses_all), axis=0)[1:]\n",
    "val_combined_losses_mean         = np.mean(np.array(val_combined_losses_all), axis=0)[1:]\n",
    "val_combined_losses_std          = np.std(np.array(val_combined_losses_all), axis=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# REGRESSION LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_regression_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_regression_losses_mean)), train_regression_losses_mean - train_regression_losses_std, train_regression_losses_mean + train_regression_losses_std, alpha=0.3)\n",
    "ax.plot(val_regression_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_regression_losses_mean)), val_regression_losses_mean - val_regression_losses_std, val_regression_losses_mean + val_regression_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Regression Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}regression_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# CLASSIFICATION LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_classification_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_classification_losses_mean)), train_classification_losses_mean - train_classification_losses_std, train_classification_losses_mean + train_classification_losses_std, alpha=0.3)\n",
    "ax.plot(val_classification_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_classification_losses_mean)), val_classification_losses_mean - val_classification_losses_std, val_classification_losses_mean + val_classification_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}classification_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# COMBINED LOSS\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE, constrained_layout=True)\n",
    "ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "ax.set_axisbelow(True)\n",
    "hep.cms.label(\n",
    "    ax     = ax,\n",
    "    data   = True,\n",
    "    label  = \"Preliminary\",\n",
    "    rlabel = \"ZeroBias 2023C (13.6 TeV)\",\n",
    "    fontsize = 36,\n",
    ")\n",
    "ax.plot(train_combined_losses_mean, label=\"Train\", color=PALETTE[0], linewidth=3)\n",
    "ax.fill_between(np.arange(len(train_combined_losses_mean)), train_combined_losses_mean - train_combined_losses_std, train_combined_losses_mean + train_combined_losses_std, alpha=0.3)\n",
    "ax.plot(val_combined_losses_mean, label=\"Validation\", color=PALETTE[-1], linewidth=3)\n",
    "ax.fill_between(np.arange(len(val_combined_losses_mean)), val_combined_losses_mean - val_combined_losses_std, val_combined_losses_mean + val_combined_losses_std, alpha=0.3)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "ax.set_ylabel(\"Combined Loss\", fontsize=36)\n",
    "ax.legend(fontsize=36)\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "if PLOT_FLAG:\n",
    "    fig.savefig(f\"{PLOT_PATH}combined_loss_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
