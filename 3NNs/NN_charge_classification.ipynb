{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMTF input stubs fit with a Deep Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mplhep as hep\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.style.use(\"CMS\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training device: {'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0', '/device:GPU:1']\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 16:50:34.607604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 10518 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2024-07-26 16:50:34.607827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:1 with 10532 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "#save device 1\n",
    "device0 = tf.config.list_physical_devices('GPU')[0]\n",
    "print(device0)\n",
    "device1 = tf.config.list_physical_devices('GPU')[1]\n",
    "print(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.getenv(\"USER\")\n",
    "\n",
    "# FILE_PATH = \"/eos/cms/store/cmst3/group/daql1scout/ml_data/run3/bmtf_stubs_refit/\"\n",
    "FILE_PATH = \"/mnt/ml_data/run3/bmtf_stubs_refit/\"\n",
    "\n",
    "FILE_SAVE_DATA = \"/mnt/ml_data/run3/bmtf_stubs_refit_dummy/Data3NN/\"\n",
    "\n",
    "FILE_NAME = \"rereco\"\n",
    "\n",
    "OUT_PATH = \".\"\n",
    "LOSS_FNAME = \"losses_2_classification.csv\"\n",
    "\n",
    "FIGSIZE = (12, 7)\n",
    "\n",
    "petroff_10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler('color', petroff_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network model for regression+classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNN(tf.keras.Model):\n",
    "    def __init__(self, architecture, reg_strength=0.01):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "\n",
    "        # Check if the architecture list has at least 2 values (input size and one hidden layer)\n",
    "        if len(architecture) < 2:\n",
    "            raise ValueError(\"Architecture must contain at least input size and one hidden layer.\")\n",
    "\n",
    "        self.layers_list = []\n",
    "\n",
    "        # Iterate over the architecture list to dynamically create dense layers followed by batch normalization\n",
    "        for i in range(1, len(architecture)):\n",
    "            self.layers_list.append(layers.Dense(architecture[i], kernel_regularizer=regularizers.l2(reg_strength)))\n",
    "            self.layers_list.append(layers.Activation('elu'))\n",
    "\n",
    "        self.classification_head = layers.Dense(1, kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "        # self.classification_head = layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(reg_strength))  # for charge\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "\n",
    "        class_output = self.classification_head(x)\n",
    "        return class_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate scheduler callback\n",
    "class CustomLRScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        optimizer, \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_improvement=0.01, \n",
    "        verbose=True\n",
    "    ):\n",
    "        super(CustomLRScheduler, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_improvement = min_improvement\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            self.optimizer.lr.assign(self.min_lr)\n",
    "        if epoch < self.decrease_epoch:\n",
    "            self.increase_flag = True\n",
    "        else:\n",
    "            self.increase_flag = False\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if loss:\n",
    "            relative_improvement = (self.best_loss - loss) / self.best_loss\n",
    "\n",
    "            if relative_improvement < self.min_improvement:\n",
    "                self.patience_counter += 1\n",
    "            else:\n",
    "                self.patience_counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "            if self.patience_counter >= self.patience:\n",
    "                self._decrease_lr()\n",
    "\n",
    "    def _decrease_lr(self):\n",
    "        old_lr = self.optimizer.lr.numpy()\n",
    "        new_lr = old_lr * self.factor\n",
    "        self.optimizer.lr.assign(new_lr)\n",
    "        if self.verbose:\n",
    "            print(f\"Decreasing learning rate to {new_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stubs_norm         = 2**2\n",
    "station_norm         = 2**2\n",
    "sector_norm          = 2**3\n",
    "wheel_norm           = 2**1\n",
    "eta_norm             = 2**8\n",
    "qeta_norm            = 2**1\n",
    "tag_norm             = 2**0\n",
    "phi_norm             = 2**11\n",
    "phib_norm            = 2**9\n",
    "quality_norm         = 2**3\n",
    "reco_pt_norm         = 2**0\n",
    "reco_pt_inverse_norm = 2**0\n",
    "reco_eta_norm        = 2**2\n",
    "reco_phi_norm        = 2**2\n",
    "reco_charge_norm     = 2**0\n",
    "info_norm          = 2**2\n",
    "\n",
    "normalizations = {\n",
    "    \"n_stubs\": n_stubs_norm,\n",
    "    \"info_a\": info_norm,\n",
    "    \"info_b\": info_norm,\n",
    "    \"info_c\": info_norm,\n",
    "    \"info_d\": info_norm,\n",
    "    \"a_stNum\": station_norm,\n",
    "    \"a_scNum\": sector_norm,\n",
    "    \"a_whNum\": wheel_norm,\n",
    "    \"a_eta_1\": eta_norm,\n",
    "    \"a_qeta_1\": qeta_norm,\n",
    "    \"a_eta_2\": eta_norm,\n",
    "    \"a_qeta_2\": qeta_norm,\n",
    "    \"a_tag\": tag_norm,\n",
    "    \"a_phi\": phi_norm,\n",
    "    \"a_phiB\": phib_norm,\n",
    "    \"a_quality\": quality_norm,\n",
    "    \"b_stNum\": station_norm,\n",
    "    \"b_scNum\": sector_norm,\n",
    "    \"b_whNum\": wheel_norm,\n",
    "    \"b_eta_1\": eta_norm,\n",
    "    \"b_qeta_1\": qeta_norm,\n",
    "    \"b_eta_2\": eta_norm,\n",
    "    \"b_qeta_2\": qeta_norm,\n",
    "    \"b_tag\": tag_norm,\n",
    "    \"b_phi\": phi_norm,\n",
    "    \"b_phiB\": phib_norm,\n",
    "    \"b_quality\": quality_norm,\n",
    "    \"c_stNum\": station_norm,\n",
    "    \"c_scNum\": sector_norm,\n",
    "    \"c_whNum\": wheel_norm,\n",
    "    \"c_eta_1\": eta_norm,\n",
    "    \"c_qeta_1\": qeta_norm,\n",
    "    \"c_eta_2\": eta_norm,\n",
    "    \"c_qeta_2\": qeta_norm,\n",
    "    \"c_tag\": tag_norm,\n",
    "    \"c_phi\": phi_norm,\n",
    "    \"c_phiB\": phib_norm,\n",
    "    \"c_quality\": quality_norm,\n",
    "    \"d_stNum\": station_norm,\n",
    "    \"d_scNum\": sector_norm,\n",
    "    \"d_whNum\": wheel_norm,\n",
    "    \"d_eta_1\": eta_norm,\n",
    "    \"d_qeta_1\": qeta_norm,\n",
    "    \"d_eta_2\": eta_norm,\n",
    "    \"d_qeta_2\": qeta_norm,\n",
    "    \"d_tag\": tag_norm,\n",
    "    \"d_phi\": phi_norm,\n",
    "    \"d_phiB\": phib_norm,\n",
    "    \"d_quality\": quality_norm,\n",
    "    # \"ptReco\": reco_pt_norm,\n",
    "    \"ptRecoInverse\": reco_pt_inverse_norm,\n",
    "    \"etaExtRecoSt2\": reco_eta_norm,\n",
    "    \"phiExtRecoSt2\": reco_phi_norm,\n",
    "    \"chargeReco\": reco_charge_norm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_features_2 = [\n",
    "    'info_a', 'info_b',\n",
    "    'a_stNum', 'a_scNum', 'a_whNum', 'a_eta_1', 'a_qeta_1', 'a_eta_2', 'a_qeta_2', 'a_tag', 'a_phi', 'a_phiB', 'a_quality',\n",
    "    'b_stNum', 'b_scNum', 'b_whNum', 'b_eta_1', 'b_qeta_1', 'b_eta_2', 'b_qeta_2', 'b_tag', 'b_phi', 'b_phiB', 'b_quality'\n",
    "]\n",
    "\n",
    "stub_features_3 = [\n",
    "    'info_a', 'info_b', 'info_c',\n",
    "    'a_stNum', 'a_scNum', 'a_whNum', 'a_eta_1', 'a_qeta_1', 'a_eta_2', 'a_qeta_2', 'a_tag', 'a_phi', 'a_phiB', 'a_quality',\n",
    "    'b_stNum', 'b_scNum', 'b_whNum', 'b_eta_1', 'b_qeta_1', 'b_eta_2', 'b_qeta_2', 'b_tag', 'b_phi', 'b_phiB', 'b_quality',\n",
    "    'c_stNum', 'c_scNum', 'c_whNum', 'c_eta_1', 'c_qeta_1', 'c_eta_2', 'c_qeta_2', 'c_tag', 'c_phi', 'c_phiB', 'c_quality'\n",
    "]\n",
    "\n",
    "stub_features_4 = [\n",
    "    \n",
    "    'a_stNum', 'a_scNum', 'a_whNum', 'a_eta_1', 'a_qeta_1', 'a_eta_2', 'a_qeta_2', 'a_tag', 'a_phi', 'a_phiB', 'a_quality',\n",
    "    'b_stNum', 'b_scNum', 'b_whNum', 'b_eta_1', 'b_qeta_1', 'b_eta_2', 'b_qeta_2', 'b_tag', 'b_phi', 'b_phiB', 'b_quality',\n",
    "    'c_stNum', 'c_scNum', 'c_whNum', 'c_eta_1', 'c_qeta_1', 'c_eta_2', 'c_qeta_2', 'c_tag', 'c_phi', 'c_phiB', 'c_quality',\n",
    "    'd_stNum', 'd_scNum', 'd_whNum', 'd_eta_1', 'd_qeta_1', 'd_eta_2', 'd_qeta_2', 'd_tag', 'd_phi', 'd_phiB', 'd_quality'\n",
    "]\n",
    "\n",
    "target_features = [\n",
    " 'chargeReco'\n",
    "]\n",
    "\n",
    "l1_features = [\n",
    "    'ptL1', 'etaL1', 'phiL1', 'hwSignL1',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3 datasets data_2\n",
    "data_2 = pd.read_csv(FILE_SAVE_DATA + 'data_2s.csv')\n",
    "data_3 = pd.read_csv(FILE_SAVE_DATA + 'data_3s.csv')\n",
    "data_4 = pd.read_csv(FILE_SAVE_DATA + 'data_4s.csv')\n",
    "\n",
    "#remove n_stubs from data_2\n",
    "data_2 = data_2.drop(columns=['n_stubs'])\n",
    "data_3 = data_3.drop(columns=['n_stubs'])\n",
    "data_4 = data_4.drop(columns=['n_stubs'])\n",
    "\n",
    "\n",
    "#remove info_a, info_b, info_c, info_d from data_4\n",
    "data_4 = data_4.drop(columns=['info_a', 'info_b', 'info_c', 'info_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_stNum</th>\n",
       "      <th>a_scNum</th>\n",
       "      <th>a_whNum</th>\n",
       "      <th>a_eta_1</th>\n",
       "      <th>a_qeta_1</th>\n",
       "      <th>a_eta_2</th>\n",
       "      <th>a_qeta_2</th>\n",
       "      <th>a_tag</th>\n",
       "      <th>a_phi</th>\n",
       "      <th>a_phiB</th>\n",
       "      <th>...</th>\n",
       "      <th>d_phiB</th>\n",
       "      <th>d_quality</th>\n",
       "      <th>ptRecoInverse</th>\n",
       "      <th>etaExtRecoSt2</th>\n",
       "      <th>phiExtRecoSt2</th>\n",
       "      <th>chargeReco</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>hwSignL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-153</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.162074</td>\n",
       "      <td>0.601459</td>\n",
       "      <td>-2.10473</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>-2.09440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-46</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-427</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.106646</td>\n",
       "      <td>-0.486673</td>\n",
       "      <td>2.01145</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>-0.489375</td>\n",
       "      <td>2.00713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>174</td>\n",
       "      <td>...</td>\n",
       "      <td>-37</td>\n",
       "      <td>6</td>\n",
       "      <td>0.182156</td>\n",
       "      <td>0.547758</td>\n",
       "      <td>1.07746</td>\n",
       "      <td>0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.554625</td>\n",
       "      <td>1.10174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>732</td>\n",
       "      <td>-91</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>0.033843</td>\n",
       "      <td>-1.94995</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>-1.94168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151</td>\n",
       "      <td>-113</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.150777</td>\n",
       "      <td>0.651324</td>\n",
       "      <td>-1.13543</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.641625</td>\n",
       "      <td>-1.11265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686165</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>-54</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.063276</td>\n",
       "      <td>0.067195</td>\n",
       "      <td>-1.67440</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>-1.67988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686166</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-360</td>\n",
       "      <td>-54</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.060426</td>\n",
       "      <td>0.048062</td>\n",
       "      <td>-1.66768</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>-1.67988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686167</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>-83</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.113663</td>\n",
       "      <td>0.763765</td>\n",
       "      <td>-1.00096</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.717750</td>\n",
       "      <td>-1.00356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686168</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>-92</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.117008</td>\n",
       "      <td>-0.757491</td>\n",
       "      <td>-1.01835</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>-0.717750</td>\n",
       "      <td>-1.02538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686169</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-494</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.110967</td>\n",
       "      <td>0.025671</td>\n",
       "      <td>2.53925</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.032625</td>\n",
       "      <td>2.51982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>686170 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        a_stNum  a_scNum  a_whNum  a_eta_1  a_qeta_1  a_eta_2  a_qeta_2  \\\n",
       "0             1        8        1        7         0        7         0   \n",
       "1             1        4       -1      -46         1      255         0   \n",
       "2             1        2        1        7         0        7         0   \n",
       "3             1        8        0        0         2      255         0   \n",
       "4             1       10        1        7         0        7         0   \n",
       "...         ...      ...      ...      ...       ...      ...       ...   \n",
       "686165        1        9        0        7         2      255         0   \n",
       "686166        1        9        0        7         2      255         0   \n",
       "686167        1       10        1        7         0        7         0   \n",
       "686168        1       10       -1        7         0        7         0   \n",
       "686169        1        5        0        0         1      255         0   \n",
       "\n",
       "        a_tag  a_phi  a_phiB  ...  d_phiB  d_quality  ptRecoInverse  \\\n",
       "0           1   -153     130  ...      32          5       0.162074   \n",
       "1           1   -427      89  ...      29          2       0.106646   \n",
       "2           1      8     174  ...     -37          6       0.182156   \n",
       "3           1    732     -91  ...       9          6       0.114194   \n",
       "4           1   -151    -113  ...       4          6       0.150777   \n",
       "...       ...    ...     ...  ...     ...        ...            ...   \n",
       "686165      1   -360     -54  ...      -9          5       0.063276   \n",
       "686166      1   -360     -54  ...      -9          5       0.060426   \n",
       "686167      1    313     -83  ...      -1          6       0.113663   \n",
       "686168      1    225     -92  ...      11          5       0.117008   \n",
       "686169      1   -494      82  ...      -9          5       0.110967   \n",
       "\n",
       "        etaExtRecoSt2  phiExtRecoSt2  chargeReco  ptL1     etaL1    phiL1  \\\n",
       "0            0.601459       -2.10473           0   9.0  0.598125 -2.09440   \n",
       "1           -0.486673        2.01145           0  10.5 -0.489375  2.00713   \n",
       "2            0.547758        1.07746           0   6.5  0.554625  1.10174   \n",
       "3            0.033843       -1.94995           1  10.5  0.010875 -1.94168   \n",
       "4            0.651324       -1.13543           1  10.5  0.641625 -1.11265   \n",
       "...               ...            ...         ...   ...       ...      ...   \n",
       "686165       0.067195       -1.67440           1  19.5  0.054375 -1.67988   \n",
       "686166       0.048062       -1.66768           1  19.5  0.054375 -1.67988   \n",
       "686167       0.763765       -1.00096           1  11.0  0.717750 -1.00356   \n",
       "686168      -0.757491       -1.01835           1  10.5 -0.717750 -1.02538   \n",
       "686169       0.025671        2.53925           0  10.5  0.032625  2.51982   \n",
       "\n",
       "        hwSignL1  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "686165         0  \n",
       "686166         0  \n",
       "686167         0  \n",
       "686168         0  \n",
       "686169         1  \n",
       "\n",
       "[686170 rows x 52 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split for the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data_2, test_data_2 = train_test_split(data_2,      test_size=0.3, random_state=42)\n",
    "train_data_2, val_data_2      = train_test_split(train_val_data_2, test_size=0.1, random_state=42)\n",
    "\n",
    "train_val_data_3, test_data_3 = train_test_split(data_3,      test_size=0.3, random_state=42)\n",
    "train_data_3, val_data_3      = train_test_split(train_val_data_3, test_size=0.1, random_state=42)\n",
    "\n",
    "train_val_data_4, test_data_4 = train_test_split(data_4,      test_size=0.3, random_state=42)\n",
    "train_data_4, val_data_4      = train_test_split(train_val_data_4, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset 2s: 789368\n",
      "Val dataset 2s: 87708\n",
      "Test dataset 2s: 375890\n",
      "----------\n",
      "Train dataset 3s: 625242\n",
      "Val dataset 3s: 69472\n",
      "Test dataset 3s: 297735\n",
      "----------\n",
      "Train dataset 4s: 432287\n",
      "Val dataset 4s: 48032\n",
      "Test dataset 4s: 205851\n"
     ]
    }
   ],
   "source": [
    "# inspect the datasets\n",
    "print(f\"Train dataset 2s: {len(train_data_2)}\")\n",
    "print(f\"Val dataset 2s: {len(val_data_2)}\")\n",
    "print(f\"Test dataset 2s: {len(test_data_2)}\")\n",
    "print('----------')\n",
    "print(f\"Train dataset 3s: {len(train_data_3)}\")\n",
    "print(f\"Val dataset 3s: {len(val_data_3)}\")\n",
    "print(f\"Test dataset 3s: {len(test_data_3)}\")\n",
    "print('----------')\n",
    "print(f\"Train dataset 4s: {len(train_data_4)}\")\n",
    "print(f\"Val dataset 4s: {len(val_data_4)}\")\n",
    "print(f\"Test dataset 4s: {len(test_data_4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that create a new dictionary with the features of nromalizations that are present in the data_2\n",
    "def get_normalizations_features(data, normalizations):\n",
    "    return {key: normalizations[key] for key in data.keys() if key in normalizations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptRecoInverse</th>\n",
       "      <th>etaExtRecoSt2</th>\n",
       "      <th>phiExtRecoSt2</th>\n",
       "      <th>chargeReco</th>\n",
       "      <th>ptL1</th>\n",
       "      <th>etaL1</th>\n",
       "      <th>phiL1</th>\n",
       "      <th>hwSignL1</th>\n",
       "      <th>a_stNum</th>\n",
       "      <th>a_scNum</th>\n",
       "      <th>...</th>\n",
       "      <th>b_eta_1</th>\n",
       "      <th>b_qeta_1</th>\n",
       "      <th>b_eta_2</th>\n",
       "      <th>b_qeta_2</th>\n",
       "      <th>b_tag</th>\n",
       "      <th>b_phi</th>\n",
       "      <th>b_phiB</th>\n",
       "      <th>b_quality</th>\n",
       "      <th>info_a</th>\n",
       "      <th>info_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281981</th>\n",
       "      <td>0.147291</td>\n",
       "      <td>0.696830</td>\n",
       "      <td>-1.125860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.674250</td>\n",
       "      <td>-1.112650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724090</th>\n",
       "      <td>0.214781</td>\n",
       "      <td>0.528629</td>\n",
       "      <td>1.652480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500250</td>\n",
       "      <td>1.592610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501506</th>\n",
       "      <td>0.176419</td>\n",
       "      <td>-0.349875</td>\n",
       "      <td>1.225480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.293625</td>\n",
       "      <td>1.199910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1366.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147459</th>\n",
       "      <td>0.041132</td>\n",
       "      <td>-0.958019</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>-0.880875</td>\n",
       "      <td>0.894481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-575.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665351</th>\n",
       "      <td>0.150967</td>\n",
       "      <td>-0.277938</td>\n",
       "      <td>-2.929450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>-0.315375</td>\n",
       "      <td>-2.923430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633271</th>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.833694</td>\n",
       "      <td>-0.342739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.837375</td>\n",
       "      <td>-0.359974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179784</th>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.834509</td>\n",
       "      <td>-3.040680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.848250</td>\n",
       "      <td>-3.054330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450924</th>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.894351</td>\n",
       "      <td>-2.683100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>0.848250</td>\n",
       "      <td>-2.683440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-253.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899024</th>\n",
       "      <td>0.005539</td>\n",
       "      <td>0.302151</td>\n",
       "      <td>-0.952849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.315375</td>\n",
       "      <td>-0.959931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548111</th>\n",
       "      <td>0.190984</td>\n",
       "      <td>-0.897677</td>\n",
       "      <td>0.916786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.916298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-629.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789368 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ptRecoInverse  etaExtRecoSt2  phiExtRecoSt2  chargeReco  ptL1  \\\n",
       "281981        0.147291       0.696830      -1.125860         0.0   7.0   \n",
       "724090        0.214781       0.528629       1.652480         1.0   6.0   \n",
       "501506        0.176419      -0.349875       1.225480         0.0  14.0   \n",
       "1147459       0.041132      -0.958019       0.919271         0.0  34.5   \n",
       "665351        0.150967      -0.277938      -2.929450         1.0   7.5   \n",
       "...                ...            ...            ...         ...   ...   \n",
       "633271        0.083020       0.833694      -0.342739         1.0  14.5   \n",
       "179784        0.188203       0.834509      -3.040680         1.0   9.5   \n",
       "450924        0.029636       0.894351      -2.683100         1.0  52.5   \n",
       "899024        0.005539       0.302151      -0.952849         0.0  31.0   \n",
       "548111        0.190984      -0.897677       0.916786         1.0   8.0   \n",
       "\n",
       "            etaL1     phiL1  hwSignL1  a_stNum  a_scNum  ...  b_eta_1  \\\n",
       "281981   0.674250 -1.112650       1.0      1.0     10.0  ...     62.0   \n",
       "724090   0.500250  1.592610       0.0      1.0      3.0  ...     46.0   \n",
       "501506  -0.293625  1.199910       1.0      3.0      2.0  ...      7.0   \n",
       "1147459 -0.880875  0.894481       1.0      2.0      2.0  ...      7.0   \n",
       "665351  -0.315375 -2.923430       0.0      3.0      6.0  ...      7.0   \n",
       "...           ...       ...       ...      ...      ...  ...      ...   \n",
       "633271   0.837375 -0.359974       0.0      2.0     11.0  ...     77.0   \n",
       "179784   0.848250 -3.054330       0.0      2.0      6.0  ...      7.0   \n",
       "450924   0.848250 -2.683440       0.0      2.0      7.0  ...      7.0   \n",
       "899024   0.315375 -0.959931       1.0      2.0     10.0  ...     29.0   \n",
       "548111  -0.870000  0.916298       0.0      2.0      2.0  ...    -81.0   \n",
       "\n",
       "         b_qeta_1  b_eta_2  b_qeta_2  b_tag   b_phi  b_phiB  b_quality  \\\n",
       "281981        2.0    255.0       0.0    1.0   -99.0   106.0        6.0   \n",
       "724090        2.0    255.0       0.0    1.0   -42.0   -75.0        5.0   \n",
       "501506        0.0      7.0       0.0    1.0 -1366.0    36.0        5.0   \n",
       "1147459       0.0      7.0       0.0    1.0  -575.0    20.0        2.0   \n",
       "665351        0.0      7.0       0.0    1.0   884.0    36.0        5.0   \n",
       "...           ...      ...       ...    ...     ...     ...        ...   \n",
       "633271        1.0    255.0       0.0    1.0   632.0   -24.0        5.0   \n",
       "179784        0.0      7.0       0.0    1.0   326.0    -8.0        6.0   \n",
       "450924        0.0      7.0       0.0    1.0  -253.0     6.0        2.0   \n",
       "899024        2.0    255.0       0.0    1.0   375.0   -11.0        5.0   \n",
       "548111        1.0    255.0       0.0    1.0  -629.0   -39.0        5.0   \n",
       "\n",
       "         info_a  info_b  \n",
       "281981      1.0     3.0  \n",
       "724090      1.0     3.0  \n",
       "501506      3.0     4.0  \n",
       "1147459     2.0     3.0  \n",
       "665351      3.0     4.0  \n",
       "...         ...     ...  \n",
       "633271      2.0     3.0  \n",
       "179784      2.0     3.0  \n",
       "450924      2.0     3.0  \n",
       "899024      2.0     3.0  \n",
       "548111      2.0     3.0  \n",
       "\n",
       "[789368 rows x 32 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "def normalize_data(train_data, val_data, test_data, data, normalizations):\n",
    "    for key in get_normalizations_features(data, normalizations):\n",
    "        train_data[key] = train_data[key] / normalizations[key]\n",
    "        val_data[key]   = val_data[key]   / normalizations[key]\n",
    "        test_data[key]  = test_data[key]  / normalizations[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data(train_data_2, val_data_2, test_data_2, data_2, normalizations)\n",
    "normalize_data(train_data_3, val_data_3, test_data_3, data_3, normalizations)\n",
    "normalize_data(train_data_4, val_data_4, test_data_4, data_4, normalizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_2(features, targets, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        classification_targets = targets[:, :]\n",
    "        class_outputs = model(features, training=True)\n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(tape.gradient(classification_loss, model.trainable_variables), model.trainable_variables))\n",
    "    return classification_loss\n",
    "\n",
    "@tf.function\n",
    "def val_step_2(features, targets, model):\n",
    "    classification_targets = targets[:, :]\n",
    "    class_outputs = model(features, training=False)\n",
    "    classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    return classification_loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_3(features, targets, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        classification_targets = targets[:, :]\n",
    "        class_outputs = model(features, training=True)\n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(tape.gradient(classification_loss, model.trainable_variables), model.trainable_variables))\n",
    "    return classification_loss\n",
    "\n",
    "@tf.function\n",
    "def val_step_3(features, targets, model):\n",
    "    classification_targets = targets[:, :]\n",
    "    class_outputs = model(features, training=False)\n",
    "    classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    return classification_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_4(features, targets, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        classification_targets = targets[:, :]\n",
    "        class_outputs = model(features, training=True)\n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(tape.gradient(classification_loss, model.trainable_variables), model.trainable_variables))\n",
    "    return classification_loss\n",
    "\n",
    "@tf.function\n",
    "def val_step_4(features, targets, model):\n",
    "    classification_targets = targets[:, :]\n",
    "    class_outputs = model(features, training=False)\n",
    "    classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    return classification_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters 2NN\n",
    "input_size_2    = len(stub_features_2)\n",
    "architecture_2   = [input_size_2 , 16,16]\n",
    "output_size_2    = len(target_features)\n",
    "learning_rate_2  = 1e-2\n",
    "num_epochs_2     = 300\n",
    "batch_size_2     = 2**8\n",
    "reg_strength_2   = 1e-3\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor_2  = 0.5\n",
    "patience_2  = 5\n",
    "min_loss_improvement_2  = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss_2  = float('inf')\n",
    "epochs_without_improvement_2  = 0\n",
    "patience_2  = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold_2  = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer_2 = optimizers.Adam(learning_rate=learning_rate_2)\n",
    "scheduler_2 = CustomLRScheduler(\n",
    "    optimizer_2, \n",
    "    factor=scale_factor_2, \n",
    "    patience=patience_2, \n",
    "    min_improvement=min_loss_improvement_2, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters 3NN\n",
    "input_size_3    = len(stub_features_3)\n",
    "architecture_3   = [input_size_3 , 8,8]\n",
    "output_size_3    = len(target_features)\n",
    "learning_rate_3  = 1e-2\n",
    "num_epochs_3     = 300\n",
    "batch_size_3     = 2**8\n",
    "reg_strength_3   = 1e-3\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor_3  = 0.5\n",
    "patience_3  = 5\n",
    "min_loss_improvement_3 = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss_3  = float('inf')\n",
    "epochs_without_improvement_3  = 0\n",
    "patience_3  = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold_3  = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer_3 = optimizers.Adam(learning_rate=learning_rate_3)\n",
    "scheduler_3 = CustomLRScheduler(\n",
    "    optimizer_3, \n",
    "    factor=scale_factor_3, \n",
    "    patience=patience_3, \n",
    "    min_improvement=min_loss_improvement_3, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters 4NN\n",
    "input_size_4    = len(stub_features_4)\n",
    "architecture_4   = [input_size_4 , 32,16,16]\n",
    "output_size_4    = len(target_features)\n",
    "learning_rate_4  = 1e-2\n",
    "num_epochs_4     = 300\n",
    "batch_size_4     = 2**8\n",
    "reg_strength_4   = 1e-3\n",
    "\n",
    "# lr scheduler\n",
    "scale_factor_4  = 0.5\n",
    "patience_4  = 5\n",
    "min_loss_improvement_4 = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss_4  = float('inf')\n",
    "epochs_without_improvement_4  = 0\n",
    "patience_4  = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold_4  = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer_4 = optimizers.Adam(learning_rate=learning_rate_4)\n",
    "scheduler_4 = CustomLRScheduler(\n",
    "    optimizer_4, \n",
    "    factor=scale_factor_4, \n",
    "    patience=patience_4, \n",
    "    min_improvement=min_loss_improvement_4, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_2 = tf.data.Dataset.from_tensor_slices((train_data_2[stub_features_2].values, train_data_2[target_features].values)).batch(batch_size_2).shuffle(buffer_size=len(train_data_2))\n",
    "val_dataset_2   = tf.data.Dataset.from_tensor_slices((val_data_2[stub_features_2].values, val_data_2[target_features].values)).batch(batch_size_2)\n",
    "test_dataset_2  = tf.data.Dataset.from_tensor_slices((test_data_2[stub_features_2].values, test_data_2[target_features].values)).batch(batch_size_2)\n",
    "\n",
    "train_dataset_3 = tf.data.Dataset.from_tensor_slices((train_data_3[stub_features_3].values, train_data_3[target_features].values)).batch(batch_size_3).shuffle(buffer_size=len(train_data_3))\n",
    "val_dataset_3   = tf.data.Dataset.from_tensor_slices((val_data_3[stub_features_3].values, val_data_3[target_features].values)).batch(batch_size_3)\n",
    "test_dataset_3  = tf.data.Dataset.from_tensor_slices((test_data_3[stub_features_3].values, test_data_3[target_features].values)).batch(batch_size_3)\n",
    "\n",
    "train_dataset_4 = tf.data.Dataset.from_tensor_slices((train_data_4[stub_features_4].values, train_data_4[target_features].values)).batch(batch_size_4).shuffle(buffer_size=len(train_data_4))\n",
    "val_dataset_4   = tf.data.Dataset.from_tensor_slices((val_data_4[stub_features_4].values, val_data_4[target_features].values)).batch(batch_size_4)\n",
    "test_dataset_4  = tf.data.Dataset.from_tensor_slices((test_data_4[stub_features_4].values, test_data_4[target_features].values)).batch(batch_size_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters 2NN classificaation: 689\n",
      "Number of trainable parameters 2NN classificaation: 689\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model_2 = MultiTaskNN(architecture_2, reg_strength=reg_strength_2)\n",
    "\n",
    "# Build the model with the batch input shape\n",
    "bs = None  # None allows for variable batch size\n",
    "model_2.build((bs, input_size_2))\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params_2 = model_2.count_params()\n",
    "trainable_vars_2 = [var for var in model_2.trainable_variables]\n",
    "trainable_params_2 = sum([tf.size(var).numpy() for var in trainable_vars_2])\n",
    "print(f\"Total number of parameters 2NN classificaation: {total_params_2}\")\n",
    "print(f\"Number of trainable parameters 2NN classificaation: {trainable_params_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters 4NN classificaation: 377\n",
      "Number of trainable parameters 4NN classificaation: 377\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model_3 = MultiTaskNN(architecture_3, reg_strength=reg_strength_3)\n",
    "\n",
    "# Build the model with the batch input shape\n",
    "bs = None  # None allows for variable batch size\n",
    "model_3.build((bs, input_size_3))\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params_3 = model_3.count_params()\n",
    "trainable_vars_3 = [var for var in model_3.trainable_variables]\n",
    "trainable_params_3 = sum([tf.size(var).numpy() for var in trainable_vars_3])\n",
    "print(f\"Total number of parameters 4NN classificaation: {total_params_3}\")\n",
    "print(f\"Number of trainable parameters 4NN classificaation: {trainable_params_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters 4NN classificaation: 2257\n",
      "Number of trainable parameters 4NN classificaation: 2257\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model_4 = MultiTaskNN(architecture_4, reg_strength=reg_strength_4)\n",
    "\n",
    "# Build the model with the batch input shape\n",
    "bs = None  # None allows for variable batch size\n",
    "model_4.build((bs, input_size_4))\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params_4 = model_4.count_params()\n",
    "trainable_vars_4 = [var for var in model_4.trainable_variables]\n",
    "trainable_params_4 = sum([tf.size(var).numpy() for var in trainable_vars_4])\n",
    "print(f\"Total number of parameters 4NN classificaation: {total_params_4}\")\n",
    "print(f\"Number of trainable parameters 4NN classificaation: {trainable_params_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(\n",
    "    num_epochs,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    out_path,\n",
    "    early_stopping_threshold,\n",
    "    patience,\n",
    "    best_val_loss,\n",
    "    loss_fname,\n",
    "    val_step,\n",
    "    train_step\n",
    "):\n",
    "    train_classification_losses = []\n",
    "    val_classification_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    out_file_path = os.path.join(out_path, loss_fname)\n",
    "    \n",
    "    with open(out_file_path, \"w\") as out_file:\n",
    "        out_file.write(\"train_classification_loss,val_classification_loss,learning_rate\\n\")\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        running_classification_loss = 0.0\n",
    "\n",
    "        for features, targets in train_dataset:\n",
    "            classification_loss = train_step(features, targets, model, optimizer)\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "\n",
    "        train_classification_loss = running_classification_loss / len(train_dataset)\n",
    "        train_classification_losses.append(train_classification_loss)\n",
    "\n",
    "        # Validation\n",
    "        running_classification_loss = 0.0\n",
    "        for features, targets in val_dataset:\n",
    "            classification_loss = val_step(features, targets, model)\n",
    "            running_classification_loss += classification_loss.numpy()\n",
    "\n",
    "        avg_val_classification_loss = running_classification_loss / len(val_dataset)\n",
    "        val_classification_losses.append(avg_val_classification_loss)\n",
    "\n",
    "        current_lr = optimizer.lr.numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"Learning rate: {current_lr:.2e}\")\n",
    "        print(f\"Train Losses - Classification: {train_classification_losses[-1]:.4f}\")\n",
    "        print(f\"Validation Losses - Classification: {val_classification_losses[-1]:.4f}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "        scheduler.on_epoch_end(epoch, {\"val_loss\": avg_val_classification_loss})\n",
    "\n",
    "        with open(out_file_path, \"a\") as output_file:\n",
    "            output_file.write(f\"{train_classification_losses[-1]},{val_classification_losses[-1]},{current_lr}\\n\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_classification_loss < (1 - early_stopping_threshold) * best_val_loss:\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = min(best_val_loss, avg_val_classification_loss)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs!\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"train_classification_losses\": train_classification_losses,\n",
    "        \"val_classification_losses\": val_classification_losses,\n",
    "        \"learning_rates\": learning_rates\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# results = train_classification_model(num_epochs_2, train_dataset_2, val_dataset_2, model_2, optimizer_2, scheduler_2, OUT_PATH, early_stopping_threshold_2, patience_2, best_val_loss_2, \"losses_2_classification.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide which snn you want to re-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN to train\n",
    "snn_to_train = {\n",
    "    '2NN:': False,\n",
    "    '3NN:': False,\n",
    "    '4NN:': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the files of the models if the dict is True\n",
    "if snn_to_train['2NN:']:\n",
    "    if os.path.exists(\"model_2\"):\n",
    "        os.remove(\"model_2\")\n",
    "    if os.path.exists(\"losses_2_classification.csv\"):\n",
    "        os.remove(\"losses_2_classification.csv\")\n",
    "if snn_to_train['3NN:']:\n",
    "    if os.path.exists(\"model_3\"):\n",
    "        os.remove(\"model_3\")\n",
    "    if os.path.exists(\"losses_3_classification.csv\"):\n",
    "        os.remove(\"losses_3_classification.csv\")\n",
    "if snn_to_train['4NN:']:\n",
    "    if os.path.exists(\"model_4\"):\n",
    "        os.remove(\"model_4\")\n",
    "    if os.path.exists(\"losses_4_classification.csv\"):\n",
    "        os.remove(\"losses_4_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the model is already trained\n",
    "if os.path.exists(OUT_PATH + 'Models/model_2' + '.keras'):\n",
    "    model_2.load_weights(OUT_PATH + 'Models/model_2' + '.keras')\n",
    "else:\n",
    "    results_2 = train_classification_model(\n",
    "        num_epochs_2,\n",
    "        train_dataset_2,\n",
    "        val_dataset_2,\n",
    "        model_2,\n",
    "        optimizer_2,\n",
    "        scheduler_2,\n",
    "        OUT_PATH,\n",
    "        early_stopping_threshold_2,\n",
    "        patience_2,\n",
    "        best_val_loss_2,\n",
    "        \"losses_2_classification.csv\",\n",
    "        val_step_2,\n",
    "        train_step_2\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model_2.save_weights(OUT_PATH + 'Models/model_2' + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the model is already trained\n",
    "if os.path.exists(OUT_PATH + 'Models/model_3' + '.keras'):\n",
    "    model_3.load_weights(OUT_PATH + 'Models/model_3' + '.keras')\n",
    "else:\n",
    "    results_3 = train_classification_model(\n",
    "        num_epochs_3,\n",
    "        train_dataset_3,\n",
    "        val_dataset_3,\n",
    "        model_3,\n",
    "        optimizer_3,\n",
    "        scheduler_3,\n",
    "        OUT_PATH,\n",
    "        early_stopping_threshold_3,\n",
    "        patience_3,\n",
    "        best_val_loss_3,\n",
    "        \"losses_3_classification.csv\",\n",
    "        val_step_3,\n",
    "        train_step_3\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model_3.save_weights(OUT_PATH + 'Models/model_3' + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the model is already trained\n",
    "if os.path.exists(OUT_PATH + 'Models/model_4' + '.keras'):\n",
    "    model_4.load_weights(OUT_PATH + 'Models/model_4' + '.keras')\n",
    "else:\n",
    "    results_4 = train_classification_model(\n",
    "        num_epochs_4,\n",
    "        train_dataset_4,\n",
    "        val_dataset_4,\n",
    "        model_4,\n",
    "        optimizer_4,\n",
    "        scheduler_4,\n",
    "        OUT_PATH,\n",
    "        early_stopping_threshold_4,\n",
    "        patience_4,\n",
    "        best_val_loss_4,\n",
    "        \"losses_4_classification.csv\",\n",
    "        val_step_4,\n",
    "        train_step_4\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model_4.save_weights(OUT_PATH + 'Models/model_4' + '.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test data from `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(test_data, stub_features, target_features, l1_features, model, normalizations):\n",
    "    # Extract features and targets\n",
    "    test_features = test_data[stub_features].values\n",
    "    test_targets  = test_data[target_features].values\n",
    "\n",
    "    # Make predictions\n",
    "    class_predictions = model(test_features, training=False)\n",
    "\n",
    "    # Combine features, targets, predictions, and L1 features into a DataFrame\n",
    "    test_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            (\n",
    "                test_features,\n",
    "                test_targets,\n",
    "                class_predictions.numpy(),\n",
    "                test_data[l1_features].values,\n",
    "            ),\n",
    "            axis=1\n",
    "        ),\n",
    "        columns=stub_features + target_features + [\"chargeReco_pred\"] + l1_features\n",
    "    )\n",
    "\n",
    "    # Apply sigmoid and threshold to predictions\n",
    "    test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 1 / (1 + np.exp(-x)))\n",
    "    test_df.loc[:, \"chargeReco_pred\"] = test_df[\"chargeReco_pred\"].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    # Rescale the features\n",
    "    for key in get_normalizations_features(test_df, normalizations):\n",
    "        test_df[key] = test_df[key] * normalizations[key]\n",
    "    \n",
    "    # Rescale the predictions\n",
    "    test_df[\"chargeReco_pred\"] = test_df[\"chargeReco_pred\"] * normalizations[\"chargeReco\"]\n",
    "\n",
    "    # Drop features, keep only predictions and targets and L1 features\n",
    "    test_df = test_df[[\"chargeReco\", \"chargeReco_pred\"] + l1_features]\n",
    "\n",
    "    # Create true values for chargeReco\n",
    "    test_df[\"chargeReco_true\"] = test_df[\"chargeReco\"]\n",
    "\n",
    "    # Transform hwSignL1 into chargeL1\n",
    "    test_df[\"chargeL1\"] = test_df[\"hwSignL1\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    test_df = test_df[[\"chargeReco_true\", \"chargeReco_pred\", \"chargeL1\"]]\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2 = process_test_data(test_data_2, stub_features_2, target_features, l1_features, model_2, normalizations)\n",
    "test_df_3 = process_test_data(test_data_3, stub_features_3, target_features, l1_features, model_3, normalizations)\n",
    "test_df_4 = process_test_data(test_data_4, stub_features_4, target_features, l1_features, model_4, normalizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Test Set 2stubs: 95.40%\n",
      "Architecture used (#features, #neurons, ..): [24, 16, 16]\n",
      "L1 Classification Accuracy on Test Set 2stubs: 95.30%\n",
      "----------------\n",
      "Classification Accuracy on Test Set 3stubs: 98.06%\n",
      "Architecture used (#features, #neurons, ..):  [36, 8, 8]\n",
      "L1 Classification Accuracy on Test Set 3stubs: 98.65%\n",
      "----------------\n",
      "Classification Accuracy on Test Set 4stubs: 98.27%\n",
      "Architecture used (#features, #neurons, ..): [44, 32, 16, 16]\n",
      "L1 Classification Accuracy on Test Set 4stubs: 98.85%\n"
     ]
    }
   ],
   "source": [
    "accuracy_2 = (test_df_2[\"chargeReco_pred\"] == test_df_2[\"chargeReco_true\"]).sum() / len(test_df_2)\n",
    "print(f\"Classification Accuracy on Test Set 2stubs: {accuracy_2*100:.2f}%\")\n",
    "print(f\"Architecture used (#features, #neurons, ..): {architecture_2}\")\n",
    "\n",
    "l1_accuracy_2 = (test_df_2[\"chargeL1\"] == test_df_2[\"chargeReco_true\"]).sum() / len(test_df_2)\n",
    "print(f\"L1 Classification Accuracy on Test Set 2stubs: {l1_accuracy_2*100:.2f}%\")\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "accuracy_3 = (test_df_3[\"chargeReco_pred\"] == test_df_3[\"chargeReco_true\"]).sum() / len(test_df_3)\n",
    "print(f\"Classification Accuracy on Test Set 3stubs: {accuracy_3*100:.2f}%\")\n",
    "print(f\"Architecture used (#features, #neurons, ..):  {architecture_3}\")\n",
    "\n",
    "l1_accuracy_3 = (test_df_3[\"chargeL1\"] == test_df_3[\"chargeReco_true\"]).sum() / len(test_df_3)\n",
    "print(f\"L1 Classification Accuracy on Test Set 3stubs: {l1_accuracy_3*100:.2f}%\")\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "accuracy_4 = (test_df_4[\"chargeReco_pred\"] == test_df_4[\"chargeReco_true\"]).sum() / len(test_df_4)\n",
    "print(f\"Classification Accuracy on Test Set 4stubs: {accuracy_4*100:.2f}%\")\n",
    "print(f\"Architecture used (#features, #neurons, ..): {architecture_4}\")\n",
    "\n",
    "l1_accuracy_4 = (test_df_4[\"chargeL1\"] == test_df_4[\"chargeReco_true\"]).sum() / len(test_df_4)\n",
    "print(f\"L1 Classification Accuracy on Test Set 4stubs: {l1_accuracy_4*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob      = True\n",
    "PLOT_PATH = f\"/eos/user/{USER[0]}/{USER}/nnreco-plots/\"\n",
    "PLOT_FLAG = False\n",
    "\n",
    "if not os.path.exists(PLOT_PATH):\n",
    "    os.makedirs(PLOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "def plot_losses_and_learning_rates(\n",
    "    train_classification_losses,\n",
    "    val_classification_losses,\n",
    "    learning_rates,\n",
    "    PLOT_FLAG,\n",
    "    PLOT_PATH,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    reg_strength,\n",
    "    regression_weight,\n",
    "    min_loss_improvement,\n",
    "    early_stopping_threshold,\n",
    "    FIGSIZE=(12, 8)\n",
    "):\n",
    "    # Plot classification losses\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "    hep.cms.label(\n",
    "        ax=ax,\n",
    "        data=True,\n",
    "        label=\"Preliminary\",\n",
    "        rlabel=\"ZeroBias 2023C (13.6 TeV)\",\n",
    "        fontsize=36,\n",
    "    )\n",
    "\n",
    "    ax.plot(train_classification_losses, label=\"Train\")\n",
    "    ax.plot(val_classification_losses, label=\"Validation\")\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "    ax.set_ylabel(\"Classification Loss\", fontsize=36)\n",
    "    ax.legend(fontsize=36)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "    if PLOT_FLAG:\n",
    "        fig.savefig(f\"{PLOT_PATH}classification_losses_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot learning rate\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    hep.cms.label(\n",
    "        ax=ax,\n",
    "        data=True,\n",
    "        label=\"Preliminary\",\n",
    "        rlabel=\"ZeroBias 2023C (13.6 TeV)\",\n",
    "        fontsize=36,\n",
    "    )\n",
    "\n",
    "    ax.plot(learning_rates, lw=3)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "    ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "    if PLOT_FLAG:\n",
    "        fig.savefig(f\"{PLOT_PATH}learning_rate_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot learning rate with log scale\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.grid(which=\"major\", axis=\"both\", alpha=0.3, color=\"gray\", linestyle=\"-\")\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    hep.cms.label(\n",
    "        ax=ax,\n",
    "        data=True,\n",
    "        label=\"Preliminary\",\n",
    "        rlabel=\"ZeroBias 2023C (13.6 TeV)\",\n",
    "        fontsize=36,\n",
    "    )\n",
    "\n",
    "    ax.plot(learning_rates, lw=3)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=36)\n",
    "    ax.set_ylabel(\"Learning Rate\", fontsize=36)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "\n",
    "    if PLOT_FLAG:\n",
    "        fig.savefig(f\"{PLOT_PATH}log_learning_rate_{learning_rate}_{batch_size}_{reg_strength}_{regression_weight}_{min_loss_improvement}_{early_stopping_threshold}.pdf\", dpi=300, facecolor=\"white\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_losses_and_learning_rates(train_classification_losses, val_classification_losses, learning_rates, PLOT_FLAG, PLOT_PATH, learning_rate, batch_size, reg_strength, regression_weight, min_loss_improvement, early_stopping_threshold, FIGSIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snn_to_train['2NN:']:\n",
    "    plot_losses_and_learning_rates(\n",
    "        results_2[\"train_classification_losses\"],\n",
    "        results_2[\"val_classification_losses\"],\n",
    "        results_2[\"learning_rates\"],\n",
    "        PLOT_FLAG,\n",
    "        PLOT_PATH,\n",
    "        learning_rate_2,\n",
    "        batch_size_2,\n",
    "        reg_strength_2,\n",
    "        0,\n",
    "        min_loss_improvement_2,\n",
    "        early_stopping_threshold_2,\n",
    "        FIGSIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snn_to_train['3NN:']:\n",
    "    plot_losses_and_learning_rates(\n",
    "        results_3[\"train_classification_losses\"],\n",
    "        results_3[\"val_classification_losses\"],\n",
    "        results_3[\"learning_rates\"],\n",
    "        PLOT_FLAG,\n",
    "        PLOT_PATH,\n",
    "        learning_rate_3,\n",
    "        batch_size_3,\n",
    "        reg_strength_3,\n",
    "        0,\n",
    "        min_loss_improvement_3,\n",
    "        early_stopping_threshold_3,\n",
    "        FIGSIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform multiple trainings to compute average loss (TAKES A LOT OF TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_4(features, targets, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        classification_targets = targets[:, :]\n",
    "        class_outputs = model(features, training=True)\n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    optimizer.apply_gradients(zip(tape.gradient(classification_loss, model.trainable_variables), model.trainable_variables))\n",
    "    return classification_loss\n",
    "\n",
    "@tf.function\n",
    "def val_step_4(features, targets, model):\n",
    "    classification_targets = targets[:, :]\n",
    "    class_outputs = model(features, training=False)\n",
    "    classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "    return classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3 datasets data_2\n",
    "data_2 = pd.read_csv(FILE_SAVE_DATA + 'data_2s.csv')\n",
    "data_3 = pd.read_csv(FILE_SAVE_DATA + 'data_3s.csv')\n",
    "data_4 = pd.read_csv(FILE_SAVE_DATA + 'data_4s.csv')\n",
    "\n",
    "#remove n_stubs from data_2\n",
    "data_2 = data_2.drop(columns=['n_stubs'])\n",
    "data_3 = data_3.drop(columns=['n_stubs'])\n",
    "data_4 = data_4.drop(columns=['n_stubs'])\n",
    "\n",
    "\n",
    "#remove info_a, info_b, info_c, info_d from data_4\n",
    "data_4 = data_4.drop(columns=['info_a', 'info_b', 'info_c', 'info_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "scale_factor_4  = 0.5\n",
    "patience_4  = 5\n",
    "min_loss_improvement_4 = 0.1\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss_4  = float('inf')\n",
    "epochs_without_improvement_4  = 0\n",
    "patience_4  = 10  # Number of epochs to wait before stopping\n",
    "early_stopping_threshold_4  = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer_4 = optimizers.Adam(learning_rate=learning_rate_4)\n",
    "scheduler_4 = CustomLRScheduler(\n",
    "    optimizer_4, \n",
    "    factor=scale_factor_4, \n",
    "    patience=patience_4, \n",
    "    min_improvement=min_loss_improvement_4, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/167\n",
      "Train Losses - Classification: 0.1464\n",
      "Validation Losses - Classification: 0.1202\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 2/167\n",
      "Train Losses - Classification: 0.0948\n",
      "Validation Losses - Classification: 0.0831\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 3/167\n",
      "Train Losses - Classification: 0.0827\n",
      "Validation Losses - Classification: 0.0839\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 4/167\n",
      "Train Losses - Classification: 0.0820\n",
      "Validation Losses - Classification: 0.0818\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 5/167\n",
      "Train Losses - Classification: 0.0818\n",
      "Validation Losses - Classification: 0.0855\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 6/167\n",
      "Train Losses - Classification: 0.0817\n",
      "Validation Losses - Classification: 0.0836\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 7/167\n",
      "Train Losses - Classification: 0.0815\n",
      "Validation Losses - Classification: 0.0823\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 8/167\n",
      "Train Losses - Classification: 0.0814\n",
      "Validation Losses - Classification: 0.0828\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 9/167\n",
      "Train Losses - Classification: 0.0813\n",
      "Validation Losses - Classification: 0.0850\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 10/167\n",
      "Train Losses - Classification: 0.0813\n",
      "Validation Losses - Classification: 0.0826\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 11/167\n",
      "Train Losses - Classification: 0.0811\n",
      "Validation Losses - Classification: 0.0840\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Epoch 12/167\n",
      "Train Losses - Classification: 0.0811\n",
      "Validation Losses - Classification: 0.0833\n",
      "Learning Rate: 0.000743\n",
      "-------------\n",
      "Decreasing learning rate to 0.00037163286469876766\n",
      "Epoch 13/167\n",
      "Train Losses - Classification: 0.0801\n",
      "Validation Losses - Classification: 0.0820\n",
      "Learning Rate: 0.000372\n",
      "-------------\n",
      "Decreasing learning rate to 0.00018581643234938383\n",
      "Epoch 14/167\n",
      "Train Losses - Classification: 0.0795\n",
      "Validation Losses - Classification: 0.0814\n",
      "Learning Rate: 0.000186\n",
      "-------------\n",
      "Decreasing learning rate to 9.290821617469192e-05\n",
      "Epoch 15/167\n",
      "Train Losses - Classification: 0.0792\n",
      "Validation Losses - Classification: 0.0809\n",
      "Learning Rate: 0.000093\n",
      "-------------\n",
      "Decreasing learning rate to 4.645410808734596e-05\n",
      "Epoch 16/167\n",
      "Train Losses - Classification: 0.0791\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000046\n",
      "-------------\n",
      "Decreasing learning rate to 2.322705404367298e-05\n",
      "Epoch 17/167\n",
      "Train Losses - Classification: 0.0790\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000023\n",
      "-------------\n",
      "Decreasing learning rate to 1.161352702183649e-05\n",
      "Epoch 18/167\n",
      "Train Losses - Classification: 0.0790\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000012\n",
      "-------------\n",
      "Decreasing learning rate to 5.806763510918245e-06\n",
      "Epoch 19/167\n",
      "Train Losses - Classification: 0.0790\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000006\n",
      "-------------\n",
      "Decreasing learning rate to 2.9033817554591224e-06\n",
      "Epoch 20/167\n",
      "Train Losses - Classification: 0.0790\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000003\n",
      "-------------\n",
      "Decreasing learning rate to 1.4516908777295612e-06\n",
      "Epoch 21/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000001\n",
      "-------------\n",
      "Decreasing learning rate to 7.258454388647806e-07\n",
      "Epoch 22/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000001\n",
      "-------------\n",
      "Decreasing learning rate to 3.629227194323903e-07\n",
      "Epoch 23/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 1.8146135971619515e-07\n",
      "Epoch 24/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 9.073067985809757e-08\n",
      "Epoch 25/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 4.536533992904879e-08\n",
      "Epoch 26/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 2.2682669964524393e-08\n",
      "Epoch 27/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 1.1341334982262197e-08\n",
      "Epoch 28/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 5.670667491131098e-09\n",
      "Epoch 29/167\n",
      "Train Losses - Classification: 0.0789\n",
      "Validation Losses - Classification: 0.0807\n",
      "Learning Rate: 0.000000\n",
      "-------------\n",
      "Decreasing learning rate to 2.835333745565549e-09\n",
      "Early stopping triggered. No improvement for 10 epochs.\n",
      "Iteration 0:\n",
      "Hyperparameters: {'architecture': [44, 39, 37], 'learning_rate': 0.0007432657089243705, 'num_epochs': 167, 'batch_size': 64, 'reg_strength': 0.0003241716258130226}\n",
      "Final validation loss: 0.08069361001253128\n",
      "--------------------\n",
      "Classification Accuracy on Test Set 4stubs: 98.20%\n",
      "Architecture used (#features, #neurons, ..): [44, 39, 37]\n",
      "L1 Classification Accuracy on Test Set 4stubs: 98.85%\n",
      "----------------\n",
      "Epoch 1/376\n",
      "Train Losses - Classification: 0.2366\n",
      "Validation Losses - Classification: 0.1401\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 2/376\n",
      "Train Losses - Classification: 0.1386\n",
      "Validation Losses - Classification: 0.1378\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 3/376\n",
      "Train Losses - Classification: 0.1364\n",
      "Validation Losses - Classification: 0.1367\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 4/376\n",
      "Train Losses - Classification: 0.1336\n",
      "Validation Losses - Classification: 0.1331\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 5/376\n",
      "Train Losses - Classification: 0.1295\n",
      "Validation Losses - Classification: 0.1287\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 6/376\n",
      "Train Losses - Classification: 0.1227\n",
      "Validation Losses - Classification: 0.1226\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 7/376\n",
      "Train Losses - Classification: 0.1164\n",
      "Validation Losses - Classification: 0.1178\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 8/376\n",
      "Train Losses - Classification: 0.1112\n",
      "Validation Losses - Classification: 0.1107\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 9/376\n",
      "Train Losses - Classification: 0.1027\n",
      "Validation Losses - Classification: 0.0989\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 10/376\n",
      "Train Losses - Classification: 0.0916\n",
      "Validation Losses - Classification: 0.0889\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 11/376\n",
      "Train Losses - Classification: 0.0858\n",
      "Validation Losses - Classification: 0.0859\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 12/376\n",
      "Train Losses - Classification: 0.0839\n",
      "Validation Losses - Classification: 0.0850\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 13/376\n",
      "Train Losses - Classification: 0.0829\n",
      "Validation Losses - Classification: 0.0840\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 14/376\n",
      "Train Losses - Classification: 0.0823\n",
      "Validation Losses - Classification: 0.0856\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 15/376\n",
      "Train Losses - Classification: 0.0819\n",
      "Validation Losses - Classification: 0.0829\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 16/376\n",
      "Train Losses - Classification: 0.0816\n",
      "Validation Losses - Classification: 0.0826\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 17/376\n",
      "Train Losses - Classification: 0.0814\n",
      "Validation Losses - Classification: 0.0834\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 18/376\n",
      "Train Losses - Classification: 0.0812\n",
      "Validation Losses - Classification: 0.0821\n",
      "Learning Rate: 0.000279\n",
      "-------------\n",
      "Epoch 19/376\n",
      "Train Losses - Classification: 0.0811\n",
      "Validation Losses - Classification: 0.0821\n",
      "Learning Rate: 0.000279\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m   \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_compatible_tensor_list\u001b[49m(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classification_loss\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/device:GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     results_4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classification_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataset_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOUT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlosses_4_classification_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_step_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_step_4\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m train_classification_losses_all\u001b[38;5;241m.\u001b[39mappend(results_4[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_classification_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    161\u001b[0m val_classification_losses_all\u001b[38;5;241m.\u001b[39mappend(results_4[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_classification_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[184], line 26\u001b[0m, in \u001b[0;36mtrain_classification_model\u001b[0;34m(num_epochs, train_dataset, val_dataset, model, optimizer, scheduler, out_path, early_stopping_threshold, patience, best_val_loss, loss_fname, val_step, train_step)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     train_classification_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_classification_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     train_classification_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:782\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_compatible_tensor_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py:250\u001b[0m, in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an element constructed from the given spec and tensor list.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    the given spec does not match the given number of tensors.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_tensor_list_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_compatible_tensor_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43melement_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py:229\u001b[0m, in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    227\u001b[0m   flat_ret\u001b[38;5;241m.\u001b[39mappend(decode_fn(component_spec, value))\n\u001b[1;32m    228\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_flat_values\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_ret\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/data/util/nest.py:87\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpack_sequence_as\u001b[39m(structure, flat_sequence):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a given flattened sequence packed into a nest.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m  If `structure` is a scalar, `flat_sequence` must be a single-element list;\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    ValueError: If nest and structure have different element counts.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:860\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[0;34m(modality, structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m    857\u001b[0m       structure, flat_sequence, expand_composites, sequence_fn\n\u001b[1;32m    858\u001b[0m   )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[0;32m--> 860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_data_pack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    862\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    863\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown modality used \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(modality)\n\u001b[1;32m    864\u001b[0m   )\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:965\u001b[0m, in \u001b[0;36m_tf_data_pack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m    957\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    958\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not pack sequence. Argument `structure` had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_structure)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, but argument `flat_sequence` had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_sequence)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements. Received structure: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    961\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstructure\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, flat_sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflat_sequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    962\u001b[0m   )\n\u001b[1;32m    964\u001b[0m _, packed \u001b[38;5;241m=\u001b[39m _tf_data_packed_nest_with_indices(structure, flat_sequence, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 965\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msequence_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:245\u001b[0m, in \u001b[0;36msequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, _wrapt\u001b[38;5;241m.\u001b[39mObjectProxy):\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;66;03m# For object proxies, first create the underlying type and then re-wrap it\u001b[39;00m\n\u001b[1;32m    243\u001b[0m   \u001b[38;5;66;03m# in the proxy type.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(instance)(sequence_like(instance\u001b[38;5;241m.\u001b[39m__wrapped__, args))\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, CustomNestProtocol):\n\u001b[1;32m    246\u001b[0m   metadata \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39m__tf_flatten__()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m__tf_unflatten__(metadata, \u001b[38;5;28mtuple\u001b[39m(args))\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/typing.py:2029\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_protocol:\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(instance, attr) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;66;03m# All *methods* can be blocked by setting them to None.\u001b[39;00m\n\u001b[1;32m   2027\u001b[0m             (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, attr, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   2028\u001b[0m              \u001b[38;5;28mgetattr\u001b[39m(instance, attr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2029\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_get_protocol_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[1;32m   2030\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__instancecheck__\u001b[39m(instance)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-215-env/lib/python3.11/typing.py:1940\u001b[0m, in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneric\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1940\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(base, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(base\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(annotations\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_abc_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m attr \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m EXCLUDED_ATTRIBUTES:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def generate_random_hyperparameters():\n",
    "    return {\n",
    "        'architecture': [len(stub_features_4)] + [np.random.randint(16, 64) for _ in range(np.random.randint(1, 4))],\n",
    "        'learning_rate': loguniform.rvs(1e-4, 1e-1),\n",
    "        'num_epochs': np.random.randint(100, 400),\n",
    "        'batch_size': 2**np.random.randint(6, 10),\n",
    "        'reg_strength': loguniform.rvs(1e-5, 1e-2)\n",
    "    }\n",
    "\n",
    "def train_classification_model(num_epochs, train_dataset, val_dataset, model, optimizer, scheduler, out_path, early_stopping_threshold, patience, best_val_loss, loss_fname, val_step, train_step):\n",
    "    train_classification_losses = []\n",
    "    val_classification_losses = []\n",
    "    learning_rates = []\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    out_file_path = out_path + '/' + \".Models/RandomSearch/\" + loss_fname\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_classification_loss = 0\n",
    "        for features, targets in train_dataset:\n",
    "            train_classification_loss += train_step(features, targets, model, optimizer)\n",
    "        train_classification_loss /= len(train_dataset)\n",
    "        train_classification_losses.append(train_classification_loss.numpy())\n",
    "\n",
    "        # Validation\n",
    "        val_classification_loss = 0\n",
    "        for features, targets in val_dataset:\n",
    "            val_classification_loss += val_step(features, targets, model)\n",
    "        val_classification_loss /= len(val_dataset)\n",
    "        val_classification_losses.append(val_classification_loss.numpy())\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.lr.numpy()\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Losses - Classification: {train_classification_losses[-1]:.4f}\")\n",
    "        print(f\"Validation Losses - Classification: {val_classification_losses[-1]:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "        scheduler.on_epoch_end(epoch, {\"val_loss\": val_classification_loss})\n",
    "\n",
    "        with open(out_file_path, \"a\") as output_file:\n",
    "            output_file.write(f\"{train_classification_losses[-1]},{val_classification_losses[-1]},{current_lr}\\n\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_classification_losses[-1] < best_val_loss - early_stopping_threshold:\n",
    "            best_val_loss = val_classification_losses[-1]\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"train_classification_losses\": train_classification_losses,\n",
    "        \"val_classification_losses\": val_classification_losses,\n",
    "        \"learning_rates\": learning_rates\n",
    "    }\n",
    "\n",
    "train_classification_losses_all = []\n",
    "val_classification_losses_all = []\n",
    "all_hyperparams = []\n",
    "\n",
    "with open('RandomSearchResults.csv', 'a') as f:\n",
    "    f.write(\"iteration,architecture,learning_rate,num_epochs,batch_size,reg_strength,accuracy,l1_accuracy\\n\")\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    # Generate random hyperparameters\n",
    "    hyperparams = generate_random_hyperparameters()\n",
    "    all_hyperparams.append(hyperparams)\n",
    "    \n",
    "    # Hyperparameters 4NN\n",
    "    input_size_4 = len(stub_features_4)\n",
    "    architecture_4 = hyperparams['architecture']\n",
    "    output_size_4 = len(target_features)\n",
    "    learning_rate_4 = hyperparams['learning_rate']\n",
    "    num_epochs_4 = hyperparams['num_epochs']\n",
    "    batch_size_4 = hyperparams['batch_size']\n",
    "    reg_strength_4 = hyperparams['reg_strength']\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10  # Number of epochs to wait before stopping\n",
    "    early_stopping_threshold = 1e-5  # Minimum improvement in loss function to be considered as improvement\n",
    "\n",
    "    optimizer_4 = optimizers.Adam(learning_rate=learning_rate_4)\n",
    "    scheduler = CustomLRScheduler(\n",
    "        optimizer_4, \n",
    "        factor=scale_factor_4, \n",
    "        patience=patience, \n",
    "        min_improvement=min_loss_improvement_4, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    train_val_data_4, test_data_4 = train_test_split(data_4, test_size=0.3, random_state=42)\n",
    "    train_data_4, val_data_4 = train_test_split(train_val_data_4, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Normalize the data\n",
    "    normalize_data(train_data_4, val_data_4, test_data_4, data_4, normalizations)\n",
    "        \n",
    "    train_dataset_4 = tf.data.Dataset.from_tensor_slices((train_data_4[stub_features_4].values, train_data_4[target_features].values)).batch(batch_size_4).shuffle(buffer_size=len(train_data_4))\n",
    "    val_dataset_4 = tf.data.Dataset.from_tensor_slices((val_data_4[stub_features_4].values, val_data_4[target_features].values)).batch(batch_size_4)\n",
    "    test_dataset_4 = tf.data.Dataset.from_tensor_slices((test_data_4[stub_features_4].values, test_data_4[target_features].values)).batch(batch_size_4)\n",
    "\n",
    "    # Create the model\n",
    "    model_4 = MultiTaskNN(architecture_4, reg_strength=reg_strength_4)\n",
    "\n",
    "    # Build the model with the batch input shape\n",
    "    bs = None  # None allows for variable batch size\n",
    "    model_4.build((bs, input_size_4))\n",
    "\n",
    "    # Initialize the optimizer with the model's trainable variables\n",
    "    optimizer_4.build(model_4.trainable_variables)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_4(features, targets, model, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            classification_targets = targets[:, :]\n",
    "            class_outputs = model(features, training=True)\n",
    "            classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "        optimizer.apply_gradients(zip(tape.gradient(classification_loss, model.trainable_variables), model.trainable_variables))\n",
    "        return classification_loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step_4(features, targets, model):\n",
    "        classification_targets = targets[:, :]\n",
    "        class_outputs = model(features, training=False)\n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(classification_targets, class_outputs)\n",
    "\n",
    "        return classification_loss\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        results_4 = train_classification_model(\n",
    "            num_epochs_4,\n",
    "            train_dataset_4,\n",
    "            val_dataset_4,\n",
    "            model_4,\n",
    "            optimizer_4,\n",
    "            scheduler,\n",
    "            OUT_PATH,\n",
    "            early_stopping_threshold,\n",
    "            patience,\n",
    "            best_val_loss,\n",
    "            f\"losses_4_classification_{i}.csv\",\n",
    "            val_step_4,\n",
    "            train_step_4\n",
    "        )\n",
    "\n",
    "    train_classification_losses_all.append(results_4[\"train_classification_losses\"])\n",
    "    val_classification_losses_all.append(results_4[\"val_classification_losses\"])\n",
    "\n",
    "    # Save the model\n",
    "    model_4.save_weights(OUT_PATH + f'Models/RandomSearch/model_4_{i}' + '.keras')\n",
    "\n",
    "    # Print the hyperparameters and final validation loss for this iteration\n",
    "    print(f\"Iteration {i}:\")\n",
    "    print(f\"Hyperparameters: {hyperparams}\")\n",
    "    print(f\"Final validation loss: {results_4['val_classification_losses'][-1]}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    # Process test data\n",
    "    test_df_4 = process_test_data(test_data_4, stub_features_4, target_features, l1_features, model_4, normalizations)\n",
    "\n",
    "    accuracy_4 = (test_df_4[\"chargeReco_pred\"] == test_df_4[\"chargeReco_true\"]).sum() / len(test_df_4)\n",
    "    print(f\"Classification Accuracy on Test Set 4stubs: {accuracy_4*100:.2f}%\")\n",
    "    print(f\"Architecture used (#features, #neurons, ..): {architecture_4}\")\n",
    "\n",
    "    l1_accuracy_4 = (test_df_4[\"chargeL1\"] == test_df_4[\"chargeReco_true\"]).sum() / len(test_df_4)\n",
    "    print(f\"L1 Classification Accuracy on Test Set 4stubs: {l1_accuracy_4*100:.2f}%\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "    # Write the number of the generation, the hyperparameters, the accuracy and the l1 accuracy on a csv file\n",
    "    with open('RandomSearchResults.csv', 'a') as f:\n",
    "        f.write(f\"{i},{hyperparams},{accuracy_4},{l1_accuracy_4}\\n\")\n",
    "\n",
    "    del model_4\n",
    "    del optimizer_4\n",
    "    del scheduler\n",
    "    del train_dataset_4\n",
    "    del val_dataset_4\n",
    "    del test_dataset_4\n",
    "    del train_data_4\n",
    "    del val_data_4\n",
    "    del test_data_4\n",
    "    del train_val_data_4\n",
    "\n",
    "# After all iterations, find the best model\n",
    "best_iteration = np.argmin([min(val_losses) for val_losses in val_classification_losses_all])\n",
    "best_hyperparams = all_hyperparams[best_iteration]\n",
    "\n",
    "print(f\"Best model found in iteration {best_iteration}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")\n",
    "print(f\"Best validation loss: {min(val_classification_losses_all[best_iteration])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
